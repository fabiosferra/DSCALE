import copy
import csv
import fnmatch
import hashlib
import inspect
import itertools
import os
import re
import random
import time
import traceback
from collections import OrderedDict
from functools import wraps
from pathlib import Path
from typing import Callable, Dict, List, Optional, Union, Tuple, Any
from itertools import cycle
import shutil

import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import Patch
from matplotlib.lines import Line2D
from matplotlib import rcParams
import subprocess

import numpy as np
import pandas as pd
import pycountry
import scipy
from scipy.stats import linregress
from joblib import Memory
import heapq
from openpyxl import load_workbook
from pandas.testing import assert_frame_equal
from collections import OrderedDict
from scipy import integrate

import pycountry
import seaborn as sns

sns.set()
from scipy import interpolate

import pprint
import datetime

import downscaler
from downscaler.utils_dictionary import sum_two_dictionaries, fun_sort_dict, find_best_key_match_by_common_values, sum_multiple_dictionaries, fun_sort_dict_by_value_length
from downscaler.utils_list import (
    fun_sort_list_order_based_on_element_name,
    fun_fuzzy_match,
    fun_wildcard,
    fun_flatten_list_recursive
)
from downscaler.utils_list import fun_find_previous_next_item
from downscaler.utils_string import fun_filter_list_of_strings, fun_first_letter_in_each_word, find_longest_common_substring,extract_text_in_parentheses
from downscaler.utils_pandas import (
    fun_dict_level1_vs_list_level2,
    fun_xs,
    fun_index_names,
    fun_add_multiply_dfmultindex_by_dfsingleindex,
    fun_remove_np_in_df_index,
    fun_fill_na_with_previous_next_values,
    fun_replace_d_dot_iso,
    fun_select_model_scenarios_combinations,
    fun_skip_first_row,
    fun_find_iso_column,
    fun_create_var_as_sum,
    fun_check_iamc_index,
    fun_recalculate_existing_var_as_sum,
    fun_check_iamc_index_and_region,
    fun_rename_index_name,
    fun_drop_duplicates,
    rename_tuple,
    fun_available_scen,
    fun_get_variable_unit_dictionary,
    fun_check_if_all_characters_are_numbers,
    fun_read_csv_or_excel,
    fun_add_units,
    fun_read_csv,
    fun_clip_df_by_dict,
    fun_phase_out_in_dates,
    fun_drop_duplicates,
    fun_clip_df_by_dict,
    summarize_dataframe,
    fun_check_missing_elements_in_dataframe,
)

# from downscaler import Step_5f_international_transport
from downscaler import CONSTANTS, IFT, plt
from downscaler.fixtures import (
    price_var,
    seq_var,
    step1_var,
    step2_var,
    step3_var,
    step5b_var,
    list_of_fuels,
    list_of_ec,
    list_of_sectors,
    primap_dict,
    check_consistency_dict,
    dict_y_den,
    iea_flow_dict,
    iea_countries,
    all_countries,
    fun_conv_settings,
    step2_primary_energy, 
    colors,
    check_IEA_countries,
    sectors_energy_demand,
    sectors_energy_demand_split_res,
)
from downscaler.mapping import iea_dict
from downscaler.fit_funcs import func_dict


# setting up the memory object for caching
# cache is located at downscaler.CONSTANTS.CACHE_DIR
memory = Memory(downscaler.CONSTANTS.CACHE_DIR)

## workaround below (to be fixed)
iam_base_year = 2010
region = "CPA"
seed = 1345

m = ["TIME", "ISO"]  # multi-index


class MemFunc:
    """Thin wrapper class around a function, to be used together with
    joblib.memory

    This wrapper is necessary because for cached functions that use other
    functions we need to keep track of the other functions, illustrated here:

    ```
    def f1(a, b):
    return a+b

    @memory.cache
    def f2(a, b):
        return f1(a, b)
    ```
    If `f1` changes, we have a problem since joblib.memory has no way of knowing
    that when looking at `f2`. The solution to the problem is giving `f1` as an
    input to `f2` **and** wrapping `f1` in this class.

    ```
    def f2(a, b, f1 : MemFunc = MemFunc(f1)):
        return f1(a, b)
    ```
    The only change needed is adding f1 as an input parameter of type MemFunc
    and setting the default to MemFunc(f1). As MemFunc implements a __call__
    method, the rest is completely user transparent.

    Internally, MemFunc saves the provided function, the source code of the
    function as well as its own source code. This last step is necessary to
    ensure that the wrapper MemFunc itself did not change.

    """

    def __init__(self, func):
        self.func = func
        self.func_code = inspect.getsource(func)
        self.Callable_code = inspect.getsource(MemFunc)

    def __call__(self, *args, **kwargs):
        return self.func(*args, **kwargs)


def make_optionally_cacheable(fun):
    """This decorator makes caching using joblib.Memory.cache optional

    Example
    -------

    It is controlled the following way:
    ```python
    import downscaler
    # very important to `import downscaler` and *not* `from downscaler import
    # USE_CACHING` (see [stackoverflow](https://stackoverflow.com/questions/
    # 1977362/how-to-create-module-wide-variables-in-python) for details)

    # to use caching set
    downscaler.USE_CACHING = True
    # to not use caching set
    downscaler.USE_CACHING = False
    ```

    Notes
    -----
    If USE_CACHE is false, the results are still cached to the disk. This is so that
    they may be used in future runs where caching is enabled.
    """

    cached_fun = memory.cache(fun)

    @wraps(fun)
    def wrapper(*args, **kwargs):
        return (
            cached_fun(*args, **kwargs)
            if downscaler.USE_CACHING
            else cached_fun.call(*args, **kwargs)[0]
        )

    return wrapper


# NOTE: maybe change fnmatch.fnmatch(x, y) to fnmatch.fnmatch(x.lower(), y.lower())
# so that we are no longer case sensitive
def match_any_with_wildcard(x: str, patterns: Union[str, List[str]]) -> bool:
    """Matches a string x against a given list of patterns.

    patterns can also be a str.

    Args:
        x (str): [description]
        patterns (Union[str, List[str]]): [description]

    Returns:
        bool: [description]
    """
    patterns = [patterns] if isinstance(patterns, str) else patterns
    return any(fnmatch.fnmatch(x, y) for y in patterns)


def convert_to_list(input: Union[str, list]) -> list:
    """Simple utility function to convert an input into a list if it is not already.

    Parameters
    ----------
    input : Union[str, list]
        Either string or list of strings

    Returns
    -------
    list
        Input if already a list, otherwise [input]
    """
    return [input] if not isinstance(input, list) else input


# Thin wrapper class for an input file path adding the hash of the file to it
# for the future this could be pimped with pydantic and file would become
# a input file path so that the existence of the file is automatically checked
class InputFile:
    def __init__(self, file: Union[str, Path]) -> None:
        self.file = file
        self.hash = self.calc_hash()

    def calc_hash(self) -> str:
        """Generate a hash from the contents of a file

        Parameters
        ----------
        file : str
            File to be hashed

        Returns
        -------
        str:
            Hexadecimal representation of the file hash

        Notes
        -----
        For details refer to https://stackoverflow.com/questions/1131220/get-md5-hash-of-big-files-in-python
        """
        with open(self.file, "rb") as f:
            file_hash = hashlib.md5()
            # we read the file at a rate of 8192 bytes a chunk
            # this takes advantage of the digest size of 128 bytes
            chunk = f.read(8192)
            while chunk:
                file_hash.update(chunk)
                chunk = f.read(8192)
        return file_hash.hexdigest()

    def __repr__(self) -> str:
        # this is just so that we get a nice representation of the class since
        # joblib.memory also writes a json with with the input parameters of
        # the function call
        return f"{self.__class__}: {self.__dict__}"


def setindex(_df, _index):
    """
    fun_df_setindex

    This function sets an index (_index) in you dataframe (_df) .
    If _index is not in the df it will not keep the df unchanged.
    If _index=False it will reset the index

    _df= your dataframe
    _index= Your index. If =False will  reset_index

    """
    try:
        _df.reset_index(inplace=True)
    except:
        dont = 1

    if _index != False:
        _df.set_index(_index, inplace=True)

    try:
        # where 1 is column,  0 is row
        _df2 = _df.drop(labels="level_0", axis=1, inplace=True)
    except:
        _df2 = _df

    try:
        # where 1 is column,  0 is row
        _df3 = _df2.drop(labels="index", axis=1, inplace=True)
    except:
        _df3 = _df2

    return _df3




def swap_country(list, pos1, pos2):
    """
    fun_lst_swap
    This function swaps the coutry order.
    It takes the country from pos1, and moves it to the pos2
    """
    list[pos1], list[pos2] = list[pos2], list[pos1]
    return list


# NOTE: This is different from set(list(list1)). The function below preserves the initial list order. Set could potentially return a completely different order
def unique(list1):
    """
    fun lst_unique
    This function returns unique elements in a list, by preserving the initial list order
    (something that set(list) will not do)
    """
    # intilize a null list
    unique_list = []
    for x in list1:
        # check if exists in unique_list or not
        if x not in unique_list:
            unique_list.append(x)
    return unique_list




def reg_stat(x, y, _base_year=""):
    """
    fun_series_stat_all
    Return R^2, slope, intercpet. where x and y are array-like.
    If you introduce a _base_year is harmonises the intercept to match _base_year data
    """

    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)
    if _base_year != "":
        intercept = y[_base_year] - slope * x[_base_year]
    return r_value**2, slope, intercept


def s_shape_lin(_x, sign=1):
    """
    fun_lst_trans_from_s_to_exp
    Transforms S-shape to exponential function by calculating the ratio between  '_x' relative to 1-_x.
    If sign=-1   it returns the inverse
    """
    _y = _x / (1 - _x * sign)
    return _y


def flexi_fun_lin(_alpha, _beta, _x, _exp=True):
    """
    fun_lst_trans_lin

    Calculates variable based on functional form: it requires as input  (_alpha, _beta) and the independent variable (_x).
    Takes as parameters: _alpha, _beta (from your regression)
    Indipendent variable=_x
    _exp= It True If returns the exponent of y
    """

    y = _alpha + _beta * _x

    if _exp == True:
        y = np.exp(y)
    return y






# READING SAVED EXCEL FILES WITH ISO
# @make_optionally_cacheable
def fun_read_df_iea_all(file: IFT) -> pd.DataFrame:
    """Reading IEA data with ISO code. It returns a dataframe (_df_iea_all)"""
    _df_iea_all = pd.read_csv(file, sep=",", encoding="latin-1")  # index_col=['ISO'] )
    return _df_iea_all


def fun_read_IEA_fuel_dict(file: IFT) -> dict:
    """Reading IEA fuel dictionary. It returns a dictionary (_IEA_fuel_dict)"""
    _df_IEA_fuel_dict = pd.read_csv(
        file,
        sep=",",  # encoding="latin-1"
    )  # index_col=['ISO'] )
    # print(_df_IEA_fuel_dict.head())
    setindex(_df_IEA_fuel_dict, "IEA")
    _IEA_fuel_dict = _df_IEA_fuel_dict.to_dict()
    return _IEA_fuel_dict


def fun_pd_sel(_df, _time, _iso, _search=""):
    """
    This function retrives data from a _df for a given time (_time), country (_iso)
    _time and _iso can be lists ,  string/float.
    They can also be empty strings "" (in this case we select all data).
    It also allows for a simple search (_search => only one element) without specifying the column.
    """
    _m = ["TIME", "ISO"]
    try:
        setindex(_df, _m)
    except:
        _m = ["TIME", "REGION"]
        setindex(_df, _m)

    if _time == "":
        _time = _df.index.get_level_values(0).unique()
    elif type(_time) == range:
        _time = list(_time)
    else:
        if type(_time) != list:
            _time = [_time]

    if type(_iso) == np.ndarray:
        _iso = list(_iso)

    elif _iso == "":
        _iso = _df.index.get_level_values(1).unique()

    else:
        if type(_iso) != list:
            _iso = [_iso]

    _df_sel = _df[
        (_df.index.get_level_values(0).isin(_time))
        & (_df.index.get_level_values(1).isin(_iso))
    ]

    if _search != "":
        try:
            _col = (
                _df_sel[_df_sel == _search]
                .dropna(axis=1, how="all")
                .columns.tolist()[0]
            )  # Finding column where the element is
            # print(_col)
            _df_sel = _df_sel[_df_sel[_col] == _search]  # filtering the data
        except:
            pass
    return _df_sel


def fun_pd_assign(_df, _time, _iso, _col, _value):
    """
    This function assigns a value (_value) to a column (_col) of a dataframe (_df), for a given time (_time)
    and a given country (_iso). It returns a Dataframe.
    """
    _m = ["TIME", "ISO"]
    setindex(_df, _m)
    _time = float(_time)
    # If time (_time) is not present in dataframe for that country (_iso) => we append the df.
    # Otherwise we just update the data for that _time and _iso.
    _count_row = (
        fun_pd_sel(_df, _time, _iso).count().sum()
    )  # we count the number of rows with ISO=_iso and TIME=_time.

    if (
        _count_row == 0
    ):  # If the number of rows is equal to zero, we append the dataframe (e.g. with 2100 data)
        # print('Assign ', _time, ' value to datraframe')
        setindex(_df, False)
        _new_row = {"TIME": _time, _col: _value, "ISO": _iso}
        # _df = pd.concat([_df, _new_row], ignore_index=True)
        _df=pd.concat([_df, pd.DataFrame([{"TIME": _time, _col: _value, "ISO": _iso}])], ignore_index=True).set_index(_m)
    # Otherwise we update the data for that time (_time) and country (_iso).
    else:
        setindex(_df, _m)
        _df.loc[
            (_df.index.get_level_values(0) == _time)
            & (_df.index.get_level_values(1) == _iso),
            _col,
        ] = _value
    return _df


def fun_pd_long_format(_df: pd.DataFrame) -> pd.DataFrame:
    """
    It Returns a df in wide format  .
    """
    m = ["TIME", "ISO"]
    setindex(_df, m)
    # .unstack()#_main['Final Energy|Industry|HeatENSHORT_REF']
    a = _df.stack()
    b = pd.DataFrame(a)
    setindex(b, False)
    setindex(b, "ISO")
    b.rename(columns={"level_2": "VARIABLE"}, inplace=True)
    b.rename(columns={0: "VALUE"}, inplace=True)
    return b


def fun_pd_wide_format(
    _df: pd.DataFrame, _region: str, _model: str, _scenario: str, _unit: str
) -> pd.DataFrame:
    """
    It returns a df in wide format (in line with IAMc), starting from a df in long format.
    """
    _df["MODEL"] = _model
    _df["REGION"] = _region
    _df["SCENARIO"] = _scenario
    _df["UNIT"] = _unit

    _df = _df.pivot_table(
        index=["MODEL", "SCENARIO", "ISO", "VARIABLE"], columns="TIME", values="VALUE"
    )

    setindex(_df, False)
    #     _df.drop(_df.index)
    return _df


def fun_iamc_unmelt(_df_all):
    """2021_01_28
    This function Unmelts dataframe: from iamc/long format  => to wide shape format.
    NOTE: it does nor work if _df_all contains more than one scenario. need to select only 1 scenario in _df_all
    TO REVERT TO THE STANDARD DATAFARME PLEASE USE => fun_iamc_melt.

    """

    # UPDATE 2021_03_02
    try:  # this works only for countries/ ISO (old function)
        m = ["MODEL", "SCENARIO", "ISO", "VARIABLE"]
        setindex(_df_all, m)

        setindex(
            _df_all, ["ISO", "MODEL", "SCENARIO", "VARIABLE"]
        )  # .pivot(['VARIABLE'])
        _df_all_new = pd.DataFrame(
            _df_all.stack()
        )  # Creating a new dataframe (in wide format)
        setindex(_df_all_new, False)

        if "TIME" not in _df_all_new.columns:
            # column name corresponing to 'TIME'
            col_time_name = _df_all_new.iloc[
                :, _df_all_new.columns.str.contains("level").fillna(False)
            ].columns[0]

            # renaming 'TIME' column
            _df_all_new.rename(
                columns={col_time_name: "TIME", 0: "VALUE"}, inplace=True
            )
            _df_all_new["TIME"] = _df_all_new["TIME"]  # .astype(int)
        else:
            _df_all_new.rename(columns={0: "VALUE"}, inplace=True)

        # Pivot 'VARIABLE'
        setindex(_df_all_new, ["ISO", "MODEL", "SCENARIO", "TIME"])

        # NOTE: 2022_02_01
        # Some variables might have results for different time lenghts.
        # Therefore Here we only keep variables with the same time resolution of the variable 'Final Energy'.
        # We do this only for native regions (e.g. JPN REMIND)
        # Example (REMIND model, JPN):
        #   'Final energy' does not have data for 2065
        #   'Diagnostics|Investment|Energy Efficiency' has data for 2065. => we remove this variable from our dataframe
        if len(_df_all_new.index.get_level_values("ISO").unique()) == 1:
            expected_time_len = len(
                _df_all_new[_df_all_new.VARIABLE == "Final Energy"]
                .index.get_level_values("TIME")
                .unique()
            )
            idx_with_different_time_resolution = [
                x
                for x in _df_all_new.VARIABLE.unique()
                if len(_df_all_new[_df_all_new.VARIABLE == x]) > expected_time_len
            ]
            _df_all_new = _df_all_new[
                ~_df_all_new.VARIABLE.isin(idx_with_different_time_resolution)
            ]

        _df_all_new = _df_all_new.pivot(columns="VARIABLE")
        _df_all_new = _df_all_new["VALUE"]

    except:  # this works for regions (e.g. df_iam_all_models, modified 2021_03_02)
        m = ["MODEL", "SCENARIO", "REGION", "VARIABLE"]
        setindex(_df_all, m)

        setindex(
            _df_all, ["REGION", "MODEL", "SCENARIO", "VARIABLE"]
        )  # .pivot(['VARIABLE'])
        _df_all_new = pd.DataFrame(
            _df_all.stack()
        )  # Creating a new dataframe (in wide format)
        setindex(_df_all_new, False)

        # column name corresponing to 'TIME'
        col_time_name = _df_all_new.iloc[
            :, _df_all_new.columns.str.contains("level").fillna(False)
        ].columns[0]

        # renaming 'TIME' column
        _df_all_new.rename(columns={col_time_name: "TIME", 0: "VALUE"}, inplace=True)
        _df_all_new["TIME"] = _df_all_new["TIME"]  # .astype(int)

        # Pivot 'VARIABLE'
        setindex(_df_all_new, ["REGION", "MODEL", "SCENARIO", "TIME"])
        _df_all_new = _df_all_new.pivot(columns="VARIABLE")
        _df_all_new = _df_all_new["VALUE"]

    return _df_all_new


def fun_iamc_melt(df_all):
    """
    2021_03_01
    This function melts again the dataset into an IAM format
    """
    # return pd.DataFrame(df_all.stack()).rename(columns={0:'VALUE'}).pivot_table(index=['MODEL','SCENARIO',"ISO",'VARIABLE'],
    #                     columns='TIME',
    #                     values='VALUE')

    # 2021_04_03 update:
    _df = (
        pd.DataFrame(df_all.stack())
        .rename(columns={0: "VALUE"})[["VALUE"]]
        .astype(float)
    )  # converted to float
    return _df.pivot_table(
        index=["MODEL", "SCENARIO", "ISO", "VARIABLE"], columns="TIME", values="VALUE"
    )


def fun_read_reg_value(
    model: str, region: str, target: str, variable: str, df_iam_all_models: pd.DataFrame
) -> pd.Series:
    """
    Reading IAM results for a given model, region, target, variable
    """
    region_name = f"{model}|{region}"
    region_name = fun_region_name(model, region)
    if type(variable) != list:
        variable = [variable]
    setindex(df_iam_all_models, "TIME")
    return df_iam_all_models[
        (df_iam_all_models.VARIABLE.isin(variable))
        & (df_iam_all_models.REGION == region_name)
        & (df_iam_all_models.SCENARIO == target)
    ]


def fun_region_name(model: str, region: str) -> str:
    """Returns `region` including the model name as a prefix (unless `model` is already included in `region`)"""
    return f"{model}|{region}" if model not in region else region


def fun_convert_time(_df, _type=float):
    """2021_03_03
    This function converts time to selected _type (e.g. string, float).
    It returns 1) the updated dataframe, and 2) the original type of TIME
    """
    index_init = list(_df.index.names)

    setindex(_df, False)
    _orig = type(_df["TIME"][0])

    if _type == str:
        _df["TIME"] = _df["TIME"].astype(int)

    _df["TIME"] = _df["TIME"].astype(_type)
    setindex(_df, index_init)
    return _df, _orig


def make_left_list(_df2, ratio, _right_list, gdp, region):
    """
    This function creates the left list, based on integral values and R squared
    """

    # creating INT X R squared variable
    setindex(_df2, "ISO")
    _df2["R_X_INT"] = 0
    _df2["ENSHORT_" + str(ratio)] = _df2["ENSHORT_INIT"]

    _left_list = []
    _left_list = _df2.sort_values("R_SQUARED", ascending=False)  # LEFT LIST
    if region in (_left_list.REGION.unique()):
        try:
            _left_list = _left_list.drop(region)  # we drop the region
        except:
            pass
    _left_list = _left_list.index.unique()

    for c in _right_list:  # we drop countries in the right_list from the left_list
        if c in _left_list:
            _left_list = _left_list.drop(c)
    _left_list = _left_list.tolist()
    return _left_list


def make_optimal_list(_df_main, _left_list, _right_list, _r, region, _threshold=1, all_permutations=True):
    """
    This function finds the optimal country list order of the _right_list and combines it with the _left_list.
    It returns a list.

    _threshold=1 => accurate solution (multiple iterations across countries until the error does not decline compared to previous iteraration)
    _threshold=0.5 => iteraration process continues if error is reduced by more than 50% compared to previous solution.
    _threshold=0 => coarse solution (only one iteration across all countries)


    """
    # NOTE: To be checked if the number of big countries significantly changes the
    # quality of the results. Since we would ideally want to check all possible
    # permutations the number of combinations to check scales with n!. So for 4
    # countries we get 4!=24 but for 6!=720.
    # To be checked with a region with a lot of countries if maybe 4 would be a good
    # value.
    # For an instant gain we could say that for a region with 3 or fewer countries, we
    # simply compute all the permutation directly instead of doing random swaps.
    seconds0 = time.time()
    right_final = _right_list
    left_list = _left_list
    count = 0
    # ratio=100

    # tolerance threshold for finding optimal list solution. 1= accurate solution. 0= coarse solution.
    # usually threshold should be between 0.90 and 1 (even if =1 we cannot guarantee we find the global min solution)
    threshold = _threshold  # 0.95

    # print('ORIGINAL LIST', right_final)
    if all_permutations:
        minval,  right_final, mylist_pos = learning_all_permutations(
                left_list,
                right_final,
                _df_main,
                region,
                _sect=False,
                _idx=count,
                _ratio=_r,
            )
        _df_main.reset_index(inplace=True)
        _df_main.set_index("ISO", inplace=True)
        print(right_final)
        print(f"With all permutations it took: {time.time()- seconds0}")
        return left_list + right_final

    
    minval = 0
    i = 0  
    time0 = time.time()
    # we find the optimal list only id we have at least two countries in the righ_list.
    if len(_right_list) >= 2:  # new 2020_10_30 night
        # print(time0)
        while i < 20:  # len(countrylist):
            if count >= (len(right_final) - 1):
                count = 0
                i = (
                    i + 1
                )  # restart another round of iteration (with another country loop)

            minval, prev_val, right_final, mylist_pos = learning_country_swap(
                left_list,
                right_final,
                _df_main,
                region,
                _sect=False,
                _idx=count,
                _ratio=_r,
            )
            # counting single country (within each iteration)
            count = count + 1

            # we store minval at the beginning of the loop (count=0) as val_count0
            if count == 1:
                val_count0 = minval
                print(right_final)

            # we break loop if at end of the  loop (e.g. count=20 if n=20) we get the same solution as in count0 (beginning of loop)
            #    if count==len(countrylist)-1 and val_count0==minval:
            if count == len(right_final) - 1 and minval >= val_count0 * threshold:
                break

            # print('COUNT', count, minval, prev_val, countrylist)
        #    print('COUNT', count, minval, prev_val, minval-val_count0, len(right_final)-1,i, mylist_pos, 'TIME:',time.time()-time0)
        # print('COUNT', count, minval, prev_val, minval-val_count0, len(right_final)-1,i, mylist_pos, left_list+right_final, 'TIME:',time.time()-time0)
    else:
        right_final = _right_list

    # print(len(_countrylist), _countrylist)

    _df_main.reset_index(inplace=True)
    _df_main.set_index("ISO", inplace=True)
    print(f"With old approach it took: {time.time()- seconds0}")
    return left_list + right_final

def optimal_ratio(
    _df,
    countrylist,
    region,
    variable,
    target,
    _opt_ratio_exogenous=False,
    rangemin=-50,
    rangemax=150,
    rangedt=10,
    _bounds_=True,
    _save_graph=True,
    _show_graph=True,
):
    """
    It finds the optimal weight between enshort_ratio and enshort_init

    Where:
    enshort_ratio=short-term trend adjusted proportionally to match IAM results
    enshort_init= historical trends (unudjusted)
    """
    _df.fillna(0, inplace=True)
    if _opt_ratio_exogenous == False:
        ## NEED TO CALCULATE THE REGIONAL SUM BEFORE STARTING THE LOOP AND THEN AGAIN AFTER THE LOOP!
        setindex(_df, "TIME")
        ratio_range = range(rangemin, rangemax, rangedt)

        for r in ratio_range:
            _df["ENSHORT_" + str(r)] = learning_may(
                _df, countrylist, r, region, _bounds=_bounds_
            )

        ## Calculating sum at the regional level:
        setindex(_df, "TIME")

        # New calculations. This one works better because: see https://github.com/iiasa/downscaler_repo/issues/202
        res={r:integral_calc(_df.reset_index().set_index('ISO'), r).sum() for r in ratio_range}
        res=fun_sort_dict(res, by='values')

        minval_key = list(res.keys())[0]
        opt_ratio,minvalue = minval_key, res[minval_key]
        if _show_graph == True:
            plt.scatter(list(res.keys()), list(res.values()))
            plt.title("Integral minimisation")
            plt.show()
            
            if _save_graph:
                plt.savefig(
                        region.replace("|", "_")
                        + "_Opt_ratio_"
                        + variable.replace("|", "_")
                        + target
                        + "_.png"
                    )

        try:
            _df.reset_index(inplace=True)
        except:
            pass

        print("ratio = ", opt_ratio)
        print("value = ", minvalue)
        print("List of countries", countrylist)

    else:
        # exogenous optimal ratio (e.g. based on baseline scenario)
        opt_ratio = _opt_ratio_exogenous
        _df["ENSHORT_" + str(opt_ratio)] = learning_may(
            _df, countrylist, opt_ratio, region, _bounds=_bounds_
        )
        minvalue = -999

    ## PINK line (optimal, based on learning)
    _df["ENSHORT_REF"] = _df["ENSHORT_" + str(opt_ratio)]  
    setindex(_df, "TIME")
    return _df, opt_ratio, minvalue  



def set_bounds(_df, _upper, _lower=0, _sect=0):
    # BOUNDS: less variability here, to get the right country order
    if _sect == 0:
        _df["LOWER_BOUND"] = _df[["ENSHORT_RATIO", "ENSHORT_INIT"]].min(axis=1) * _lower
        _df["UPPER_BOUND"] = _df[["ENSHORT_RATIO", "ENSHORT_INIT"]].max(axis=1) * _upper
    else:
        _df["LOWER_BOUND"] = _df[["ENSHORT_RATIO", "ENSHORT_INIT"]].min(axis=1) * _lower
        _df["UPPER_BOUND"] = _df[["ENSHORT_RATIO", "ENSHORT_INIT"]].max(axis=1) * _upper
    return _df




# NOTE: This function was changed to enable caching. In order for this to be
# possible the data type of file was changed from IFT to InputFile. This is so
# that joblib.Memory takes the hash of the contents of the file and not just the
# file name. This means that the input file for fun_read_df_iam_all needs to be
# updated to InputFile in all use cases.


# @make_optionally_cacheable
def fun_read_df_iam_all(
    file: InputFile,
    interpolate: bool = False,
    model: str = None,
    region: str = None,
    target: str = None,
    variable: Union[str, list] = None,
    add_model_name_to_region=True,
    categorical_index: bool = True,
) -> pd.DataFrame:
    """Reading IAMs results.

    Parameters
    ==========
    file: pathlib.Path or str
        CSV File name with IAMs results.
    interpolate: bool
        If True performs interpolation if missing data after 2060. This was created for the NGFS project
        (MESSAGE and REMIND only provide data with 10 years time steps after 2060).
    model: str, default=None
    region: str, default=None
    target: str, default=None
    variable: Union[str, list], default=None
    Returns
    =======
    pd.DataFrame
        Contains IAMs results (at the regional level).

    Notes
    =====
    Return to this function for refactoring. For now we leave it as is since we just
    want to get to creating a first test case. In the future we will make heavy use of
    pyam since most of the functionality is already there.

    .. note:: Deprecated for jupyter notebooks since the input type needs to be changed
    to InputFile

    """

    # , encoding="latin-1") ## 2021_07_16
    _df_iam_all = pd.read_csv(file.file, sep=",")

    def convert_cols(x):
        return x.upper() if isinstance(x, str) else x

    _df_iam_all.columns = map(convert_cols, _df_iam_all.columns)

    ## If none of the region start with the model name, we add it to the region name => e.g. MESSAGE|Western Europe
    if (
        add_model_name_to_region
        and len(_df_iam_all.REGION.str.startswith(str(model) + "|").unique()) == 1
        and _df_iam_all.REGION.str.startswith(str(model) + "|").unique().tolist()[0]
        == False
    ):  ## This means that none of the region start with the model name e.g. MESSAGE|Western Europe
        # _df_iam_all['REGION'] = [str(model)+'|'+str(i) for i in _df_iam_all.REGION]
        _df_iam_all.loc[:, "REGION"] = (
            _df_iam_all["MODEL"].astype(str) + "|" + _df_iam_all["REGION"].astype(str)
        )

    if model is not None:
        _df_iam_all = _df_iam_all[_df_iam_all.MODEL == model]
    if region is not None:
        if region not in _df_iam_all.REGION.unique():
            raise ValueError(
                f"Given {region} not found in the IAM data. If you can't"
                "find the region use 'region'=None instead."
            )
        _df_iam_all = _df_iam_all[_df_iam_all.REGION == region]
    if target is not None:
        _df_iam_all = _df_iam_all[_df_iam_all.SCENARIO == target]
    if variable is not None:
        variable = [variable] if not isinstance(variable, list) else variable
        _df_iam_all = _df_iam_all[_df_iam_all.VARIABLE.isin(variable)]

    if interpolate is True:
        # Interpolating missing data
        cols_dict={x:str(x) for x in range(2060,2105,5)}
        colonne = list(cols_dict.values())
        _df_iam_all[colonne] = _df_iam_all[colonne].apply(
            pd.to_numeric, errors="coerce"
        )

        _df_iam_all[colonne] = _df_iam_all[colonne].interpolate(
            method="linear", limit_direction="forward", axis=1
        )

        # _df_iam_all.columns = map(str, _df_iam_all[int(i) for i in colonne])
        _df_iam_all.rename(columns=cols_dict, inplace=True )

    _df_iam_all=fun_drop_columns(_df_iam_all, case_sensitive=False, col_names=["unnamed"])
    _df_iam_all = _df_iam_all.melt(id_vars=["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])

    _df_iam_all.rename(columns={"variable": "TIME", "value": "VALUE"}, inplace=True)
    _df_iam_all["TIME"] = pd.to_numeric(_df_iam_all["TIME"])

    ## save as categorical values (we exclude the region, to perform +'r' operation later)
    ## NOTE: categorical values can only compare equality or not= > therefore we exclude time for the moment (as we need time>=2010)
    if categorical_index:
        _df_iam_all.loc[:, ["MODEL", "SCENARIO", "VARIABLE", "UNIT"]] = _df_iam_all.loc[
            :, ["MODEL", "SCENARIO", "VARIABLE", "UNIT"]
        ].astype("category")
    return _df_iam_all


def fun_read_df_countries(file: IFT) -> pd.DataFrame:
    """Reading Country-region mapping. It returns a Dataframe (_df_countries)"""
    _df_countries = pd.read_csv(file, sep=",", encoding="latin-1")  
    setindex(_df_countries, "COUNTRY_NAME")

    return _df_countries


def integral_calc(_df, _r, ref='ENSHORT_INIT'):
    """
        Calculating integral (distance between historical trend f"ENSHORT_{str(r)}" projections and current "ENSHORT" projections) for all countries and for the whole time horizon
        It returns a column named 'INTEGRAL' with its value for each country and time
        _rsquared: influence of R squared on calcualting the integral

        _df= your dataframe with data
        _r= your assumption about your current enshort projections (if =0 you are using the simple ratio approach to match regional IAM results)
        _rsquared: If equal to "True" we  weigth the value of the integral depending on the R_squared of the historical regression (we give more weight to robust trend projections).
        Example: if r_squardef str_ends(_df, _col, _str, _end=True):
    ed is equal to 1 theTruen your historical trend is very robust, so we give to the integral a weight =1.
        By contrast if your R_squared is equal to zero then your historical relationship is not robust at all, therefore we give it a weight equal to zero (there is no need to minimise the distance between historical trends and projections).
    """
    # TODO: consider log-log function vs linear. This one is linear
    return _df.groupby(_df.index).apply(lambda g: integrate.trapz(100*g["R_SQUARED"]* # R-squared
                                                                # (g["HIST_END_YEAR"]-g["HIST_START_YEAR"])* # Optionally consider the lenght of time series
                                                                 np.abs((g[f"ENSHORT_{str(_r)}"]/g.Y_DEN - g[ref]/g.Y_DEN )) , # Difference (in absolute value) between the Current EI (ENSHORT_(_r)) and the expected EI (ENSHORT_HIST) 
                                                                  x=g.X_NUM/g.X_DEN) # Calculate the difference over GDP per capita
                                                                  ).sort_values()
    # return _df["INTEGRAL_" + str(_r)]

def learning_country_swap(
    _left_list: List[str], _right_list: List[str], _df: pd.DataFrame, region: str, _sect: bool, 
    _idx: int = 0, _ratio: int = 100, rsquared: float = 0.95
) -> Tuple[float, float, List[str], str]:
    """
    This function swaps the country order in order to minimize the sum of all countries' integral (for a given ratio).
    
    Parameters:
        _left_list (List[str]): List of countries that will not change (exogenously defined).
        _right_list (List[str]): List of countries that will be optimized during learning.
        _df (pd.DataFrame): Dataframe containing the relevant data.
        region (str): Region information.
        _sect (bool): False if main sector, otherwise True.
        _idx (int): Position of the country. It starts from zero. Default is 0.
        _ratio (int): Ratio value for calculation. Default is 100.
        rsquared (float): R-squared value for calculation. Default is 0.95.

    Returns:
        Tuple[float, float, List[str], str]: A tuple containing:
            - minval: The value of the integral associated with the new list.
            - prev_val: Previous value of the integral associated with the old list.
            - countrylist: The new country list (associated with minval).
            - position: The position descriptor of the new country list.
    """
    
    def calculate_integral(country_list: List[str], ratio: int) -> float:
        """
        Helper function to calculate the integral for a given country list and ratio.
        """
        setindex(_df, "TIME")
        _df["ENSHORT_" + str(ratio)] = learning_may(_df, _left_list + country_list, ratio, region)
        # integral_23bis(_df, ratio, _rsquared=rsquared)
        setindex(_df, "TIME")
        # New calculations. This one works better because: see https://github.com/iiasa/downscaler_repo/issues/202
        return integral_calc(_df.reset_index().set_index('ISO'), ratio).sum()
        # Old calculations
        # return _df.loc[:, ("INTEGRAL_" + str(r))].fillna(0).sum()

    country_lists = {
        "orig": copy.deepcopy(_right_list),
        "start": swap_country(copy.deepcopy(_right_list), _idx, 0),
        "end": swap_country(copy.deepcopy(_right_list), _idx, len(_right_list) - 1),
        "mid": swap_country(copy.deepcopy(_right_list), _idx, round(len(_right_list) / 2)),
        "next": swap_country(copy.deepcopy(_right_list), _idx, _idx + 1),
        "rand": swap_country(copy.deepcopy(_right_list), random.randint(0, len(_right_list) - 1), random.randint(0, len(_right_list) - 1)),
        "rand2": swap_country(copy.deepcopy(_right_list), random.randint(0, len(_right_list) - 1), random.randint(0, len(_right_list) - 1)),
        "rand_list": copy.deepcopy(random.sample(_right_list, k=len(_right_list)))
    }

    integrals = {key: calculate_integral(lst, _ratio) for key, lst in country_lists.items()}
    prev_val = integrals["orig"]
    minval_key = min(integrals, key=integrals.get)
    
    return integrals[minval_key], prev_val, country_lists[minval_key], minval_key

def learning_all_permutations(
    _left_list: List[str], _right_list: List[str], _df: pd.DataFrame, region: str, _sect: bool, 
    _idx: int = 0, _ratio: int = 100, rsquared: float = 0.95
) -> Tuple[float, float, List[str], str]:
    """
    This function swaps the country order in order to minimize the sum of all countries' integral (for a given ratio).
    
    Parameters:
        _left_list (List[str]): List of countries that will not change (exogenously defined).
        _right_list (List[str]): List of countries that will be optimized during learning.
        _df (pd.DataFrame): Dataframe containing the relevant data.
        region (str): Region information.
        _sect (bool): False if main sector, otherwise True.
        _idx (int): Position of the country. It starts from zero. Default is 0.
        _ratio (int): Ratio value for calculation. Default is 100.
        rsquared (float): R-squared value for calculation. Default is 0.95.

    Returns:
        Tuple[float, float, List[str], str]: A tuple containing:
            - minval: The value of the integral associated with the new list.
            - prev_val: Previous value of the integral associated with the old list.
            - countrylist: The new country list (associated with minval).
            - position: The position descriptor of the new country list.
    """
    
    def calculate_integral(country_list: List[str], ratio: int) -> float:
        """
        Helper function to calculate the integral for a given country list and ratio.
        """
        setindex(_df, "TIME")
        _df["ENSHORT_" + str(ratio)] = learning_may(_df, _left_list + country_list, ratio, region)
        # integral_23bis(_df, ratio, _rsquared=rsquared)
        setindex(_df, "TIME")
        # New calculations. This one works better because: see https://github.com/iiasa/downscaler_repo/issues/202
        return integral_calc(_df.reset_index().set_index('ISO'), ratio).sum()
        # Old calculations
        # return _df.loc[:, ("INTEGRAL_" + str(r))].fillna(0).sum()

    permutations = set(get_permutations(_right_list))

    country_lists = {n[0]:list(n[1]) for n in enumerate(permutations) }

    integrals = {key: calculate_integral(lst, _ratio) for key, lst in country_lists.items()}
    integrals=fun_sort_dict(integrals, by='values')
    # prev_val = integrals["orig"]
    minval_key = list(integrals.keys())[0]
    
    return integrals[minval_key],  country_lists[minval_key], minval_key

def learning_may(
    _df: pd.DataFrame,
    _countrylist: List[str],
    _r: float,
    region: str,
    _bounds: bool = True,
    _showvar: Union[bool, List[str]] = False
    ) -> pd.DataFrame:
    """
    Recalculates the short-term projections (ENSHORT) to be closer to the short-term trend based on the given ratio (_r) assumptions.
    This is particularly useful for countries at the beginning of the list (_countrylist). Countries at the end of the list might need to deviate more to match the regional IAM results.

    NOTE:
    ENSHORT_INIT: short-term projections just based on historical trend, without considering IAM results
    ENSHORT_RATIO: Short-term projection adjusted (linearly, using ratio) to match IAM results.

    We aim at improving our adjusted projection by using different ratios for different countries (to try to minimize the overall mismatch between ENSHORT_RATIO and ENSHORT_INIT).

    We decide how 'close' a country should be to its short-term projection. This is based on the ratio (_r) assumptions.
    The closer the first country is to its own short-term projections, the more other countries will need to deviate from their short-term projections.

    Therefore, the order of the country list (as input to this function) is important.

    Parameters:
    -----------
    _df : pd.DataFrame
        The input dataframe containing the initial short-term and adjusted projections.
    _countrylist : List[str]
        List of country codes to process in order.
    _r : float
        Ratio assumption used to adjust the short-term projections. Should be given as a percentage (e.g., 100 for 100%).
    region : str
        The regional code to exclude from the calculations.
    _bounds : bool, optional
        Whether to apply upper and lower bounds to the projections (default is True).
    _showvar : Union[bool, List[str]], optional
        If False, returns only the adjusted ENSHORT projections. If True, returns the entire dataframe with additional variables.
        If a list of column names is provided, returns only those columns (default is False).

    Returns:
    --------
    pd.DataFrame
        Dataframe containing the adjusted short-term projections and optionally other variables.
    """

    # Exclude rows with ISO == 0 and reset index
    
    try:
        _df = _df[_df.ISO != 0]  # EXCLUDING ISO=0
    except:
        pass
    try:
        _df.reset_index(inplace=True)
    except:
        pass
    try:
        _df.set_index("TIME", inplace=True)
    except:
        pass

    # Global sum of ENSHORT_RATIO excluding the region
    _globsum = _df[_df.ISO != region].ENSHORT_RATIO.groupby("TIME").sum()
    _time = _df.index
    _r = _r / 100

    _df["ENSHORT"] = _df["ENSHORT_RATIO"]  # initializing enshort

    # Initialize variables
    _delta = 0
    _enshort_prev = 0

    if region in _countrylist:
        _countrylist = [c for c in _countrylist if c != region]

    _df["ENSHORT_INIT_ADJ"] = _df["ENSHORT_INIT"]

    for c in _countrylist:
        _weight = _df.loc[_df.ISO == c, "ENSHORT_RATIO"] / (_globsum - _enshort_prev)

        if c != _countrylist[-1]:  # if this is NOT the last country:
            _df.loc[_df.ISO == c, "ENSHORT"] = (
                _df.loc[_df.ISO == c, "ENSHORT_INIT"] * _r
                + _df.loc[_df.ISO == c, "ENSHORT_RATIO"] * (1 - _r)
                + _delta * _weight
            )
        else:  # if this is the last country
            _df.loc[_df.ISO == c, "ENSHORT"] = _df.loc[_df.ISO == c, "ENSHORT_RATIO"] + _delta

        if _bounds:  # Apply bounds
            _df["ENSHORT"] = _df[["LOWER_BOUND", "ENSHORT"]].max(axis=1)
            _df["ENSHORT"] = _df[["UPPER_BOUND", "ENSHORT"]].min(axis=1)

        _delta += _df.loc[_df.ISO == c, "ENSHORT_RATIO"] - _df.loc[_df.ISO == c, "ENSHORT"]
        _enshort_prev += _df.loc[_df.ISO == c, "ENSHORT_RATIO"]

    if _bounds:
        _globsum_max = _df[_df.ISO != region].ENSHORT.groupby("TIME").sum()
        _delta_max = _globsum - _globsum_max
        _df["SLACK_MAX"] = _df["UPPER_BOUND"] - _df["ENSHORT"]
        _df["SLACK_MAX_SUM"] = _df[_df.ISO != region].groupby("TIME")["SLACK_MAX"].transform("sum")
        _df["WEIGHT_MAX"] = _df["SLACK_MAX"] / _df["SLACK_MAX_SUM"]
        _df["DELTA_MAX"] = _delta_max
        _df["ENSHORT"] += _df["DELTA_MAX"] * _df["WEIGHT_MAX"]
        _df["ENSHORT"] = _df[["LOWER_BOUND", "ENSHORT"]].max(axis=1)

        _globsum_min = _df[_df.ISO != region].ENSHORT.groupby("TIME").sum()
        _delta_min = _globsum - _globsum_min
        _df["SLACK_MIN"] = _df["LOWER_BOUND"] - _df["ENSHORT"]
        _df["SLACK_MIN_SUM"] = _df[_df.ISO != region].groupby("TIME")["SLACK_MIN"].transform("sum")
        _df["WEIGHT_MIN"] = _df["SLACK_MIN"] / _df["SLACK_MIN_SUM"]
        _df["DELTA_MIN"] = _delta_min
        _df["ENSHORT"] += _df["DELTA_MIN"] * _df["WEIGHT_MIN"]
        _df["ENSHORT"] = _df[["UPPER_BOUND", "ENSHORT"]].min(axis=1)

    _df["ENSHORT_" + str(_r * 100)] = _df["ENSHORT"]
    _df.loc[2010, "ENSHORT_" + str(_r * 100)] = _df.loc[2010, "ENSHORT_INIT"]

    if not _showvar:
        return _df["ENSHORT_" + str(_r * 100)]
    else:
        return _df[_showvar]


def unmelt(_df, _var):
    """From long to wide format. It return a new df (_df_wide) with the selected variabel (_var)
    _df: your original dataframe
    _var: your selected variable
    """
    setindex(_df, False)  # create a new df(df2) only with the selected variable (_var)
    try:
        _df2 = _df[[_var, "TIME", "ISO"]]
        _df2["TIME"] = _df2["TIME"].astype(int)
        _df_wide = _df2.pivot(
            index="TIME", columns="ISO", values=_var
        )  # unmelt from long to wide format
    except:
        _df2 = _df[[_var, "TIME", "REGION"]]
        _df2["TIME"] = _df2["TIME"].astype(int)
        _df_wide = _df2.pivot(
            index="TIME", columns="REGION", values=_var
        )  # unmelt from long to wide format

    return _df_wide

def load_model_mapping(_model_name, _df_countries, file: IFT):
    """
    Load regional-country mapping (the same used in Pyam)
    It returns an updated df_countries and a list of regions
    """

    # df_pyam_mapping = pd.read_csv(
    #     file, index_col=["ISO"], sep=",", encoding="utf-8"
    # )  # encoding='latin-1')

    df_pyam_mapping = pd.read_csv(
        # "default_mapping.csv", index_col=["ISO"], sep=",", encoding="utf-8"
        # downscaler.CONSTANTS.INPUT_DATA_DIR / "default_mapping.csv",
        # index_col=["ISO"],
        # sep=",",
        # encoding="latin"
        file,
        index_col=["ISO"],
        sep=",",
        encoding="latin",
    )

    # Mapping Model name dictionary (NGFS mapping)
    ditc_model_name = {
        #  "MESSAGEix-GLOBIOM 1.0": "MESSAGE-GLOBIOM",
        "REMIND-MAgPIE 1.7-3.0": "REMIND-MAGPIE",
        "GCAM 4.2": "GCAM4",
        "GCAM 4.4": "GCAM4",
        "GCAM 5.2": "GCAM4",
    }
    if _model_name in ditc_model_name:
        txt = (
            f"We are searching for {ditc_model_name[_model_name]} instead of {_model_name}. \n"
            " Please type `y` if you want to continue with the current behavior (suggested for NGFS project). "
            f"Otherwise please type `n` if you want to use {_model_name} (suggested for other projects)"
        )
        action = input(txt)
        if action.lower() in ["yes", "y"]:
            _model = ditc_model_name[_model_name]
        else:
            _model = _model_name
    else:
        _model = _model_name

    # Creating Region list, Excluding nan REGION
    try:
        region_list = df_pyam_mapping[_model + ".REGION"].unique().tolist()
    except:
        raise ValueError(
            f"Error when reading regional mapping for {_model_name}. Please make sure the project/file name is correct: {file}"
        )
    ## Purpose of the below is to remove any region= na. THIS IS DANGEROUS as not always the last region is na
    # blacklist = [region_list[-1]] ## THIS IS DANGEROUS
    # region_list = [str(e) + "r" for e in region_list if e not in blacklist]

    ## 2021_11_05 Purpose of the below is to remove any region= na  (if region==na it means that type(region)== float instead of string):
    region_list = [str(i) + "r" for i in region_list if type(i) != float]

    # Copying variables to df_countries
    setindex(_df_countries, "ISO")
    _df_countries.loc[:, _model + ".REGION"] = df_pyam_mapping.loc[
        :, _model + ".REGION"
    ]
    _df_countries.loc[:, "R5_region"] = df_pyam_mapping.loc[:, "R5_region"]
    #     _df_countries.loc[:,'REGION']= _df_countries.loc[:, _model+'.REGION'] ## NATIVE REGIONS same regions as pyam
    _df_countries.loc[:, "REGION"] = (
        _df_countries.loc[:, _model + ".REGION"] + "r"
    )  # NATIVE REGIONS same regions as pyam

    _df_countries.loc[:, "IPCC"] = _df_countries.loc[
        :, "R5_region"
    ]  # R5 REGIONS same regions as pyam
    setindex(_df_countries, "COUNTRY_NAME")

    return _df_countries, region_list


def fun_country_map(model, country_mapping_file, pyam_mapping_file):
    df_countries = fun_read_df_countries(country_mapping_file)
    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    return df_countries[["ISO", "REGION"]]


def fun_countrylist(model, project, region):
    country_mapping_file = (
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv"
    )
    pyam_mapping_file = CONSTANTS.INPUT_DATA_DIR / f"{project}" / "default_mapping.csv"
    df_countries = fun_read_df_countries(country_mapping_file)
    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    countrylist = df_countries[df_countries.REGION == region].ISO.tolist()

    if len(countrylist) != 0:
        return countrylist

    elif region.find("|") != -1:
        countrylist = df_countries[
            df_countries.REGION == region.split("|")[1]
        ].ISO.tolist()
        return countrylist
    else:
        return countrylist


def fun_regions(model, project):
    country_mapping_file = (
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv"
    )
    pyam_mapping_file = CONSTANTS.INPUT_DATA_DIR / project / "default_mapping.csv"
    df_countries = fun_read_df_countries(country_mapping_file)
    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    return regions


def fun_country2region(model, c, country_mapping_file, pyam_mapping_file):
    country_dict = fun_country_map(model, country_mapping_file, pyam_mapping_file)
    txt1 = f"Please consider adding it to the `country_dict` in the function `fun_get_iam_regions_associated_with_countrylist`,"
    txt2 = f"as we do for the EU27 and EU28"
    if c not in country_dict.ISO.unique():
        raise ValueError(
            f"{c} country is not available in the default mapping file. {txt1}{txt2}"
        )
    region = country_dict[country_dict.ISO == c]["REGION"][0]
    return region


# NOTE: ssp_model and ssp_scenario should be turned into fixtures
def fun_read_gdpcap(
    c, ssp_file: InputFile, ssp_model="OECD Env-Growth", ssp_scenario="SSP2"
):
    """This function reads projected GDP per capita from SSPs
    It returns a Pd series"""
    df_ssp = pd.read_csv(
        ssp_file.file, sep=",", encoding="utf-8"
    )  # encoding='latin-1')
    df_ssp.rename(columns={"REGION": "ISO"}, inplace=True)

    df_ssp = df_ssp[
        (df_ssp.MODEL == ssp_model)
        & (df_ssp.ISO == c)
        & (df_ssp["SCENARIO"].str.contains(ssp_scenario))
        & (df_ssp.MODEL == ssp_model)
    ]
    # time_col = [str(x) for x in range(2010, 2105, 5)]
    setindex(df_ssp, False)
    df_ssp = df_ssp.melt(["MODEL", "SCENARIO", "UNIT", "VARIABLE", "ISO"])
    df_ssp.rename(columns={"variable": "TIME"}, inplace=True)
    df_ssp["TIME"] = df_ssp["TIME"].astype(int)
    df_ssp = df_ssp[df_ssp.TIME >= 2010]  # only time greater equal than 2010
    setindex(df_ssp, ["TIME", "ISO"])
    df_gdpcap = (
        df_ssp[df_ssp.VARIABLE == "GDP|PPP"].value
        / df_ssp[df_ssp.VARIABLE == "Population"].value
    )

    # setindex(df_ssp, ['ISO','VARIABLE'])
    return df_gdpcap.dropna()  # [[time_col]]


def fun_read_gdp_gdpcap(
    c,
    ssp_file: InputFile,
    ssp_model="OECD Env-Growth",
    ssp_scenario="SSP2",
):
    """This function reads projected GDP per capita from SSPs
    It returns a Pd series"""
    df_ssp = pd.read_csv(
        ssp_file.file,
        sep=",",
        encoding="utf-8",
    )  # encoding='latin-1')
    df_ssp.rename(columns={"REGION": "ISO"}, inplace=True)

    df_ssp = df_ssp[
        (df_ssp.MODEL == ssp_model)
        & (df_ssp.ISO == c)
        & (df_ssp["SCENARIO"].str.contains(ssp_scenario))
        & (df_ssp.MODEL == ssp_model)
    ]
    # time_col = [str(x) for x in range(2010, 2105, 5)]
    setindex(df_ssp, False)
    df_ssp = df_ssp.melt(["MODEL", "SCENARIO", "UNIT", "VARIABLE", "ISO"])
    df_ssp.rename(columns={"variable": "TIME"}, inplace=True)
    df_ssp["TIME"] = df_ssp["TIME"].astype(int)
    df_ssp = df_ssp[df_ssp.TIME >= 2010]  # only time greater equal than 2010
    setindex(df_ssp, ["TIME", "ISO"])
    df_gdpcap = pd.Series(
        df_ssp[df_ssp.VARIABLE == "GDP|PPP"].value
        / df_ssp[df_ssp.VARIABLE == "Population"].value,
        name="GDPCAP",
    )
    df_gdp = pd.Series(df_ssp[df_ssp.VARIABLE == "GDP|PPP"].value, name="GDP|PPP")
    return pd.concat(
        [df_gdp, df_gdpcap],
        axis=1,
    ).dropna()  # df_gdpcap.dropna()  # [[time_col]]


def fun_max_tc(
    beta_short,
    obj_short,
    beta_long,
    x_min: float = 7.5,
    y_min: int = 2040,
    x_max: float = 36,
    y_max: int = 2200,
):
    slope_tc = (y_max - y_min) / (x_max - x_min)
    intercept_tc = y_min - slope_tc * x_min

    if beta_short * beta_long < 0:
        return y_min
    else:
        return max(y_min, min(y_max, round(intercept_tc + slope_tc * obj_short)))


def replace_country_fit_with_regional_fit_if_nan(country_fit, regional_fit):
    if pd.isnull(country_fit):
        return regional_fit["fit_func"]
    else:
        return country_fit


def set_obj_if_missing(x):
    if pd.isnull(x):
        return 0
    else:
        return x


def fun_add_model_name_to_region(df: pd.DataFrame, model: str):
    """[This function add the model to the region name, if none of the region start with the model name ( e.g. MESSAGE|Western Europe).]

    Args:
        df ([pd.DataFrame]): [Dataframe with regional IAMs results]
        model ([str]): [model]

    Returns:
        [pd.DataFrame]: [Updated dataframe]
    """
    ## If none of the region start with the model name, we add it to the region name => e.g. MESSAGE|Western Europe
    if (
        len(df.REGION.str.startswith(str(model) + "|").unique()) == 1
        and df.REGION.str.startswith(str(model) + "|").unique().tolist()[0] == False
    ):  ## This means that none of the region start with the model name e.g. MESSAGE|Western Europe
        df.loc[:, "REGION"] = df["MODEL"].astype(str) + "|" + df["REGION"].astype(str)
    return df[df.MODEL == model]


def fun_match_wildcard(sel_tring, full_list, model_in_region_name, m=""):
    """[summary]

    Args:
        sel_list ([type]): [my full list]
        full_list ([type]): [my selected string (e.g. AFR* )]

    Returns:
        [type]: [description]
    """
    if model_in_region_name:
        return [
            m + "|" + str(i) + "r"
            for i in full_list
            if match_any_with_wildcard(i, sel_tring)
        ]
    else:
        return [
            str(i) + "r" for i in full_list if match_any_with_wildcard(i, sel_tring)
        ]


def fun_historic_data(
    _sector,
    _c,
    df_iea_melt,
    flow_sub_list=False,
    as_percentage=True,
    sum_countries=True,
):
    """
    2020_11_09
    This function returns historical data for a given sector and country.
    If flow_sub_list != False, it returns the data divided by the denominator.
    CAREFUL DOES NOT WORK WITH GDP  => DF_IEA_H??
    It returns a dataframe
    ## TROUBLE SHOOTING - CHECK => DF_IEA_MELT.FLOW
    """
    from downscaler.fixtures import dict_y_den, iea_flow_dict

    sel = iea_flow_dict[_sector]

    if as_percentage:
        sel_den = iea_flow_dict[dict_y_den[_sector]]

    if type(sel[0]) == str:
        sel[0] = [sel[0]]

    if type(sel[1]) == str:
        sel[1] = [sel[1]]

    if sum_countries == True:
        num = (
            fun_pd_sel(
                df_iea_melt[
                    (df_iea_melt.FLOW.isin(sel[0])) & (df_iea_melt.PRODUCT.isin(sel[1]))
                ],
                "",
                _c,
            )["VALUE"]
            .groupby("TIME")
            .sum()
            * 0.041868
            / 1e3
        )
        if flow_sub_list != False:
            fuel = (
                fun_pd_sel(
                    df_iea_melt[
                        (df_iea_melt.FLOW.isin(sel[0]))
                        & (df_iea_melt.PRODUCT.isin(sel[1]))
                        & (df_iea_melt.PRODUCT.isin(flow_sub_list))
                    ],
                    "",
                    _c,
                )["VALUE"]
                .groupby("TIME")
                .sum()
                * 0.041868
                / 1e3
            )

    else:  # ADDED 2020_11_22
        num = (
            fun_pd_sel(
                df_iea_melt[
                    (df_iea_melt.FLOW.isin(sel[0])) & (df_iea_melt.PRODUCT.isin(sel[1]))
                ],
                "",
                _c,
            )["VALUE"]
            .groupby(["TIME", "ISO"])
            .sum()
            * 0.041868
            / 1e3
        )

        if flow_sub_list != False:
            fuel = (
                fun_pd_sel(
                    df_iea_melt[
                        (df_iea_melt.FLOW.isin(sel[0]))
                        & (df_iea_melt.PRODUCT.isin(sel[1]))
                        & (df_iea_melt.PRODUCT.isin(flow_sub_list))
                    ],
                    "",
                    _c,
                )["VALUE"]
                .groupby(["TIME", "ISO"])
                .sum()
                * 0.041868
                / 1e3
            )

    num = (1 / num) ** (-1)
    num = num.replace([np.inf, -np.inf], np.nan).dropna()

    if flow_sub_list == False:
        return num
    else:
        return fuel / num if as_percentage else fuel


def fun_hist_share(df_iea_melt, var, countrylist, year, iea_flow_dict, iam_value):
    """[This function computes the shares of countries within region for a given variable `var` and a given `year`, based on  historical data.
    contained in a `df_iea_melt`.]

    Args:
        df_iea_melt ([type]): [Dataframe with historical data (e.g.  df_iea_melt)]
        countrylist ([type]): [List of countries within region]
        year ([type]): [year (e.g. base year)]
        reg_var : [regional iam results]

    Returns:
        [type]: [description]
    """
    # NOTE: to do:
    # if regional values and sum of historical country-level results have a different sign (e.g. -0.2 / +0.4), at the base year
    # we allocate the difference to the country level by using absolute shares (of countries).
    # Reason being: exporter countries will end up importing fuels (and vice-versa) if we just use historical data.

    iam_value = iam_value / 0.041868 * 1e3
    df = df_iea_melt[
        (df_iea_melt.FLOW.isin(iea_flow_dict[var][0]))
        & (df_iea_melt.PRODUCT.isin(iea_flow_dict[var][1]))
    ]  # NOTE: TO BE EXPANDED!!!!!!!!!!!!!!!!!!

    # NOTE we use the minus here as we look at trade (Net exports)
    reg_value = -df[df.ISO.isin(countrylist)].groupby("TIME").sum().loc[year]

    if iam_value * reg_value["VALUE"] <= 0:
        # We allocate this difference across countries using  absolute shares
        abs_val = np.abs(
            df[(df.ISO.isin(countrylist))]
            .groupby(["TIME", "ISO"])
            .sum()
            .loc[year]["VALUE"]
        )
        if abs_val.sum() == 0:
            # This means we have no historical data. Therefore we do an equal allocation across countries
            abs_val = abs_val + 1 / len(abs_val)

        abs_share = abs_val / abs_val.sum()

        if round(abs_share.sum(), 5) != 1:
            raise Exception(
                f"exceptio 1 sum of abs_share not equal to 1, it is equal to: {abs_share.sum()} instead. Variable: {var}."
            )
        if reg_value["VALUE"] != 0:
            add_value = (reg_value["VALUE"] - iam_value) * abs_share
        else:
            add_value = 0

    else:
        add_value = 0

    # Updating data in case of mismatch with regional IAM results
    country_value = (
        -df[(df.ISO.isin(countrylist))]
        .groupby(["TIME", "ISO"])
        .sum()
        .loc[year]["VALUE"]
        + add_value
    )

    # Calculate shares
    if country_value.sum() == 0:
        # country_value=pd.Series([1/len(country_value) for i in df.ISO.dropna().unique().tolist()], index=df.ISO.dropna().unique().tolist())#1/len(country_value) ## distribute equally across countries
        country_value = country_value + 1 / len(
            country_value
        )  ## distribute equally across countries

    country_share = country_value / (country_value.sum())

    if round(country_share.sum(), 5) != 1:
        # if round(country_share.sum(), 5) != 0:
        raise Exception(
            f"exception 2: sum of country level share is not equal to 1, it is equal to: {country_share.sum()} instead."
        )
    return country_share


def fun_hist_all_data(df_iea_melt, var_list, countrylist, years, iea_flow_dict):
    """[This function computes the shares of countries within region for a given variable `var` and a given `year`, based on  historical data.
    contained in a `df_iea_melt`.]

    Args:
        df_iea_melt ([type]): [Dataframe with historical data (e.g.  df_iea_melt)]
        var_list: List of variables (e.g.  Trade|Primary Energy|Biomass|Volume)
        countrylist ([type]): [List of countries within region]
        years ([type]): [years to be returned]

    Returns:
        [pd.Series]: [historical data for selected  `var_list`, `countrylist` and `years`]
    """

    df_all = pd.DataFrame()
    for var in var_list:
        df = df_iea_melt[
            (df_iea_melt.FLOW.isin(iea_flow_dict[var][0]))
            & (df_iea_melt.PRODUCT.isin(iea_flow_dict[var][1]))
        ]  # NOTE: TO BE EXPANDED!!!!!!!!!!!!!!!!!!
        df["VARIABLE"] = var
        df_all = pd.concat([df_all, df])

    country_value = (
        df_all[(df_all.ISO.isin(countrylist))]
        .groupby(["TIME", "ISO", "VARIABLE"])
        .sum()
        .loc[years]["VALUE"]
        * 0.041868
        / 1e3
    )

    return country_value


def bounds_harmo(_df, _var, _lower_var, _upper_var, region, variable, df_iam):
    """
    This function sets lower/upper bounds on a variable (_var) of a dataframe (_df).
    Then it re-harmonises the data to match IAM results (for sector _s)
    Last Update: 2020_08_28
    """
    # =============================================
    # INTERPRETING UPPER/LOWER  DATA
    # =============================================
    if _lower_var == "":
        _lower_var = 0  ## WE ASSUME LOWER BOUND IS ZERO (UNLESS OTHERWISE INDICATED)
    else:
        if type(_lower_var) != int and type(_lower_var) != float:
            _lower_var = _df[_lower_var]

    if _upper_var == "":
        _upper_var = 1e90  ## THIS IS JUST A HIGH NUMBER
    else:
        if type(_upper_var) != int and type(_upper_var) != float:
            _upper_var = (
                _df[_upper_var] * 0.95
            )  ## we lower the upper bound a bit (e.g. by 5%) => This might be helpful if we use an iterative process (we might need to run this function in a loop multiple times to make sure all bounds are satisfied)

    # =============================================
    # IMPOSING UPPER / LOWER BOUND
    # =============================================
    _df[_var] = _df[_var].clip(_lower_var, _upper_var)

    # =============================================
    # HARMONISATION
    # =============================================
    _country_ratios = (
        fun_pd_sel(_df, "", "")[_var]
        / fun_pd_sel(_df, "", "")[_var].groupby("TIME").sum()
    )  ## country share
    #     print(_country_ratios)
    # commented 2021_01_24
    _df_enshort_value = (
        fun_pd_sel(df_iam, "", region, variable)["VALUE"] * _country_ratios
    ).reset_index()  ## calculating values+ resetting index

    # 2021_01_24 replaced line above with the 2 columns below:
    #     _df_enshort_value=fun_pd_sel(df_iam,'',region,sectors[_s])['VALUE']*_country_ratios
    #     _df_enshort_value=_df_enshort_value.reset_index() ## calculating values+ resetting index

    _df_enshort_value = _df_enshort_value[["TIME", "ISO", 0]]  ## selecting columns

    ## added 2021_01_24
    #     fun_pd_sel(_df_enshort_value,'','' )
    # #     print('_df_enshort_value',_df_enshort_value.index)
    #     _df_enshort_value=_df_enshort_value.drop_duplicates(subset=None, keep='first') ## dropping duplicates

    #     print('_df_enshort_value',_df_enshort_value.index)
    #     print('_df[_var]',_df[_var].index)
    _df[_var] = fun_pd_sel(_df_enshort_value, "", "")[
        0
    ]  ## copy new values in the original df

    return _df[_var].groupby("TIME").sum()


def fun_check_index_data(df_reg: pd.DataFrame, df_downs: pd.DataFrame) -> None:
    init_downs_index_names = ["MODEL", "SCENARIO", "ISO", "VARIABLE", "REGION"]
    df_reg_index = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]

    if len(df_reg) == 0:
        raise ValueError("The `df_reg` is empty. Please provide a non-empty dataframe")
    if df_reg.index.names != df_reg_index:
        raise ValueError(
            f"This function requires the  index names for `df_reg` equal to {df_reg_index}. Current dataframe index names: {df_reg.index.names}"
        )

    if len(df_downs) == 0:
        raise ValueError(
            "The `df_downs` is empty. Please provide a non-empty dataframe"
        )

    if df_downs.index.names != init_downs_index_names:
        raise ValueError(
            f"This function requires the  index names for `df_downs` equal to {init_downs_index_names}. Current dataframe index names: {df_downs.index.names}"
        )


def fun_convert_columns_to_type(
    df_downs, type=int, time_range=range(1990, 2201)
) -> pd.DataFrame:
    """This funcion converts the type of columns (e.g. '2010') to a given `type` e.g. (2010) for all columns within a given `time_range`.
    It returns the updated dataframe.
    """
    init_cols = df_downs.columns
    init_index_names = df_downs.index.names
    if type == str:
        cols: list = [int(x) for x in time_range]
    else:
        cols: list = [str(x) for x in time_range]

    str_columns = [x for x in init_cols if x not in cols]
    df_downs.set_index(str_columns, append=True, inplace=True)

    if type == str:
        df_downs.columns = [str(x) for x in df_downs.columns]
    else:
        df_downs.columns = [int(x) for x in df_downs.columns]
    setindex(df_downs, init_index_names)
    return df_downs


def fun_validation(
    CONSTANTS,
    RESULTS_DATA_DIR,
    selection_dict: dict,
    project_name: str,
    pd_dataframe_or_csv_str: Union[str, Path, pd.DataFrame],
    model_patterns: Union[str, list] = "*MESSAGE*",
    region_patterns: Union[str, list] = "*Western*",
    target_patterns: Union[str, list] = "*h*",
    vars=[
        "Primary Energy|Coal|w/ CCS",
        "Primary Energy|Coal|w/o CCS",
        "Primary Energy|Gas|w/ CCS",
        "Primary Energy|Gas|w/o CCS",
        "Emissions|CO2|Energy",
        "Carbon Sequestration|CCS|Biomass",
        # "Carbon Sequestration|CCS|Fossil",
    ],
    cols: list = [str(x) for x in range(2010, 2055, 5)],  # we check data until 2050
    # harmonize_and_save_to_csv=False,
    harmonize=False,
    save_to_csv=False,
    skip_regions: Union[None, List] = None,
    df_iam_all_models: Union[pd.DataFrame, None] = None,
    no_decimals: int = 1,
    _add_model_name_to_region: bool = False,
) -> Union[None, pd.DataFrame]:
    """
    This function validates downscaled results againts regional IAM results for a given time range `cols` and for a list of variables `vars`. If the sum of country level results will be the same as regional IAMs results it will throw an error.
    Downscaled results `pd_dataframe_or_csv_str` can be provided as a path/str or as a pd.DataFrame.

    If `harmonize`==True it will return the harmonized dataframe in line with regional IAM results.
    If `harmonize`==True and `save_to_csv` ==True, it will also save the results as a csv file (with the file suffix '_VALIDATION_REG_HARMO.csv').

    NOTE:`cols` will be overwritten with the full time series while harmonizing the data
    """
    expected_index_or_colums = [
        "MODEL",
        "SCENARIO",
        "ISO",
        "VARIABLE",
    ]
    expected_index_or_colums_v2 = [
        "MODEL",
        "SCENARIO",
        "REGION",
        "VARIABLE",
    ]

    ## VALIDATION: check if sum of country level data matches regional IAMs results
    print("--------------")
    print("Regional data validation")

    input_file = CONSTANTS.INPUT_DATA_DIR / project_name / "snapshot_all_regions.csv"
    pyam_mapping_file = CONSTANTS.INPUT_DATA_DIR / project_name / "default_mapping.csv"

    region_patterns = convert_to_list(region_patterns)
    model_patterns = convert_to_list(model_patterns)
    target_patterns = convert_to_list(target_patterns)
    pyam_mapping_file = InputFile(
        CONSTANTS.INPUT_DATA_DIR / project_name / "default_mapping.csv"
    )
    if df_iam_all_models is None:
        df_iam_all_models = fun_read_df_iam_all_and_slice(
            model_patterns, target_patterns, input_file, _add_model_name_to_region
        )
        # df_iam_all = df_iam_all_models.copy(deep=True)
        df_iam_all_models.loc[:, "REGION"] = df_iam_all_models.loc[:, "REGION"] + "r"
        df_iam_all_models = df_iam_all_models[df_iam_all_models.VARIABLE.isin(vars)]

    else:
        if list(df_iam_all_models.index.names) != [None]:
            df_iam_all_models.reset_index(inplace=True)

    models = [
        m
        for m in df_iam_all_models.MODEL.unique().tolist()
        if match_any_with_wildcard(m, model_patterns)
    ]

    df_iam_all_models = fun_read_df_iam_all_and_slice(
        model_patterns, target_patterns, input_file, _add_model_name_to_region
    )
    df_iam_all_models.loc[:, "REGION"] = df_iam_all_models.loc[:, "REGION"] + "r"
    df_iam_all_models = df_iam_all_models[df_iam_all_models.VARIABLE.isin(vars)]

    # Main loop
    for model in selection_dict.keys():
        print("--------------")
        print(model)

        df_countries = fun_read_df_countries(
            CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv",
        )
        df_countries, regions = load_model_mapping(
            model, df_countries, pyam_mapping_file.file
        )

        region_mapping = df_countries.set_index("ISO")["REGION"].to_dict()

        if (
            type(pd_dataframe_or_csv_str) == str
            or type(pd_dataframe_or_csv_str) == Path
        ):
            csv_str = pd_dataframe_or_csv_str
            try:
                df_all_scen_short_long = pd.read_csv(
                    RESULTS_DATA_DIR / (csv_str.replace("MODEL", model) + ".csv"),
                    index_col=expected_index_or_colums,
                )
            except:
                df_all_scen_short_long = pd.read_csv(
                    RESULTS_DATA_DIR / (csv_str.replace("MODEL", model) + ".csv")
                )

                df_all_scen_short_long.rename(columns={"REGION": "ISO"}, inplace=True)

                if (
                    harmonize == True
                    and max([len(x) for x in df_all_scen_short_long.ISO]) != 3
                ):
                    raise ValueError(
                        "The function `fun_validation` is trying to rename D.ISO with ISO. This might lead to problems if you want to re-harmonize your dataframe. It seems that  your dataframe contains D.ISO values. Please use a Dataframe without D.ISO -  Aborting"
                    )

                df_all_scen_short_long["ISO"] = [
                    x.replace("D.", "") for x in df_all_scen_short_long.ISO
                ]
                setindex(
                    df_all_scen_short_long, ["MODEL", "SCENARIO", "ISO", "VARIABLE"]
                )

        elif type(pd_dataframe_or_csv_str) == pd.DataFrame:
            df_all_scen_short_long = pd_dataframe_or_csv_str
            if sorted(expected_index_or_colums) == sorted(
                list(
                    set(expected_index_or_colums).intersection(
                        set(df_all_scen_short_long.index.names)
                    )
                )
            ):
                setindex(df_all_scen_short_long, expected_index_or_colums)
            elif sorted(expected_index_or_colums_v2) == sorted(
                list(
                    set(expected_index_or_colums_v2).intersection(
                        set(df_all_scen_short_long.index.names)
                    )
                )
            ):
                df_all_scen_short_long = (
                    (df_all_scen_short_long.reset_index())
                    .rename(columns={"REGION": "ISO"})
                    .set_index(expected_index_or_colums)
                )

            elif sorted(expected_index_or_colums) == sorted(
                list(
                    set(expected_index_or_colums).intersection(
                        set(df_all_scen_short_long.columns)
                    )
                )
            ):
                df_all_scen_short_long = (
                    (df_all_scen_short_long.reset_index())
                    .rename(columns={"REGION": "ISO"})
                    .set_index(expected_index_or_colums)
                )
            elif sorted(expected_index_or_colums_v2) == sorted(
                list(
                    set(expected_index_or_colums_v2).intersection(
                        set(df_all_scen_short_long.columns)
                    )
                )
            ):
                df_all_scen_short_long.rename(columns={"REGION": "ISO"}, inplace=True)

            else:
                raise ValueError(
                    f"The dataframe `pd_dataframe_or_csv_str` should contain {expected_index_or_colums} in the index.names or columns"
                )

            # df_all_scen_short_long = df_all_scen_short_long.loc[model]
        else:
            raise ValueError(
                "`pd_dataframe_or_csv_str` must be either a pd.DataFrame or a string/path"
            )

        if (
            type(pd_dataframe_or_csv_str) == str
            or type(pd_dataframe_or_csv_str) == Path
        ):
            print(RESULTS_DATA_DIR / f"{csv_str}.csv")

        setindex(df_all_scen_short_long, False)
        df_all_scen_short_long.loc[:, "MODEL"] = [
            x.replace("_downscaled", "") for x in df_all_scen_short_long.MODEL
        ]

        if model in df_all_scen_short_long.MODEL.unique():
            setindex(df_all_scen_short_long, ["MODEL", "SCENARIO", "ISO", "VARIABLE"])

            ## Below we remove lines that contain ('MODEL', 'SCENARIO', 'ISO', 'VARIABLE'):
            
            if (
                len(
                    fun_xs(df_all_scen_short_long, {"MODEL":"MODEL"})
                )
                > 0
            ):
                df_all_scen_short_long.drop(
                    df_all_scen_short_long.loc[
                        df_all_scen_short_long.index.names
                    ].index,
                    inplace=True,
                )

            # df_all_scen_short_long["REGION"] = [
            #     f"{model}|{region_mapping[x.replace('D.','')]}"
            #     for x in df_all_scen_short_long.index.get_level_values("ISO")
            # ]

            df_all_scen_short_long["REGION"] = [
                f"{model}|{region_mapping[x.replace('D.','')]}"
                if x in region_mapping
                else "not available"
                for x in df_all_scen_short_long.index.get_level_values("ISO")
            ]

            df_all_scen_short_long.set_index("REGION", append=True, inplace=True)
            for var in vars:
                if (
                    var
                    not in df_all_scen_short_long.index.get_level_values(
                        "VARIABLE"
                    ).unique()
                ):
                    print(
                        f"{var} not present in downscaled results - we skip this variable"
                    )
                elif var not in df_iam_all_models.VARIABLE.unique():
                    print(
                        f"{var} not present in `df_iam_all_models` - we skip this variable"
                    )
                else:
                    df_iam = df_iam_all_models.set_index(
                        ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT", "TIME"],
                        append=True,
                    ).unstack("TIME")["VALUE"]
                    df1 = (
                        df_iam.xs(var, level="VARIABLE")
                        .xs(model, level="MODEL", drop_level=False)
                        .groupby(["MODEL", "SCENARIO", "REGION"])
                        .sum()
                        .drop(2005, axis=1)
                    ).fillna(0)
                    time_steps_columns = [
                        x for x in pd.Series(df1.columns).diff().iloc[1:].unique()
                    ]

                    if len(time_steps_columns) > 1:
                        # NOTE This means uneven time steps in df1:
                        # e.g. 2055,2060,2070,2080 ... (e.g. as in the NGFS (2021) project)
                        # In this case we drop columns than only contain np.na
                        # We do not always apply (e.g. NGFS 2022 project), otherwise there might be a mismatch between columns in df1 and df2
                        df1 = df1.dropna(how="all", axis=1)

                    df2 = (
                        df_all_scen_short_long.xs(var, level="VARIABLE")
                        .groupby(["MODEL", "SCENARIO", "REGION"])
                        .sum()
                    )
                    if "UNIT" in df2.columns:
                        df2=df2.drop("UNIT", axis=1)
                    df2.columns.name = "TIME"
                    # try:
                    #     df1.columns = df2.columns
                    # except:
                    #     aa=1
                    df1.columns = [str(int(float(x))) for x in df1.columns]
                    df2.columns = [str(int(float(x))) for x in df2.columns]
                    regions_list = df2.index.get_level_values("REGION").unique()
                    df1 = df1[df1.index.get_level_values("REGION").isin(regions_list)]
                    # df1.columns = df2.columns
                    print(var)
                    try:
                        assert_frame_equal(
                            np.round(
                                df1.loc[df2.index, cols].xs(model, drop_level=False),
                                no_decimals,
                            ),
                            np.round(
                                df2[cols].xs(model, drop_level=False), no_decimals
                            ),
                            check_less_precise=no_decimals + 1,
                        )
                    except:  ## we check for which region we get an error
                        if harmonize:
                            print(f"{var} - we have harmonized the results")

                            ## version 2
                            cols: list = [str(x) for x in range(2010, 2105, 5)]

                            if (
                                fun_get_variable_unit_dictionary(df_iam)[var] == "EJ/yr"
                                and "trade" not in var.lower()
                            ):
                                if "UNIT" in df_all_scen_short_long.columns:
                                    df_all_scen_short_long = (
                                        df_all_scen_short_long.drop("UNIT", axis=1)
                                    )
                                # Constrained harmonization for energy variable (greater than zero)
                                df_all_scen_short_long = fun_constrained_harmonization(
                                    var,
                                    df_iam.reset_index().set_index(
                                        [
                                            "MODEL",
                                            "SCENARIO",
                                            "REGION",
                                            "VARIABLE",
                                            "UNIT",
                                        ]
                                    ),
                                    df_all_scen_short_long,
                                    min_threshold=1e-6,
                                )
                            else:
                                df_all_scen_short_long = fun_reg_harmo_single_variable_all_regions_targets_negative_values(
                                    var,
                                    df_iam.reset_index().set_index(
                                        [
                                            "MODEL",
                                            "SCENARIO",
                                            "REGION",
                                            "VARIABLE",
                                            "UNIT",
                                        ]
                                    ),
                                    df_all_scen_short_long,
                                )

                        else:
                            regions = list(
                                df2.index.get_level_values("REGION").unique()
                            )

                            if skip_regions is not None:
                                [regions.remove(x) for x in skip_regions]

                            for region in regions:
                                print(var, f" - {region}")
                                df1_sub = np.round(
                                    df1.xs(
                                        f"{region}", level="REGION", drop_level=False
                                    ),
                                    no_decimals + 1,
                                ).fillna(0)
                                df2_sub = np.round(
                                    df2.xs(
                                        f"{region}", level="REGION", drop_level=False
                                    ),
                                    no_decimals + 1,
                                )
                                common_idx = pd.MultiIndex.from_tuples(
                                    list(
                                        set(df2_sub.index).intersection(
                                            set(df1_sub.index)
                                        )
                                    )
                                )

                                try:
                                    assert_frame_equal(
                                        df1_sub.loc[common_idx, cols].xs(
                                            model, drop_level=False
                                        ),
                                        df2_sub.loc[common_idx, cols].xs(
                                            model, drop_level=False
                                        ),
                                        check_less_precise=no_decimals + 1,
                                    )
                                except:  ## we print the two dataframes and we raise error
                                    print(df1_sub[cols])
                                    print(df2_sub[cols])
                                    assert_frame_equal(
                                        df1_sub.loc[df2_sub.index, cols].xs(
                                            model, drop_level=False
                                        ),
                                        df2_sub[cols].xs(model, drop_level=False),
                                        check_less_precise=no_decimals + 1,
                                        check_dtype=False,
                                    )
            if harmonize and save_to_csv:
                df_all_scen_short_long.to_csv(
                    RESULTS_DATA_DIR
                    / Path(
                        csv_str.replace("MODEL", model) + "_VALIDATION_REG_HARMO.csv"
                    )
                )
            if harmonize:
                return df_all_scen_short_long
            else:
                print(
                    "Validation done: Sum of downscaled country-level data coincides with regional IAM results"
                )
                return df_all_scen_short_long
        else:
            print(
                f"we could not find {model} in your downscaled dataframe - we skip this model"
            )


def fun_sel_iam(selection_dict: dict, df_iam_all_models: pd.DataFrame) -> pd.DataFrame:
    """This function slices the df_iam_all_models based on selection_dict. It returns the updated dataframe df_iam_all_models"""
    res = {}
    list = ["regions", "targets"]
    for l in list:
        # suffix = "r" if l == "regions" else ""
        mylist = []
        for k in selection_dict.keys():
            mylist += selection_dict[k][l]
        # mylist = [m + suffix for m in mylist]
        res[l] = unique(mylist)
    print(res)

    df_iam_all_models = df_iam_all_models[
        (df_iam_all_models.REGION.isin(res["regions"]))
        & (df_iam_all_models.SCENARIO.isin(res["targets"]))
        & (df_iam_all_models.MODEL.isin(selection_dict.keys()))
    ]
    return df_iam_all_models


def fun_make_var_dict(
    keys_list: list,
    values_list: list,
    demand_dict: bool = True,
    main_level: str = "Final Energy",
) -> dict:
    """
    Create dictionary `new_var_dict` that contains a list of new variables (not available in the previously downscaled results) to be created as the sum of energy carriers.
     `sector_main` is the new variable that we want to create for all sectors defined in 'keys_list', as the sum of energy carriers defined in `values_list`
     `keys_list` defines the list SECTORS. This will be new_var_dict.keys()
           # Example: keys_list = ['Industry']
     `values_list` defines the list of ENERGY CARRIERS. This will be new_var_dict.values()
           # Example: values_list= ['Liquids', 'Solids', ...]
     `new_var_dict` is the dictionary of the new variable name (new_var_dict.keys()) defined as the a of variables (listed in new_var_dict.values())
           # Example:  new_var_dict = {'Final Energy|Industry': ['Final Energy|Industry|Liquids', 'Final Energy|Industry|Solids', ...]}

    Returns:
        [dict]: [Dictionary with variables to be harmonized]
    """

    new_var_dict = {}

    sector_main = f"{main_level}|SECTOR"
    sector_list = [f"{main_level}|SECTOR|{ec}" for ec in values_list]

    new_var_dict = {
        (sector_main.replace("SECTOR", x)): [
            s.replace("SECTOR", x) for s in sector_list
        ]
        for x in keys_list
    }

    ## Creating list of variables to be harmonized (this will Include 'Final Energy')
    var_dict = new_var_dict.copy()
    var_dict[f"{main_level}"] = [
        f"{main_level}|SECTOR|".replace("SECTOR|", s) for s in keys_list
    ]

    var_dict = dict(
        OrderedDict(reversed(list(var_dict.items())))
    )  ##   Reverse dictionary order starting from 'Final Energy'

    if demand_dict:
        return var_dict, new_var_dict
    for keys, values in var_dict.items():
        if keys != f"{main_level}":
            var_dict[keys] = [
                x.split("|")[0] + "|" + x.split("|")[2] + "|" + x.split("|")[1]
                for x in values
            ]
    return var_dict


## Growth rate
def fun_growth_index(
    _df: pd.DataFrame,
    _base_year: Union[str, int],
    _max_growth_compared_to_median: int = 10,
) -> pd.DataFrame:
    """This function returns the dataframe using indexed growth rates (where base_year values=1).
    By default, we assume that growth rates (in each country) cannot exceed 10 times the median growth rate (`_max_growth_compared_to_median`)

    """
    # NOTE: To calculate percentage change (year by year) you can use:
    # _df.pct_change(axis=1, periods=1).
    # Periods=1 means use previous value to calculate % change

    growth = _df.iloc[:, :].div(_df[_base_year], axis=0)

    # We calculate the median growth rates, and take the maximum median across all the time periods:
    median_growth = growth.median().max()

    # We assume that maximum growth is (by default) 1000 times the median growth rate
    max_growth = _max_growth_compared_to_median * median_growth
    return np.minimum(growth, max_growth)


def fun_df_iea_all(CONSTANTS, IEA_fuel_dict: dict) -> pd.DataFrame:
    """Create dataframe with IEA data. It returns the dataframe"""
    df_iea_all = fun_read_df_iea_all(
        CONSTANTS.INPUT_DATA_DIR / "Extended_IEA_en_bal_2019_ISO.csv"
    )  # Reading IEA data
    df_iea_all["IAM_FUEL"] = df_iea_all["PRODUCT"]  # IEA DATA
    df_iea_all = df_iea_all.replace(
        {"IAM_FUEL": IEA_fuel_dict["FUEL"]}
    )  ## IEA DATA: ADDING IAM_FUEL with standardised fuel name
    return df_iea_all


def fun_reg_harmo_single_variable_all_regions_targets_negative_values(
    var: str,
    df_reg: pd.DataFrame,
    df_downs_all_var: pd.DataFrame,
    idx_allow_adj: Union[None, list, str, tuple] = None,
) -> pd.DataFrame:
    """
    This function harmonizes previously downscaled results `df_downs_all_var` with
    regional IAM results `df_reg` for a given `var`. It harmonizes one single variable
    `var` for one single model, for all regions and targets.

    It is also possible to apply adjustements only for a subset of the dataframe, as
    defined in the index `idx_allow_adj`. Please note that this can lead to negative
    values. If you want to keep values above/below a given threshold please use the
    function `fun_constrained_harmonization`.
    Example: apply adjustments only to Canada:
        # idx_allow_adj=dfnew.xs('CAN', level='ISO', drop_level=False).index

    This function differs from `fun_reg_harmo_single_variable_all_regions_targets` as
    it allocates the difference between sum of country level results and regional IAMs
    using absolute values in downscaled results. Therefore this function differs from
    `fun_reg_harmo_single_variable_all_regions_targets` only if our variable `var` can
    go below zero. Otherwise the results will be the same.

     NOTE:
     The standard harmonization function
     `fun_reg_harmo_single_variable_all_regions_targets` can lead to three artifacts if
      `var` is not greater than zero:
     1) The sign of countries can be swapped.
        Example:
            - country1 = 3
            - country2 = -3.01
            - sum of countries = -0.01
            - regional iam results = 0.01

            If we apply the standard function
            `fun_reg_harmo_single_variable_all_regions_targets` we get:
            - country1= -3
            - country2= 3.01

            With the current function we preserve the original sign, we get:
            - country1= 3.01
            - country2= -3.00

    2) Data can be artificially magnified.
        Example:
            - country1 = 0.99
            - country2 = -1
            - sum of countries = -0.01
            - regional iam results = 1

            If we apply the standard function
            `fun_reg_harmo_single_variable_all_regions_targets` we get:
            - country1= -99
            - country2= 100

            With the current function we get:
            - country1= 1.49
            - country2= -0.49

    3) We get division by zero errors if the sum of country level results == 0.
        Example:
            - country1 = 1
            - country2 = -1
            - sum of countries = 0
            - regional iam results = 1

            If we apply the standard function
            `fun_reg_harmo_single_variable_all_regions_targets` we get:
            - country1= np.inf
            - country2= np.inf

            With the current function we get:
            - country1= 1.5
            - country2= -0.5
    It returns the updated dataframe.
    """
    orig_df_downs_all_var = df_downs_all_var.copy(deep=True)
    add_unit_later = False
    if "UNIT" in df_downs_all_var.index.names:
        add_unit_later = True
        unit_col = df_downs_all_var.reset_index("UNIT").UNIT.copy()
        df_downs_all_var = df_downs_all_var.droplevel("UNIT")

    # Check if the region in the `df_downs_all_var` is present in the `df_reg`
    check_region = df_downs_all_var.reset_index().REGION.unique()[0]
    if check_region not in df_reg.reset_index().REGION.unique():
        # NOTE `check_region` is not available in the IAMs results. (but we have not checked yet for which variable the regions is missing)
        # Therefore we check if there are regions in common in the two datasets (df_reg and df_downs_all_var) that contain the variable `var`.
        # If this is the case, then `var` must be missing for `check_region` (but is available for others -> we skip this region). Otherwise we raise an error
        regions_df_reg = df_reg.xs(var, level="VARIABLE").reset_index().REGION.unique()
        regions_df_downs_all_var = df_downs_all_var.reset_index().REGION.unique()
        # We skip block below temporarily for NGFS 2024 First round
        if not len(set(regions_df_reg).intersection(regions_df_downs_all_var)):
            raise ValueError(
                f"{check_region} is present in `df_downs_all_var`. However it is not present in the regional IAM results `df_reg`. "
                "Please check your region name in the `df_downs_all_var`."
            )
        print(
            f"{var} is not present in the IAMs results for {check_region}."
            f"It is only available for: {regions_df_reg}. We skip this region"
        )
        return orig_df_downs_all_var
    # With the line below this function not modify the dataframe inplace
    df_downs_all_var = df_downs_all_var.copy(deep=True)

    fun_check_index_data(df_reg, df_downs_all_var)

    groupby_index = ["MODEL", "SCENARIO", "VARIABLE", "REGION"]

    if var not in df_downs_all_var.index.get_level_values("VARIABLE"):
        raise ValueError(
            f"We could not find the variable {var} in the dataframe `df_downs_all_var` "
        )

    if var not in df_reg.index.get_level_values("VARIABLE"):
        raise ValueError(
            f"We could not find the variable {var} in the regional IAMs results dataframe `df_reg`"
        )

    df_downs = df_downs_all_var.xs(var, level="VARIABLE", drop_level=False)

    if len(df_downs.index.get_level_values("MODEL").unique()) > 1:
        raise ValueError(
            f'`df_downs` should contain only one model. Your dataframe contains {df_downs.index.get_level_values("MODEL").unique()} '
        )

    model = df_downs.index.get_level_values("MODEL").unique()[0]
    # if 2005 in df_reg.columns:
    df_reg_sel = (
        df_reg.xs(var, level="VARIABLE", drop_level=False)
        .xs(model, level="MODEL", drop_level=False)
        # .groupby(["MODEL", "SCENARIO", "REGION", "VARIABLE"])
        .groupby(groupby_index)
        .sum()
        # .drop(2005, axis=1)
    )

    df_downs = fun_convert_columns_to_type(df_downs, type=int)
    df_reg_sel = fun_convert_columns_to_type(df_reg_sel, type=int)

    df_sum_iam = df_reg_sel.groupby(groupby_index).sum()
    df_sum_country = df_downs.groupby(groupby_index).sum()

    ## Common index and cols
    cols = list(set(df_sum_country.columns).intersection(df_sum_iam.columns))
    idx = list(set(df_sum_iam.index).intersection(df_sum_country.index))
    cols.sort()

    # Difference between IAMs and sum of country level results
    diff = df_sum_iam.loc[idx] - df_sum_country.loc[idx]

    if idx_allow_adj is None:
        idx_allow_adj = df_downs.index
    df_down_sel = df_downs[df_downs.index.isin(idx_allow_adj)][cols]

    msg = (
        f"We cold not find any match between your selected variable {var} and your selected index `idx_allow_adj`. "
        f"If you do not want to apply adjustments to a specific subset of the dataframe, please select `idx_allow_adj=None`."
    )
    if idx_allow_adj is None and (df_down_sel.index == df_downs.index).all():
        raise ValueError(msg)
    if len(df_down_sel) == 0:
        raise ValueError(msg)
    ## Compute country_ratio (weights) within region, using absolute values
    country_ratio = (
        np.abs(df_down_sel) / np.abs(df_down_sel).groupby(groupby_index).sum()
    )
    country_ratio = fun_fill_na_with_previous_next_values(country_ratio)

    ## Allocate `diff`  based on `country_ratio` (and Update results in `df_downs`)
    allocate_diff = diff * country_ratio
    setindex(allocate_diff, df_downs.index.names)
    idx_diff = allocate_diff.index
    new_val = df_downs.loc[idx_diff, cols].fillna(0) + allocate_diff.loc[idx_diff, cols]
    df_downs.loc[idx_diff, cols] = new_val.loc[idx_diff, cols]

    # NOTE: the below silent the error `A value is trying to be set on a copy of a slice from a DataFrame` but messes up with the unit
    # df_updated = pd.concat(
    #     [df_downs[~df_downs.index.isin(new_val.index)], new_val], axis=0
    # )
    # Convert columns to time_type (str or int)
    # df_downs = fun_convert_time_to_type(df_downs_all_var, df_updated)

    df_downs = fun_convert_time_to_type(df_downs_all_var, df_downs)

    # Set same unit as IAMs data (because we harmonized the results to match regional
    # IAMs results)
    unit = df_reg.xs(var, level="VARIABLE").index.get_level_values("UNIT").unique()[0]
    if add_unit_later:
        df_downs.loc[:, "UNIT"] = unit
        df_downs_all_var["UNIT"] = unit_col
    # Copy updated values to df_downs_all_var (which contains all variables, not just
    # our selected `var`)
    idx = df_downs.index
    df_downs_all_var.loc[idx] = df_downs.loc[idx]
    if add_unit_later:
        df_downs_all_var = df_downs_all_var.set_index("UNIT", append=True)
    return df_downs_all_var


def fun_convert_time_to_type(df_downs_all_var, df_downs):
    time_type = str
    check_time_float = [x for x in range(2010, 2050) if x in df_downs_all_var.columns]
    if len(check_time_float) > 0:
        time_type = int
    return fun_convert_columns_to_type(df_downs, type=time_type)


def fun_constrained_harmonization(
    var: str,
    df_reg: pd.DataFrame,
    df_downs_all_var: pd.DataFrame,
    idx_allow_adj: Union[None, list, str, tuple] = None,
    min_threshold=-np.inf,
    max_threshold=np.inf,
) -> pd.DataFrame:
    """
    This function shuold be used if you want to harmize a subset of the dataframe, while
    at the same time introducing min/max thresholds (to your selected variable 'var').
    It applies changes to 1 single variable `var`, 1 single model, all regions and all
    targets. All the other variables will be not affected.

    It returns the update dataframe.
    """
    # Step 0 - Check input data
    fun_check_index_data(df_reg, df_downs_all_var)
    if var not in df_downs_all_var.index.get_level_values("VARIABLE"):
        raise ValueError(
            f"We could not find the variable {var} in the dataframe `df_downs_all_var` "
        )
    if len(df_downs_all_var.index.get_level_values("MODEL").unique()) > 1:
        raise ValueError(
            f'`df_downs_all_var` should contain only one model. Your dataframe contains {df_downs_all_var.index.get_level_values("MODEL").unique()} '
        )
    if var not in df_reg.index.get_level_values("VARIABLE"):
        raise ValueError(
            f"We could not find the variable {var} in the regional IAMs results dataframe `df_reg`"
        )

    # Step 1 -  We harmonize selected variables
    df = df_downs_all_var.copy(deep=True)
    df = fun_reg_harmo_single_variable_all_regions_targets_negative_values(
        var, df_reg, df, idx_allow_adj
    )

    # Step 2 - We add minimum and maximum constraints to your selected `var` (only for
    # that variable , the rest of the datraframe will not change).
    # This could break regional IAM harmonization.
    clip_index = df.xs(var, level="VARIABLE", drop_level=False).index
    df.loc[clip_index, :] = df.loc[clip_index, :].clip(min_threshold, max_threshold)

    # Step 3 - We re-harmonize the selected variable `var`, without using idx_allow_adj
    df = fun_reg_harmo_single_variable_all_regions_targets_negative_values(
        var, df_reg, df
    )

    return df


def fun_df_iea_melt(
    df_iea_all: pd.DataFrame, sectors_required: list, time_range_list: list
) -> pd.DataFrame:
    """Create dataframe with melted IEA data. It takes as input the df_iea_all, and returns the melted dataframe"""
    df_iea_sel = df_iea_all[df_iea_all.FLOW.isin(sectors_required)]
    df_iea_melt = pd.DataFrame()
    df_iea_melt = df_iea_sel.melt(
        id_vars=["COUNTRY", "FLOW", "PRODUCT", "IAM_FUEL", "ISO"],
        value_vars=time_range_list,
    )
    df_iea_melt.rename(columns={"variable": "TIME", "value": "VALUE"}, inplace=True)
    df_iea_melt["VALUE"] = pd.to_numeric(df_iea_melt["VALUE"], errors="coerce")
    df_iea_melt["TIME"] = pd.to_numeric(df_iea_melt["TIME"], errors="coerce")
    return df_iea_melt


def fun_read_down_NOT_harmo(
    PREV_STEP_RES_DIR, model, _region, _target, step1b, _file_suffix
):
    """Reading previously downscaled data. It returns a dataframe"""
    _csv_suff = "_harmo" if step1b else ""
    try:
        f_name = f"{model}_{_region}_{_file_suffix}{_csv_suff}.csv"
        _df = pd.read_csv(PREV_STEP_RES_DIR / f_name, sep=",", encoding="latin-1")
        _df = _df[_df.TARGET == _target]
    except:
        f_name = f"{_region}_{_file_suffix}{_csv_suff}.csv"
        f_name = f_name.replace("|", "_")  ## Replacing separator 2021_05_08
        # except:
        #     # Use harmo results if available, otherwise wo_harmo
        #     f_harmo=f_harmo=PREV_STEP_RES_DIR  / f"{_region.replace('|', '_')}_{_file_suffix}{_csv_suff}.csv"
        #     f_wo_harmo=PREV_STEP_RES_DIR  / f"{_region.replace('|', '_')}_{_file_suffix}{_csv_suff.replace('_harmo','')}.csv"
        #     if os.path.isfile(f_harmo):
        #         f_name = f_harmo
        #     elif os.path.isfile(f_wo_harmo):
        #         f_name = f_wo_harmo  ## Replacing separator 2021_05_08
        #         txt='we are using final energy results not harmonized - do you wish to continue? '
        #         print(txt)
        #         action = msvcrt.getch()
        #         if action.lower().decode() not in ["yes", "y"]:
        #             raise ValueError(f"Simulation aborted by the user (user input={action})")
        #     print("running...")
        _df = pd.read_csv(PREV_STEP_RES_DIR / f_name, sep=",", encoding="latin-1")
        _df = _df[_df.TARGET == _target]

    return _df


def fun_check_if_duplicated_sectors_in_step1(df):
    """This function checks if the dataframe contains 1 functional form, 1 target and that
    does not contain duplicated results (multiple sectors for a given country in a given time period).
    """

    pick_a_country = df.ISO.unique()[0]
    len_sector = len(df.SECTOR.unique())
    len_df = len(df[df.ISO == pick_a_country].loc[2010])
    target = df.TARGET.unique()
    func = df.FUNC.unique()

    if len(func) != 1:
        raise ValueError(
            f"We found {len(func)} functional forms in the dataframe: {func}. We need only 1 functional form here"
        )

    if len(target) != 1:
        raise ValueError(
            f"We found {len(target)} target in the dataframe: {target}. We need only 1 target here"
        )

    if len_sector != len_df:
        raise ValueError(
            f"Dataframe contains multiple sectorial results for functional form {func} and target {target}: {len(df[df.ISO==pick_a_country].loc[2010].SECTOR)}"
        )


def fun_load_downs_data_from_step1_and_reshape(
    PREV_STEP_RES_DIR,
    c: str,
    model: str,
    var_list: str,
    __file_suffix: str,
    target: str,
    step1b: bool,
    _func: str = "log-log",
):
    """
    This function loads existing downscaled data from step1 and reshapes the dataframe.
    """
    region_name = fun_region_name(model, c)
    ## reading the data (not harmonised) from csv data
    dfa = pd.DataFrame()
    dfa = fun_read_down_NOT_harmo(
        PREV_STEP_RES_DIR,
        model,
        region_name.replace("|", "_"),
        target,
        step1b,
        __file_suffix,
    )
    setindex(dfa, "TIME")
    if _func in dfa.FUNC.unique():
        dfa = dfa[dfa.FUNC == _func]

    else:
        raise ValueError(
            f"{_func} functional form not found in step1. Please choose one of the following: {dfa.FUNC.unique()}"
        )
    dfa.index = dfa.index.astype(int)
    
    # Check if we have duplicated sectors
    check=dfa[dfa.METHOD=='wo_smooth_enlong'] if 'METHOD' in dfa.columns else dfa
    fun_check_if_duplicated_sectors_in_step1(check)

    my_df_all = pd.DataFrame()
    for var in var_list:
        selcols=["ISO","SECTOR"]
        selcols=["ISO","METHOD", "SECTOR"] if 'METHOD'in dfa.columns else selcols
        mydf = dfa[[var]+selcols]  # without slicing dataframe
        mydf = mydf.set_index(selcols, append=True).unstack()[var]
        # renaming columns data as in the df_main
        mydf = mydf.rename(columns={col: col + var for col in mydf.columns})
        my_df_all = pd.concat([my_df_all, mydf], axis=1)
    my_df_all.columns.names = [None]
    return my_df_all

def fun_load_data_from_step1_and_modify_column_names(
    PREV_STEP_RES_DIR,
    region,
    model,
    file_suffix,
    target: str,
    step1b: bool,
    var_list: str,
    _func: str = "log-log",
) -> pd.DataFrame:

    """
    This function loads data from step1 for a given `model`, `target` ,  `var`(ENSHORT_REF/ENLONG_RATIO) and countrylist `c_list`.
    It returs a dataframe with reshaped dataframe and renamed columns.
    """
    # Below we Load ENSHORT_REF, ENLONG_RATIO for all sectors
    _df_sect = fun_load_downs_data_from_step1_and_reshape(
        PREV_STEP_RES_DIR,
        region,
        model,
        var_list,
        file_suffix,
        target,
        step1b,
        _func=_func,
    )
    _df_sect = _df_sect.loc[:, _df_sect.columns != False]
    return fun_pd_sel(_df_sect, "", "")



def fun_adjust_column_names_df_desired(
    df_desired: pd.DataFrame,  # IAM_fuel_dict: dict
) -> pd.DataFrame:
    """
    This function renames the columns of the dataframe `df_desired`, based on the IAM_fuel_dict.
        It returns the updated daframe
    """
    from downscaler.fixtures import IAM_fuel_dict

    new_dict = {
        v: f"Secondary Energy|Electricity{k}ENSHORT_REF"
        for k, v in IAM_fuel_dict.items()
    }
    fun_pd_sel(df_desired, "", "")

    fuel_list = new_dict.keys()
    # df_desired = df_desired[fuel_list]
    return df_desired.rename(columns=new_dict)  # .drop(
    # "Secondary Energy|ElectricityENSHORT_REF", axis=1
    # )


def fun_sel_sectors(
    string: str,
    sectors: list,
    allowed_sub_sectors_list: Union[None, list],
    sep: str = "|",
) -> list:
    """
    This function returns all sectors containing 'string'.
    We select the 'main' sector (as the one which contains the lowest numbet of
    delimeter '|'). It returns a list of lists:fun_sel_sectors(string)
    1) The first list contains the main sector. If we could not find a main sector,
     this will be equal to False
    2) The second list contains all the sub-sectors (of the next energy level, to
    avoid double counting) for a given main sector (but only if contained in a
    predefined `allowed_sub_sectors_list`, unless this is None)
    """
    mylist = [x.split("|") for x in sectors if string in x]
    len_list = [len(x) for x in mylist]
    main_s = [sep.join(x) for x in mylist if len(x) == min(len_list)]
    sub_s = [sep.join(x) for x in mylist if len(x) == min(len_list) + 1]
    if allowed_sub_sectors_list is not None:
        sub_s = [
            x for x in sub_s if x.split("|")[min(len_list)] in allowed_sub_sectors_list
        ]
    return [main_s] + [sub_s]


def fun_sub_sectors(var, region_name, target, df_iam_all_models, allowed_sub_sectors):
    """
    This function finds sub_sectors (or fuels) based on data availability for a given region/model (region_name) and target.
    It works for all energy levels (Final, Secondary, Primary)
    It finds the sub_sectors for the next delimiter '|', in order to avoid double counting.
    It returns a list
    This function differs from fun_sel_sectors:
    - fun_sel_sectors: returns both the main sector and the sub_sectors. only works for Final Energy level (it is based on  sectors list)
    - This function instead returns only the sub_sectors (or the main sector if we could not find the sub_sectors) and works for all energy levels (it is based on df_iam_all_models data).
    Therefore this function checks for data availaibility (that depend on selected scenario/model).
    Example. if var='Secondary Energy|Liquids' it will return:
            ['Secondary Energy|Liquids|Biomass',
            'Secondary Energy|Liquids|Coal',
            'Secondary Energy|Liquids|Oil']
    NOTE:
    In the example above the returned list excludes 'Secondary Energy|Liquids|Biomass|w/ CCS' as it already accounted in 'Secondary Energy|Liquids|Biomass'
    """
    setindex(df_iam_all_models, False)
    sects_nov = df_iam_all_models[
        (df_iam_all_models.VARIABLE.str.startswith(var))
        & (df_iam_all_models.REGION == region_name)
        & (df_iam_all_models.SCENARIO == target)
    ].VARIABLE.unique()
    ## 2020_11_06
    sects_nov = fun_sel_sectors(var, list(sects_nov), allowed_sub_sectors)[1]

    if len(sects_nov) != 0:
        counter = 0
        counter = [i.count("|") for i in sects_nov]
        counter_min = min(counter)

        n = counter_min + 1
        new_list = []
        for i in sects_nov:
            groups = i.split("|")
            joined = "|".join(groups[:n]), "_".join(groups[n:])
            new_list = new_list + [joined[0]]

        return unique(new_list)

    else:
        return [var]  # we return the input list (if there are no sub-sectors)


def fun_hydrogen_downs(
    _df,
    df_iam_all_models,
    model,
    region,
    targets,
    var_iam="Final Energy",
    _var_list=None,
    long_term='ENLONG_RATIO',
):
    """
    MODIFIED:2020_11_26
    This function downscales hydrogen by appling the same  regional share of hydrogen/electricity to all countries.
    It takes as input a dataframe (_df) and a list of reference variables (_var_list, e.g. ['ENSHORT_REF','ENLONG_RATIO'])
    It returns a Dataframe.
    """
    if _var_list is None:
        _var_list = ["ENSHORT_REF", long_term]

    # List of na data for hydrogen (e.g. we do not have 'R_SQUARED', 'BETA' etc.)
    my_na_list = list(
        set(_df.columns)
        - set(_var_list)
        - {"Population", "GDP|PPP", "COUNTRYLIST", "FUNC", "OPT_RATIO", "GDP"}
    )

    if region not in df_iam_all_models.REGION.unique():
        region = region[:-1]
        if region not in df_iam_all_models.REGION.unique():
            raise ValueError(
                f"Unable to find {region} in df_iam_all_models. These are the regions available {df_iam_all_models.REGION.unique()} "
            )
    for target in targets:
        # Step 0 - regional data for each target
        h2 = fun_read_reg_value(
            model, region, target, var_iam + "|Hydrogen", df_iam_all_models
        )["VALUE"]

        el = fun_read_reg_value(
            model, region, target, var_iam + "|Electricity", df_iam_all_models
        )["VALUE"]

        h2_to_el = h2 / el  ## regional Hydrogen ratio

        # Step 1 we get country-level "Final Energy|Electricity" and rename it as 'Hydrogen'
        df_hydrogen = _df.xs(var_iam + "|Electricity", level="SECTOR", drop_level=False)
        df_hydrogen = df_hydrogen.xs(target, level="TARGET", drop_level=False)
        df_hydrogen = df_hydrogen.rename(
            {var_iam + "|Electricity": var_iam + "|Hydrogen"}
        )

        # Step 2 we multiply it by the regional share of hydrogen (for each _var_list)
        for _var in _var_list:
            df_hydrogen.loc[:, _var] = df_hydrogen.loc[:, _var] * h2_to_el
            df_hydrogen.loc[:, my_na_list] = np.nan
        _df = pd.concat([_df, df_hydrogen])
    return _df


# fun_secondary_fuels_enlong(models[0], target, region, 'Final Energy|Solids')

# ### ENLONG (Final and Secondary)
#
# This function makes long-term projections for both Final and Secondary Energy levels.
#
# ##### Step 1 - Calculating Fuel mix
# First we calculate the fuel mix for the energy carrier, based on IAMs results.
# (this can either happen at final or secondary energy levels - see note below).
# We do this by calling the function: fun_secondary_fuels_enlong(model, target, region, var_iam).
# This will return the fuel share and also a list of fuels for that energy carrier (var_iam) e.g. Final Energy|Solids
#
# ##### Step 2 - Calculating conversion (from Final to Secondary)
# Then we calculate the conversion from Final to Secondary. This conversion captures both:
# - regional efficiency losses (same regional efficiency)
# - trade with other regions (implicit assumption is that trade will be allocated proportionally to all countries)
#
# <img src="image/CONV_enlong.png" width="500" height="300">
#
# ##### Step 3.1 - Calculating Final Energy
# We multiply the fuel mix (based on regional data), by Final Energy (previously downscaled at the counrty level).
# note: we already have Final Energy at the country level (previously donwscaled). We don't have  Secondary energy yet.
#
# <img src="image/FEN_enlong.png" width="500" height="300">
#
#
# ##### Step 3.2 - Calculating Secondary Energy (by using conversion from Final to Secondary)
# Then we do the same for secondary energy (by mutiplying by the conversion factor as calculated in step 2)
#
# <img src="image/SEN_enlong.png" width="500" height="300">
#
#
# ##### Note:
# In order to control for where the fuels split happens (either at final or secondary energy level) we have two blocks:
# - if f.startswith("Final") => Fuel split happens in Final. e.g. [Final Energy|Solids|Coal, Final Energy|Solids|Biomass]
# - elif f.startswith("Secondary") => Fuel split happens in Secondary e.g. [Secondary Energy|Transportation|Oil, Secondary Energy|Transportation|Biomass]
#
# Depending on where the fuel split happens we change the names of the variables accordingly
#
#


def fun_secondary_fuels_enlong(
    model: str,
    target: str,
    region: str,
    var: str,
    df_iam_all_models: pd.DataFrame,
    allowed_sub_sectors: list,
    var_to_be_removed: Union[None, List] = None,
) -> Union[pd.DataFrame, list]:
    """
    Regional proportion share of secondary fuels (e.g. oil) within energy carries (e.g. Liquids),
    based on regional iam results.
    It returns a pd.DataFrame and a list of fuels (if applicable)
    """
    region_name = fun_region_name(model, region)
    fuel_list = fun_sub_sectors(
        var, region_name, target, df_iam_all_models, allowed_sub_sectors
    )

    if var_to_be_removed is None:
        var_to_be_removed = [
            "Secondary Energy|Electricity|Fossils",
            "Secondary Energy|Electricity|Fossil",
            "Secondary Energy|Electricity|Non-Biomass Renewables",
            "Secondary Energy|Electricity|Storage Losses",
            "Secondary Energy|Electricity|Transmission Losses",
            "Secondary Energy|Liquids|Fossil",
        ]
    for x in var_to_be_removed:
        if x in fuel_list:
            fuel_list.remove(x)

    fuel = df_iam_all_models[
        (df_iam_all_models.VARIABLE.isin(fuel_list))
        & (df_iam_all_models.REGION == region_name)
        & (df_iam_all_models.SCENARIO == target)
    ]
    if len(fuel):
        tot = fuel.groupby("TIME").sum()
        fuel_pivot = fuel.pivot(index="TIME", columns="VARIABLE", values="VALUE")
        fuel_share = pd.DataFrame()
        for i in fuel_pivot.columns:
            fuel_share.loc[:, i] = fuel_pivot.loc[:, i] / tot["VALUE"]

        if len(fuel_share):
            setindex(fuel_share, "TIME")
            return fuel_share.loc[2010:], fuel_list

    return pd.DataFrame()


def fun_secondary_and_final(
    var_iam: str,
    model: str,
    region: str,
    target: str,
    df_all: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    allowed_sub_sectors: list,
    df_iea_melt: pd.DataFrame,
    verbose=False,
    _var="ENLONG_RATIO",
    long_term='ENLONG_RATIO',
    _base_year=2010,
) -> pd.DataFrame:
    """
    This function makes long-term projections for both Final and Secondary Energy levels.
    """
    col1 = df_all.columns
    fuel_mix_share, fuel_list = fun_secondary_fuels_enlong(
        model, target, region, var_iam, df_iam_all_models, allowed_sub_sectors
    )

    countrylist = df_all.index.get_level_values("ISO").unique()

    # NOTE we need these imports (iea_oil ... iea_gases) below as they are used when
    # calculating fuel = set(eval(("iea_" + ec)..
    from downscaler.fixtures import (
        iea_biomass,
        iea_coal,
        iea_gas,
        iea_gases,
        iea_liquids,
        iea_natural_gas,
        iea_oil,
        iea_other,
        iea_solids,
        list_of_ec,
        list_of_fuels,
        list_of_sectors,
    )

    # Step1 fuel mix and list of fuels
    if _var == "ENSHORT_REF":
        fuel_mix_share = None

        # e.g. var_iam : 'Final Energy|Residential and Commercial|Solids' already present in iea_flow_dict
        my_ec = [x for x in list_of_ec if var_iam.find(x) != -1]
        ec = eval(f"iea_{my_ec[0].lower()}".replace(" ", "_"))
        main = fun_historic_data(
            # We replace `ec` otherwise var_iam may be not present in `iea_flow_dict.keys()`
            var_iam.replace(f"|{ec}", ""),
            countrylist.tolist(),
            df_iea_melt,
            flow_sub_list=ec,  # Only Energy carrier
            sum_countries=False,
            as_percentage=False,
        )

        fuel_mix_share = pd.DataFrame(index=df_all.index)
        for f in fuel_list:
            # e.g ["Biomass", "Solids"][0], we just take "Biomass". Ec always come after fuels
            my_fuel = [x for x in list_of_fuels if f.find(x) != -1]

            # eval(f"iea_{my_fuel[0].lower()}".replace(" ", "_"))
            try:
                fuel = list(
                    set(
                        eval(f"iea_{my_fuel[0].lower()}".replace(" ", "_"))
                    ).intersection(eval(f"iea_{my_ec[0].lower()}".replace(" ", "_")))
                )
            except:
                print("Error in using `eval` to calcute `fuel`")
            ## consider creating a new function

            sub = fun_historic_data(
                # We replace `ec` otherwise var_iam may be not present in `iea_flow_dict.keys()`
                var_iam.replace(f"|{ec}", ""),
                countrylist.tolist(),
                df_iea_melt,
                flow_sub_list=fuel,  # Intersect energy carrier and fuel
                sum_countries=False,
                as_percentage=False,
            )

            # share of fuel in main sector at the base year for each country

            if len(sub) > 0 and len(main) > 0:
                share = (sub / main).fillna(0).loc[_base_year]
                share_single_var = (
                    pd.DataFrame(
                        share.to_dict(),
                        index=df_all.index.get_level_values("TIME").unique(),
                    )
                    .stack()
                    .sort_index()
                )
                fuel_mix_share[f] = share_single_var
            else:
                print("problem here")
                fuel_mix_share[f] = 1e-9
        # print("should be it for enshort_ref")

    if (
        _var == long_term
        or fuel_mix_share.sum().sum() == 0
        or fuel_mix_share is None
    ):
        try:
            fuel_mix_share, fuel_list = fun_secondary_fuels_enlong(
                model, target, region, var_iam, df_iam_all_models, allowed_sub_sectors
            )
        except:
            fuel_mix_share = 0
            fuel_list = [""]

    # Step 2 Calculating conversion from Final to Secondary (regional data)
    secondary_reg = (
        fun_read_reg_value(model, region, target, fuel_list, df_iam_all_models)
        .groupby("TIME")
        .sum()["VALUE"]
    )
    final_reg = fun_read_reg_value(
        model,
        region,
        target,
        var_iam.replace("Secondary", "Final"),
        df_iam_all_models,
    )["VALUE"]
    conv = secondary_reg / final_reg
    # this includes both regional efficiencies and trade (equally distributed across
    # all countries)

    ## Step 3 Multiply by conversion rate
    if type(fuel_mix_share) != int:
        for f in fuel_list:
            if f.startswith("Final"):
                # This means fuel list is available at Final energy level
                try:
                    ## Step 3.1 Final Energy = Final (country-level)  x fuel mix share (reg level)
                    df_all[f + _var] = df_all[var_iam + _var] * fuel_mix_share[f]

                    ## Step 3.2 Calculating Secondary Energy by using conversion ratio (conv) at the regional level
                    df_all[f.replace("Final", "Secondary") + _var] = (
                        df_all[var_iam + _var] * fuel_mix_share[f] * conv
                    )

                except:
                    print(f, " not working")

            elif f.startswith("Secondary"):
                # This means fuel list is available at Secondary energy level
                try:
                    # below we change names accordingly as f starts with 'Secondary'.
                    # Step 3.1 Final Energy =
                    #  Final (country-level) x fuel mix share (reg level)
                    df_all[f.replace("Secondary", "Final") + _var] = (
                        df_all[var_iam.replace("Secondary", "Final") + _var]
                        * fuel_mix_share[f]
                    )
                    # Step 3.2 Calculating Secondary Energy by using conversion ratio
                    # (conv) at the regional level. (note that country-level demand is
                    # available only at Final Energy level. This is why we multiply
                    # by conv)
                    df_all[f + _var] = (
                        df_all[var_iam.replace("Secondary", "Final") + _var]
                        * conv
                        * fuel_mix_share[f]
                    )

                except:
                    print(f, " not working")
            else:
                print("Energy Level of ", f.upper(), "is not Final nor Secondary")

    # step4 regional harmonization (we need this only for enshort_ref)
    # if _var == "ENSHORT_REF":
    # reg_var = fun_read_reg_value(
    #     model, region, target, fuel_list, df_iam_all_models
    # )["VALUE"]
    for f in fuel_list + [var_iam]:
        try:
            if f in df_iam_all_models.VARIABLE.unique():
                reg_var = fun_read_reg_value(
                    model, region, target, f, df_iam_all_models
                )["VALUE"]
            else:
                # If a variable is not reported by IAMs we use the ENLONG_RATIO results
                # as regional data
                reg_var = df_all[f"{f}{long_term}"].groupby("TIME").sum()
            flag_na_data = False
            if _var == "ENSHORT_REF":
                true_if_na = (
                    df_all[f"{f}{_var}"]
                    .groupby("TIME")
                    .sum()
                    .replace({0: np.nan})
                    .isna()
                    .unique()
                ).tolist()

                if True in true_if_na:
                    if len(true_if_na) == 1:
                        # No data for ENSHORT_REF: use ENLONG_RATIO instead
                        df_all.loc[:, f"{f}{_var}"] = df_all.loc[:, f"{f}{long_term}"]
                    else:  # we have data but na is present in data series
                        # fill na with previous time period value if we have no data for
                        # enshort ref
                        df_all.loc[:, f"{f}{_var}"] = df_all.loc[:, f"{f}{_var}"].ffill(
                            axis=0
                        )

            ratio = reg_var / df_all[f"{f}{_var}"].groupby("TIME").sum()
            df_all.loc[:, f"{f}{_var}"] = df_all.loc[:, f"{f}{_var}"] * ratio

        except:
            print(f"harmo did not work for {f}")

    col2 = df_all.columns
    if verbose:
        print("new variables created by `fun_secondary_and_final`:")
        print([x for x in col2 if x not in col1])
    return df_all


def fun_secondary_conv(
    model: str, target: str, region: str, var: str, df_iam_all_models: pd.DataFrame
) -> Union[pd.Series, float]:
    """
    This function provides the conversion from FINAL to SECONDARY energy, for a given variable (var) based on regional IAMs results.
    We ignores trade of secondary products.
    Var can be: Solids, Liquids, Gases, Electricity
    It returns a series.
    """

    region_name = fun_region_name(model, region)

    var_sec = var.rsplit("|")[0] + "|" + var.rsplit("|")[1]

    setindex(df_iam_all_models, "TIME")
    secondary = df_iam_all_models[
        (
            df_iam_all_models.VARIABLE
            == var.replace("Primary", "Secondary").replace("Final", "Secondary")
        )
        & (df_iam_all_models.REGION == region_name)
        & (df_iam_all_models.SCENARIO == target)
    ]

    final = df_iam_all_models[
        (
            df_iam_all_models.VARIABLE
            == var_sec.replace("Primary", "Final").replace("Secondary", "Final")
        )
        & (df_iam_all_models.REGION == region_name)
        & (df_iam_all_models.SCENARIO == target)
    ]
    try:
        conv = (
            fun_pd_sel(secondary, "", region_name)["VALUE"]
            / fun_pd_sel(final, "", region_name)["VALUE"]
        ).unstack()[region_name]

        if len(conv.dropna(inplace=False)) != 0:
            return conv
        ## If we did not exit the function it means we have n/a data for this region/variable. in this case we assume conv=1
        print("ASSUMPTION:", var)
        return conv.fillna(1)  ## we return ratio =1
    except Exception:
        print(
            var.replace("Primary", "Secondary"),
            "secondary",
            secondary[["VARIABLE", "VALUE"]],
        )
        print(
            var_sec.replace("Primary", "Final"),
            "final",
            final[["VARIABLE", "VALUE"]],
        )


def fun_electricity_efficiencies(
    model: str, target: str, region: str, var: str, df_iam_all_models: pd.DataFrame
) -> Union[pd.DataFrame, float]:
    """
    This function returns the electricity efficiencies based on regional IAMs results.
    If IAMs data are missing we assume 1/33% efficiency assumption for all fuels except gas (1/50% assumption).
    """
    if "|Electricity" not in var:
        raise Exception(
            "This function works only if variables contains string |Electricity , input= ",
            var,
        )

    _primary = fun_read_reg_value(
        model,
        region,
        target,
        var.replace("Final", "Primary").replace("Secondary", "Primary"),
        df_iam_all_models,
    )["VALUE"]

    _secondary = fun_read_reg_value(
        model,
        region,
        target,
        var.replace("Primary", "Secondary").replace("Final", "Secondary"),
        df_iam_all_models,
    )["VALUE"]

    if len(_primary.dropna()) == 0:
        var_primary = (
            var.replace("Secondary", "Primary")
            .replace("Final", "Primary")
            .rsplit("|")[0]
            + "|"
            + var.replace("Secondary", "Primary")
            .replace("Final", "Primary")
            .rsplit("|")[1]
            + "|"
            + var.replace("Secondary", "Primary")
            .replace("Final", "Primary")
            .rsplit("|")[2]
        )
        _primary = fun_read_reg_value(
            model, region, target, var_primary, df_iam_all_models
        )["VALUE"]

    if len(_secondary.dropna()) == 0:
        ## Below creating Variable name for secondary energy:
        var_sec = (
            var.replace("Primary", "Secondary")
            .replace("Final", "Secondary")
            .rsplit("|")[0]
            + "|"
            + var.replace("Primary", "Secondary")
            .replace("Final", "Secondary")
            .rsplit("|")[2]
            + "|"
            + var.replace("Primary", "Secondary")
            .replace("Final", "Secondary")
            .rsplit("|")[1]
        )

        _secondary = fun_read_reg_value(
            model, region, target, var_sec, df_iam_all_models
        )["VALUE"]

    if len(_primary) != 0:
        return _primary / _secondary
    else:
        return (
            1 / 0.5
            if "Electricity|Gas" in var or "Electricity|Natural Gas" in var
            else 3
        )


import numpy as np
import pandas as pd

def fun_rand_elc_criteria(
    _fuel_list: List[str] = None, 
    _columns: List[str] = None, 
    __seed: int = None
) -> pd.DataFrame:
    """
    This function generates a random electricity criteria DataFrame for various energy sources.
    It creates random values for each fuel type and category (e.g., cost criteria, governance),
    normalizes them to ensure the sum of values for each fuel is 1, and adjusts specific entries
    based on predefined rules.

    Parameters
    ----------
    _fuel_list : list[str], optional
        A list of fuel types for which random criteria are generated, by default None.
        If None, a default list of common fuels is used.
        
    _columns : list[str], optional
        A list of column names representing criteria categories (e.g., cost, governance), 
        by default None. If None, a default list is used.
        
    __seed : int, optional
        A seed value for reproducibility of random numbers, by default None. If provided,
        the random seed will be set to this value.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing random weights for each fuel type and criteria category.
        The sum of each fuel's weights across categories is normalized to 1.
    """

    # Default fuel types if none provided
    if _fuel_list is None:
        _fuel_list = [
            "SOL", "WIND", "BIO", "HYDRO", "COAL", 
            "GAS", "OIL", "GEO", "NUC"
        ]

    # Default columns/criteria if none provided
    if _columns is None:
        _columns = [
            "df_cost_criteria", "df_gw_all_fuels", 
            "df_base_year_share", "df_gov"
        ]

    # Set random seed if provided
    np.random.seed()
    if __seed is not None:
        np.random.seed(__seed)

    # Create random DataFrame with fuel types as columns and criteria as rows
    df = pd.DataFrame(
        {i: np.random.rand(len(_columns)) for i in _fuel_list}, 
        index=_columns
    )

    # Adjust specific criteria for certain fuels
    df.loc["df_cost_criteria", ["COAL", "GAS", "OIL", "GEO", "NUC"]] = 0
    df.loc["df_gw_all_fuels", ["SOL", "WIND", "BIO", "HYDRO", "NUC"]] = 0
    df.loc["df_gov", ["BIO", "HYDRO", "COAL", "GAS", "OIL", "GEO"]] = 0

    # Round the random values to 3 decimal places
    df = df.round(3)

    # Normalize the values across each fuel type so that their sum is 1
    df = df / df.sum(axis=0)

    # Transpose, reset index, and rename columns for final output
    df = df.T
    df = df.reset_index()
    df = df.rename(columns={"index": "IAM_FUEL"})
    
    return df



def fun_get_fen(_df_all: pd.DataFrame, f2: str, ec: str, var: str) -> pd.Series:
    """Get final energy for a given fuel and energy carrier. It returns a pd.Series"""
    _fen = _df_all.iloc[
        :,
        (_df_all.columns.str.contains(f2))
        & (_df_all.columns.str.contains("Final Energy"))
        & (_df_all.columns.str.contains(ec))
        & (_df_all.columns.str.contains(var)),
    ].sum(axis=1)

    if np.abs(_fen).sum() == 0:
        _fen = _df_all.iloc[
            :,
            (_df_all.columns.str.contains(f2))
            & (_df_all.columns.str.contains("Final Energy"))
            & (_df_all.columns.str.contains(var)),
        ].sum(axis=1)

    if np.abs(_fen).sum() == 0:
        _fen = _df_all.iloc[
            :,
            (_df_all.columns.str.contains("Final Energy"))
            & (_df_all.columns.str.contains(var)),
        ].sum(axis=1)
    return _fen


def fun_from_final_to_secondary_enshort_combi(
    var_iam: str,
    model: str,
    region: str,
    target: str,
    _df_all: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    allowed_sub_sectors: list,
    _var="ENSHORT_REF",
) -> pd.DataFrame:
    """2021_02_11 Update: Function can  be applied to all secondary energy energy carriers.
    This function converts final energy (by fuel) into secondary energy.
    We only need this function for SHORT-TERM projections.
    It returns the updated df_all dataframe
    if use_ec == False, we search for fuel used in the subsectors
    """
    ## Step 0 Energy carrier
    ec = var_iam.rsplit("|")[1]

    ## Step1 Get list of fuels for this variable (var_iam)
    if ec == "Solids":
        ## If ec=solids, we get the List of fuels from final energy level
        var_iam = var_iam.replace("Secondary", "Final")

    try:
        fuel_list = fun_secondary_fuels_enlong(
            model, target, region, var_iam, df_iam_all_models, allowed_sub_sectors
        )[1]
    except Exception:
        fuel_list = [""]

    ## Step 2 Calculate variable as sum of fuels
    for f in fuel_list:
        try:
            ## Calculating Secondary Energy as the sum of Final energy values
            f2 = f.rsplit("|")[-1]
            my_reg_var = f.replace("Final", "Secondary")
            myvar = my_reg_var + _var

            fen = fun_get_fen(_df_all, f2, ec, _var)
            _df_all[f.replace("Final", "Secondary") + _var] = fen

            # Regional harmonization
            share = _df_all[myvar] / _df_all[myvar].groupby("TIME").sum()
            reg_var = fun_read_reg_value(
                model, region, target, my_reg_var, df_iam_all_models
            )["VALUE"]
            _df_all.loc[:, myvar] = share * reg_var

        except Exception as e:
            print(f, f" ****** ERROR *** not working! Details: {e}")
    return _df_all


def fun_apply_secondary_mix_to_final_sectors(
    df_all: pd.DataFrame, ec: str, var: str = "ENLONG_RATIO", sectors: list = None, long_term="ENLONG_RATIO"
) -> pd.DataFrame:
    """
    2020_11_29
    This function calculates sectorial final energy mix, based on fuel mix from secondary energy
    NOTE:We only need this for ENSHORT_REF
    """

    df_res = df_all.copy(deep=True)
    if sectors is None:
        sectors = ["Transportation", "Industry", "Residential and Commercial"]
    fuel = df_all.iloc[
        :,
        (
            df_all.columns.str.startswith(f"Secondary Energy|{ec}")
            & df_all.columns.str.contains(var)
        ),
    ]
    cols = fuel.columns
    tot = df_all.iloc[
        :,
        (
            df_all.columns.str.startswith(f"Secondary Energy|{ec}")
            & df_all.columns.str.contains(var)
        ),
    ].sum(axis=1)

    for s in sectors:
        tot_sector = df_all.iloc[
            :,
            df_all.columns.str.startswith("Final Energy")
            & df_all.columns.str.contains(ec)
            & df_all.columns.str.contains(s)
            & df_all.columns.str.contains(var),
        ]

        for f in cols:
            pos = f.count("|")
            try:
                var = "Final Energy|" + s + "|" + ec + "|" + f.rsplit("|")[pos]
            except:
                print("problem here")
            # NOTE example: df_all['Final Energy|Industry|Liquids|BiomassENLONG_RATIO']=Secondary Energy|Liquids|BiomassENLONG_RATIO/"Secondary Energy|LiquidsENLONG"*Final Energy|Industry|LiquidsENLONG_RATIO
            # In other words FEN|Industry|Liquids|Bio = % of bio in secondary liquids   X   FEN|Industry|Liquids
            df_res[var] = fuel[f] / tot * tot_sector.iloc[:, 0]

            # Harmonization based on elong_ratio
            # NOTE IAMs do not provide data for these variables. (e.g.'Final Energy|Transportation|Liquids|GasENSHORT_REF )
            # Therfore we take regional data from sum of country level results based on ENLONG_RATIO
            var_long = var.replace("ENSHORT_REF", long_term)
            ratio = (
                df_res[var_long].groupby("TIME").sum()
                / df_res[var].groupby("TIME").sum()
            )
            df_res.loc[:, var] = df_res.loc[:, var] * ratio
    return df_res


def fun_primary_conv(
    model: str, target: str, region: str, var: str, df_iam_all_models: pd.DataFrame
) -> Union[float, pd.DataFrame]:
    """
    This function tries to get Electricity efficiency data using function fun_electricity_efficiencies().
    Otherwise we use an efficiency =1
    It returns either a data series or a number
    """
    try:
        eff = fun_electricity_efficiencies(
            model, target, region, var, df_iam_all_models
        )
    except Exception:
        eff = 1
    return eff


def fun_primary_energy(
    model: str,
    region: str,
    target: str,
    df_all: pd.DataFrame,
    var: str,
    df_iam_all_models: pd.DataFrame,
    long_term='ENLONG_RATIO'
) -> pd.DataFrame:
    """This function creates primary energy variables based on results from secondary energy variables. It returns the updated dataframe"""

    f_list = [
        "Coal",
        "Oil",
        "Gas",
        "Biomass",
        "Nuclear",
        "Geothermal",
        "Hydro",
        "Solar",
        "Wind",
    ]

    ## WE Read secondary Energy values
    for f in f_list:
        new_df = pd.DataFrame()
        new_df = df_all.iloc[
            :,
            df_all.columns.str.startswith("Secondary Energy")
            & df_all.columns.str.contains(var)
            & ~(
                df_all.columns.str.contains("Industry")
                | df_all.columns.str.contains("Residential and Commercial")
            )
            & df_all.columns.str.contains(f + "EN")
            # This ('+EN') avoids problem of Gas vs Natural Gas in Secondary Energy|Electricity.
            # In this way we distinguish between energy carrier (Gases) and fuel (GasENSHORT/ENLONG)
        ]

        for col in new_df.columns:
            ## we convert secondary to primary by multiplying by a  conversion  factor
            new_df.loc[:, col] = new_df.loc[:, col] * (
                fun_primary_conv(
                    model,
                    target,
                    region,
                    col.replace("ENSHORT_REF", "").replace(long_term, ""),
                    df_iam_all_models,
                )
            )

        ## We sum all secondary energy fuels (converted to primary by using a conversion factor)
        df_all.loc[:, "Primary Energy|" + f + var] = new_df.sum(axis=1)

        # Sort index
        df_all = df_all.sort_index()

        # Fill zero values
        myvar = "Primary Energy|" + f + var
        primary = df_all.loc[:, myvar].unstack().replace(0, np.nan).copy()
        primaryfill = fun_fill_na_with_previous_next_values(primary, _axis=0).stack()
        df_all.loc[:, myvar] = primaryfill

        ## We adjust results TO MATCH IAM RESULTS (PRIMARY ENERGY BY FUEL)
        reg_val = fun_read_reg_value(
            model, region, target, "Primary Energy|" + f, df_iam_all_models
        )
        if len(reg_val) == 0:  ## then "Non Biomass Renewables'
            reg_val = fun_read_reg_value(
                model,
                region,
                target,
                "Primary Energy|" + "Non-Biomass Renewables|" + f,
                df_iam_all_models,
            )

        ratio_adj = df_all.loc[:, myvar] / df_all.loc[:, myvar].groupby("TIME").sum()

        try:
            df_all.loc[:, myvar] = ratio_adj * reg_val["VALUE"]
        except:
            df_all.loc[:, myvar] = ratio_adj * fun_drop_duplicates(reg_val["VALUE"])
    return df_all


def filter_country(_fuel, _df_temp, _threshold=0.4):
    """This function provides a list of countries, with share of  agiven fuel in the electricity mix >= _threshold

    Args:git
        _fuel ([type]): [Selected fuel]
        _df_temp ([type]): [dataframe]
        _threshold (float, optional): [Electriciy threshold]. Defaults to 0.4.


    Returns:
        [list]: [List of countries with _fuel above _threshold ]
    """
    _df_temp["FILTER"] = _df_temp[_fuel] / _df_temp["DEMAND"]

    ## create a new variabel % share of fuel in elec mix
    clist_sel = (
        _df_temp[_df_temp["FILTER"] >= _threshold]
        .index.get_level_values("ISO")
        .unique()
    )  ## iso with % above threshold
    return clist_sel.tolist()


def adjust_country_with_low_share(
    _f: str,
    _df_all: pd.DataFrame,
    df_desired: pd.DataFrame,
    model: str,
    target: str,
    df_iam_all_models: pd.DataFrame,
    region: str,
    __threshold: float = 0.5,
    _var="ENSHORT_REF",
    step1: bool = True,
    step2: bool = True,
) -> pd.DataFrame:
    """This function harmonises results to match IAM data, for a given fuel _f, based on some conditions:
    - we first harminse countries with a share of _f below a given threshold _threshold at the base year.
    - then we harmonise all countries to match IAM results

    Args:
        _f ([str]): [selected fuel e.g. (Hydro, Geothermal)]
        _df_all ([pd.DataFrame]]): [dataframe that contains the downscaled data]
        __threshold (float, optional): [% Share in fuel mix (threshold)]. Defaults to 0.3.

    Returns:
        [pd.DataFrame]]: [dataframe with updated results]
    """
    ## Select countries above/below threshold:
    if _f == "Geothermal":
        ## Countries with high share of fuel _f at the base year
        clist_sel = filter_country("GEO", df_desired, _threshold=__threshold)
    else:
        ## Countries with high share of fuel _f
        clist_sel = filter_country(_f.upper(), df_desired, _threshold=__threshold)

    clist_sel_opposite = (
        _df_all[~_df_all.index.get_level_values("ISO").isin(clist_sel)]
        .index.get_level_values("ISO")
        .unique()
        .tolist()
    )

    # Sum across countries with low share of fuel _f:
    sum_country_low_share = (
        fun_pd_sel(_df_all, "", clist_sel_opposite)[
            f"Secondary Energy|Electricity|{_f}" + _var
        ]
        .groupby("TIME")
        .sum()
    )

    # Current sum across all countries
    current_sum = (
        fun_pd_sel(_df_all, "", "")[f"Secondary Energy|Electricity|{_f}" + _var]
        .groupby("TIME")
        .sum()
    )

    # Regional data
    reg_sum = fun_read_reg_value(
        model, region, target, f"Secondary Energy|Electricity|{_f}", df_iam_all_models
    )["VALUE"]

    # Step 1 we allocate delta to countries with low share of fuel _f (only if we need to decrease hyrdo/geothermal => delta_to_be_allo<0 ):
    if step1:
        delta_to_be_allo = reg_sum - current_sum  # delta to be allocated

        # Select time when delta_to_be_allo is below zero (this is when we want to keep hydro/geoth above zero)
        time_delta_below_zero = delta_to_be_allo[
            delta_to_be_allo < 0
        ].index.tolist()  # Select time when delta_to_be_allo is below zero (this is when we want to keep hydro/geoth above zero)
        if 2010 in time_delta_below_zero:
            # remove the base year (proportional adjustments across all countries)
            time_delta_below_zero.remove(2010)

        # delta to be allo cannot lead to negative results, therefore we clip it
        delta_to_be_allo = np.maximum(delta_to_be_allo, -sum_country_low_share * 0.9)

        index_clist_sel_opposite = fun_pd_sel(_df_all, "", clist_sel_opposite).index
        delta_to_countries = (
            fun_pd_sel(_df_all, "", clist_sel_opposite)[
                f"Secondary Energy|Electricity|{_f}" + _var
            ]
            / sum_country_low_share
        ) * delta_to_be_allo

        ## Apply Adjustement
        _df_all.loc[
            index_clist_sel_opposite, f"Secondary Energy|Electricity|{_f}" + _var
        ] = (
            _df_all.loc[
                index_clist_sel_opposite, f"Secondary Energy|Electricity|{_f}" + _var
            ]
            + delta_to_countries
        )

        # Clip to avoid data below zero
        _df_all.loc[
            index_clist_sel_opposite, f"Secondary Energy|Electricity|{_f}" + _var
        ] = _df_all.loc[
            index_clist_sel_opposite, f"Secondary Energy|Electricity|{_f}" + _var
        ].clip(
            1e-9, np.inf
        )

    # Step 2 We harmonise again the data (if there is still a mismatch) in all countries to make sure we match regional IAMs results ( e.g. if delta is above zero, or if countries cannot afford adjustments, or if data=1e-9 in some individual countries to avoid data below zero)
    if step2:
        current_sum = (
            fun_pd_sel(_df_all, "", "")[f"Secondary Energy|Electricity|{_f}" + _var]
            .groupby("TIME")
            .sum()
        )

        _df_all[f"Secondary Energy|Electricity|{_f}" + _var] = (
            _df_all[f"Secondary Energy|Electricity|{_f}" + _var] * reg_sum / current_sum
        )

    return fun_pd_sel(_df_all, "", "")


# ### Heat Equations
#
# This function downscales Final Energy heat (Residential and Commercial and Industry) for both long-term and short term projections.
# The rational behind this function is that Heat is a substitute for Electricity (higher electricity means less need for heat).
#
# As usual we apply two criteria for the downscaling.
# The ENSHORT criteria is based on observed historical data.
# The ENLONG criteria is entirely based on regional IAM results.
#
#
# #### ENSHORT:
# If we have  obserbed historical data for heat, we apply the equtions below:
# <img src="image/Heat_short.png" width="500" height="300">
#
# In case historical heat is =0 in all countries, we apply the ENLONG equation (also for enshort),please see below
#
# #### ENLONG:
# For the long-term projections we apply the same Heat/Electricity ratio that we observe at the regional level.
# <img src="image/Heat_long.png" width="500" height="300">


# ### Iterative adjustments
# Here we create a function to adjust fuels (or sub-sectors), in order to match both:
# - Previously downscaled Total Energy Carriers (at the country level) and
# - Regional IAMs results at the fuel (or sub-sectors) level.
#
# We apply this function only for short-term projections ('ENSHORT_REF').
# By definition, long-term projections ('ENLONG_RATIO') are already in line with IAM results (no need to adjust)
#
# ##### Step 1 - Check IAM results
# As a first step, this function checks that regional IAM results makes sense.
# In particular we check if the sum across fuels (or sectors) matches the total.
# If so, we go ahead we the adjustements (step2).
#
# If not:
# - If the sum across all fuels (or sub-sectors) is greater than the sector, it means there is something wrong with the data. In this case we abort the downscaling.
# - If the total is greater than the sum across fuels, it means that there is a fuel (or sector) not reported. We call this fuel (or sector) 'Other'. We add this 'Other' fuel/sector to our dataframe. Then we go ahead with the adjustments (Step2)
#
# ##### Step 2 - Adjustments
# We create a while loop. Inside this while loop we want to match two conditions:
# - a)  match the previously downscaled 'total' (e.g. Final Energy|Residential and Commercial|Solids,  by country)
# - b)  match the regional IAM results by fuel (e.g. Final Energy|Residential and Commercial|Solids|Biomass).
#
# We proportionally adjust the results to match: first condition a) and then condition b). Please note that condition b) will be always satisfied (as this is the last thing we do in the loop).
#
# Therefore, our goal is to keep on iterating until condition a) is met.
# Condition a) is satisfied if <i>conv_ec</i>  - which measures the difference between the sum across countries and regional IAM results - is small enough (e.g. lower than 0.001 Ej).
#
# If the algorithm (for some reasons) convergences to a solution that does match regional IAM results, we break the loop (to avoid that the while loop keep on iterating forever). How we do this? We measure the % variation of conv_ec across iterations (delta/conv_ec). If the variation is small, it means that the solution is not changing, therefore we reached a convergence.
#
#


def fun_reshape_reg_ratio(reg_ratio: pd.DataFrame, targets: list) -> pd.DataFrame:
    """Reshapes reg_ratio and filters targets"""
    reg_ratio.index.names = ["TARGET", "UNIT", "TIME"]
    reg_ratio = reg_ratio.reset_index()
    reg_ratio = reg_ratio.set_index(["TIME", "TARGET"])[0]
    return reg_ratio[reg_ratio.index.get_level_values("TARGET").isin(targets)]


def fun_heat_enlong(
    df_iam: pd.DataFrame,
    model: str,
    region: str,
    targets: list,
    str_heat: str,
    heat_c: pd.DataFrame,
    ele_c: pd.DataFrame,
    long_term='ENLONG_RATIO'
) -> pd.DataFrame:
    """This function calculates heat in the long term, using same heat/el ratio
    at the regional level.
    """

    str_ele = str_heat.replace("Heat", "Electricity")
    levels = ["MODEL", "VARIABLE", "REGION"]
    num = df_iam.xs((model, str_heat, f"{region}"), level=tuple(levels))
    den = df_iam.xs((model, str_ele, f"{region}"), level=tuple(levels))

    # regional share: heat/electricity
    reg_ratio = (num / den).stack()
    reg_ratio = fun_reshape_reg_ratio(reg_ratio, targets)

    # Calculates heat:
    enlong_ratio = ele_c.loc[:, long_term] * reg_ratio

    # Fix indexes and get rid of `np.na` countries
    enlong_ratio = enlong_ratio[
        ~enlong_ratio.index.get_level_values("ISO").isin([np.nan])
    ]
    enlong_ratio = enlong_ratio.reset_index().set_index(heat_c.index.names)
    return enlong_ratio[0]


def fun_heat_downs(
    _df: pd.date_range,
    df_iam: pd.DataFrame,
    model: str,
    region: str,
    targets: list,
    df_iea_melt: pd.DataFrame,
    iea_flow_dict: dict,
    _func: str,
    long_term='ENLONG_RATIO',
) -> pd.DataFrame:
    """This function downscales Heat for a given region, for all targets."""

    str_ele = "Final Energy|Electricity"

    str_heat_list = fun_str_sector_list(_df, df_iam)

    for str_heat in str_heat_list:
        str_ele = str_heat.replace("Heat", "Electricity")

        # Get downscaled electricity
        ele_c = _df.xs(str_ele, level="SECTOR").reset_index()
        ele_c.set_index(["TIME", "TARGET", "ISO"], inplace=True)

        # Create new heat df `heat_c` copied from `ele_c` (and rename variable) :
        heat_c = ele_c.rename(index={str_ele: str_heat})
        heat_c.iloc[:, :] = np.nan
        heat_c.loc[:, "SECTOR"] = str_heat
        heat_c.loc[:, "FUNC"] = _func

        long=short=fun_heat_enlong(
                df_iam,
                model,
                region,
                targets,
                str_heat,
                heat_c,
                ele_c,
            )
        try:
            short=fun_heat_enshort(
                df_iam,
                model,
                region,
                targets,
                str_heat,
                heat_c,
                ele_c,
                df_iea_melt,
                iea_flow_dict,
                list(heat_c.index.get_level_values("ISO").unique()),
            )
        except:
            short=long

        func_dict = {
            "ENSHORT_REF": short,
            long_term: long,
        }

        for x in ["ENSHORT_REF", long_term]:
            heat_c.loc[:, x] = func_dict[x]

        setindex(heat_c, _df.index.names)
        _df = pd.concat([_df,heat_c])

    return _df


def fun_str_sector_list(
    _df: pd.DataFrame,
    df_iam: pd.DataFrame,
    sector: str = "Heat",
    ref_sector: str = "Electricity",
) -> set:
    """Returns set of heat sectors, based on previously downscaled result and IAM data

    Parameters
    ----------
    _df : pd.DataFrame
        downscaled results from step1
    df_iam : pd.DataFrame
        regional IAM results
    sector : str, optional
        Replace ref_sector with `sector`, by default "Heat"
    ref_sector : str, optional
        Check Final energy sectors that contain `ref_sector`, by default "Electricity"

    Returns
    -------
    set
        Heat sectors
    """
    iam_heat_list = fun_iam_final_sectors(df_iam, sector)
    allowed_heat_sectors = fun_downs_available_sectors(_df, sector, ref_sector)
    return set(iam_heat_list).intersection(allowed_heat_sectors)


def fun_downs_available_sectors(_df, sector, ref_sector):
    return [
        x.replace(ref_sector, sector)
        for x in _df.index.get_level_values("SECTOR").unique()
        if ref_sector in x and "Final" in x and x != f"Final Energy|{ref_sector}"
    ]


def fun_iam_final_sectors(df_iam, sector):
    return [
        x
        for x in df_iam.xs("EJ/yr", level="UNIT")
        .index.get_level_values("VARIABLE")
        .unique()
        if sector in x and "Final" in x
    ]


def fun_iterative_adj(
    model: str,
    region: str,
    target: str,
    var_iam: str,
    df_all: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    allowed_sub_sectors: list,
    overwrite_total: bool = False,
) -> pd.DataFrame:
    """
    This function adjust fuels (or sub-sectors) results until:
    - we match the previously downscaled 'Total' at the country level
    - we match the regional IAM results at the fuel (or sub-sectors) level.

    If overwrite_total=True it re-calculate the total (var_iam) as the sum of individual fuels
    This function is intended for ENSHORT_REF only. (no need to adjut ENLONG_RATIO, because fuels
    are consistent by definition, as we apply the same fuel share from the model)

    It takes as input the previsouly downscaled results dataframe (e.g. df_all).
    It returns the updated Dataframe.
    """
    seconds0 = time.time()
    region_name = fun_region_name(model, region)

    fuel_list = fun_sub_sectors(
        var_iam, region_name, target, df_iam_all_models, allowed_sub_sectors
    )

    ## STEP1 BEFORE START: CHECKING IAM results (check if sum across fuels matched total energy carries)
    sum_fuels = (
        fun_read_reg_value(model, region, target, fuel_list, df_iam_all_models)["VALUE"]
        .groupby("TIME")
        .sum()
    )
    tot_ec = (
        fun_read_reg_value(model, region, target, var_iam, df_iam_all_models)["VALUE"]
        .groupby("TIME")
        .sum()
    )
    other = tot_ec - sum_fuels
    # Difference betwee Total energy carries and the sum of all fuels. If>=0 then there is an 'Other' fuel not reported
    iam_error = ((other) ** 2).sum()

    if iam_error >= 1e-5:
        print("ERROR IN IAM RESULTS - SUM OF FUELS DOES NOT MATCH TOTAL")
        print(
            "Creating 'OTHER' variable, defined as the difference between the total and the sum across fuels (or sub-sectors)"
        )
        fuel_list = fuel_list + ["OtherENSHORT_REF"]

    if other.min() <= -0.1:
        print(
            "WARNING: ERROR IN IAM RESULTS - SUM OF FUELS (or SUB-SECTORS) GREATER THAN the TOTAL "
        )
        print("***** ITERATIVE ADJUSTMENTS ABORTED *****")
    else:
        ## STEP 2: adjusting fuels
        fuel_list = [f + "ENSHORT_REF" for f in fuel_list]
        conv_ec = 1e3  ## initialising convenrgence value (just a large number)
        conv_ec0 = conv_ec
        delta = 1e3  ## Initialisinz delta (difference in convergence)
        if len(fuel_list) > 1:
            # While loop (we only check the energy carrier convergence, because regional convergence is at the end of the loop, hence always satisfied)
            while conv_ec >= 0.001:
                ## Condition a) Matching total Final Energy Solids by country (previously downscaled):
                ratio_ec_country = (
                    df_all[fuel_list].sum(axis=1) / df_all[var_iam + "ENSHORT_REF"]
                )  # matching Previously downscaled Total energy carrier (at counrtry level)
                for f in fuel_list:
                    df_all[f] = (df_all[f] / ratio_ec_country).clip(
                        0, df_all[var_iam + "ENSHORT_REF"]
                    )

                ## Condition b) Matching regional results by fuels:
                for f in fuel_list:
                    if f == "OtherENSHORT_REF":
                        reg_value = other  ## difference between total and  sum across fuels/sectors
                    else:
                        reg_value = fun_read_reg_value(
                            model,
                            region,
                            target,
                            f.replace("ENSHORT_REF", ""),
                            df_iam_all_models,
                        )["VALUE"]

                    ratio_fuel_reg = df_all[f].groupby("TIME").sum() / reg_value
                    df_all[f] = df_all[f] / ratio_fuel_reg

                if abs(delta / conv_ec) <= 0.001:
                    ## If Percentage variation of conv_ec is small (we break the loop)
                    print("breaking loop...")
                    print(
                        "Algorithm converged to a solution, although the sum across countries does not match regional IAM results"
                    )
                    break

                conv_ec = ((1 - ratio_ec_country) ** 2).sum()
                delta = conv_ec0 - conv_ec

                conv_ec0 = conv_ec  ## storing  data
                print(conv_ec, delta / conv_ec)

            if overwrite_total == True:  # and break_flag==0:
                df_all[var_iam + "ENSHORT_REF"] = df_all[fuel_list].sum(
                    axis=1
                )  # Updates the total Energy carrier as the sum of sub-fuels
            print("TIME ELAPSED:", time.time() - seconds0)
    return df_all.sort_index()


def fun_iterative_adj_electricity_hydro_geothermal(
    model: str,
    region: str,
    target: str,
    var_iam: str,
    df_all: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    _elc_trade_adj: bool,
    _gw_nuc_min: float = 0.5,  # Gw threshold for nuclear
    min_iter: int = 3,
    max_iter:int = 30,
    conv_threshold: float = 0.001,
) -> pd.DataFrame:
    """
    This function adjust fuels (electricity mix) results, while trying to keep hydro_geothermal values above 2010 values across countries.
    NOTE: This function is intended for ENSHORT_REF only. (no need to adjut ENLONG_RATIO)

    If _elc_trade_adj==True, adjustments are made until:
    - we match the previously downscaled 'Total Electricity' at the country level (we try to minimise trade)
    - we match the regional IAM results at the fuel (or sub-sectors) level.

    If overwrite_total=True it re-calculate the total (var_iam) as the sum of individual fuels

    If _elc_trade_adj==False, thsi function will run iteratively (for a minimum of min_iter (e.g. 5)  iterations) to try to stabilise hydro and geothermal above 2010 data.

    It takes as input the previously downscaled results dataframe (e.g. df_all).
    It returns the updated Dataframe.
    """
    seconds0 = time.time()
    fuel_list = [
        "Secondary Energy|Electricity|Biomass",
        "Secondary Energy|Electricity|Coal",
        "Secondary Energy|Electricity|Gas",
        "Secondary Energy|Electricity|Geothermal",
        "Secondary Energy|Electricity|Hydro",
        #                  'Secondary Energy|Electricity|Non-Biomass Renewables|Solar',
        #                  'Secondary Energy|Electricity|Non-Biomass Renewables|Wind',
        "Secondary Energy|Electricity|Solar",
        "Secondary Energy|Electricity|Wind",
        "Secondary Energy|Electricity|Nuclear",
        "Secondary Energy|Electricity|Oil",
    ]

    ## STEP1 BEFORE START: CHECKING IAM results (check if sum across fuels matched total energy carries)
    sum_fuels = (
        fun_read_reg_value(model, region, target, fuel_list, df_iam_all_models)["VALUE"]
        .groupby("TIME")
        .sum()
    )
    tot_ec = (
        fun_read_reg_value(model, region, target, var_iam, df_iam_all_models)["VALUE"]
        .groupby("TIME")
        .sum()
    )

    # Difference betwee Total energy carries and the sum of all fuels. If>=0 then there is an 'Other' fuel not reported
    other = tot_ec - sum_fuels
    iam_error = ((other) ** 2).sum()

    ## STEP 2: adjusting fuels
    df_all.loc[
        df_all["Secondary Energy|Electricity|NuclearENSHORT_REF"]
        <= (_gw_nuc_min * 8760 * 3.6 / 1e6),
        "Secondary Energy|Electricity|NuclearENSHORT_REF",
    ] = 0  ## Threshold in GW installed

    # Placeoholder for hydro and geothermal
    clipped_fuels_2010 = [
        "Secondary Energy|Electricity|GeothermalENSHORT_REF",
        "Secondary Energy|Electricity|HydroENSHORT_REF",
    ]

    ## set correct index 2021_09_03, otherwise line below will not work
    fun_pd_sel(df_all, "", "")

    ## CLIP GEO AND HYDRO TO 2010 VALUES
    df_all[clipped_fuels_2010] = df_all[clipped_fuels_2010].clip(
        df_all.loc[2010, clipped_fuels_2010], np.inf
    )

    fuel_list = [f + "ENSHORT_REF" for f in fuel_list]
    conv_ec = 1e3  ## initialising convenrgence value (just a large number)
    conv_ec0 = conv_ec
    delta = 1e3  ## Initialisinz delta (difference in convergence)
    iter = 0

    # While loop (we only check the energy carrier convergence, because regional convergence is at the end of the loop, hence always satisfied)
    while conv_ec >= conv_threshold:
        ## CLIP GEO AND HYDRO TO 2010 VALUES
        df_all[clipped_fuels_2010] = df_all[clipped_fuels_2010].clip(
            df_all.loc[2010, clipped_fuels_2010], np.inf
        )

        final_electricity = df_all[
            var_iam.replace("Secondary", "Final") + "ENSHORT_REF"
        ] * fun_secondary_conv(
            model, target, region, "Final Energy|Electricity", df_iam_all_models
        )

        # matching Previously downscaled Total energy carrier (at counrtry level)
        ratio_ec_country = df_all[fuel_list].sum(axis=1) / final_electricity

        ## Condition a) Matching total Final Energy Electricity by country (previously downscaled):
        if _elc_trade_adj == True:
            for f in fuel_list:
                df_all[f] = (df_all[f] / ratio_ec_country).clip(0, final_electricity)

        ## Condition b) Matching regional results by fuels:
        for f in fuel_list:
            if f == "OtherENSHORT_REF":
                ## difference between total and  sum across fuels/sectors
                reg_value = other
            else:
                reg_value = fun_read_reg_value(
                    model,
                    region,
                    target,
                    f.replace("ENSHORT_REF", ""),
                    df_iam_all_models,
                )["VALUE"]

            ratio_fuel_reg = df_all[f].groupby("TIME").sum() / reg_value
            df_all[f] = df_all[f] / ratio_fuel_reg

            ## Nuclear GW installed capacity threshold
            df_all.loc[
                df_all["Secondary Energy|Electricity|NuclearENSHORT_REF"]
                <= (_gw_nuc_min * 8760 * 3.6 / 1e6),
                "Secondary Energy|Electricity|NuclearENSHORT_REF",
            ] = 0  # 1e-10 ## not zero

        if abs(delta / conv_ec) <= conv_threshold and iter > min_iter:
            ## If Percentage variation of conv_ec is small (we break the loop)
            print("breaking loop...")
            print(
                "Algorithm converged to a solution, although the sum across fuels does not match Energy Demand in all countries"
            )
            break

        conv_ec = ((1 - ratio_ec_country) ** 2).sum()
        delta = conv_ec0 - conv_ec

        conv_ec0 = conv_ec  ## storing  data
        iter = iter + 1
        print(conv_ec, delta / conv_ec, iter, "iter")
        if iter > max_iter:
            break
        if (delta / conv_ec) == np.nan and iter>(min_iter+1):
            break

    print("TIME ELAPSED:", time.time() - seconds0)
    return df_all.sort_index()


def fun_df_ssp_melt(
    df_ssp: pd.DataFrame, ssp_scenario: str, ssp_model: str
) -> pd.DataFrame:
    """This function melts the df_ssp dataframes. It returns the melted df_ssp"""
    df_ssp_melt = df_ssp.melt(id_vars=["SCENARIO", "VARIABLE", "MODEL", "UNIT", "ISO"])
    df_ssp_melt.rename(columns={"variable": "TIME", "value": "VALUE"}, inplace=True)
    df_ssp_melt["TIME"] = pd.to_numeric(df_ssp_melt["TIME"])

    ## SSP data selection (MODEL AND SCENARIO)
    df_ssp_melt = df_ssp_melt[df_ssp_melt["SCENARIO"].str.contains(ssp_scenario)]
    df_ssp_melt = df_ssp_melt[df_ssp_melt.MODEL == ssp_model]

    return fun_pd_sel(df_ssp_melt, "", "")


def fun_read_df_ssp(
    _pop: str, _gdp: str, ssp_scenario: str, ssp_model: str
) -> pd.DataFrame:
    """This function loads the df_ssp dataframes. It returns two df_ssp dataframes in two different format (normal and melted)"""
    df_ssp = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "SspDb_country_data_2013-06-12.csv",
        sep=",",
        encoding="utf-8",
    )  # encoding='latin-1')
    df_ssp.rename(columns={"REGION": "ISO"}, inplace=True)
    df_ssp = df_ssp.loc[
        (df_ssp.VARIABLE == _pop) | (df_ssp.VARIABLE == _gdp)
    ]  # ['2020']
    setindex(df_ssp, False)
    # Melting the SSP df
    df_ssp_melt = fun_df_ssp_melt(df_ssp, ssp_scenario, ssp_model)

    return df_ssp, df_ssp_melt


def fun_load_platts(sel_power_plants: Union[List, None] = None) -> pd.DataFrame:
    """This function loads the PLATTS database. It return a platts dataframe
    The WEPP Data has 10 unit status codes:
    CAN = cancelled,
    CON = under construction,physical site work is underway,
    DAC = deactivated, mothballed,
    DEF = deferred, no long scheduled,
    DEL = delayed, construction was started but later halted,
    OPR = in operation,
    PLN = planned, still in development or design,
    RET = retired from service,
    STN = shutdown,
    UNK = operating status unknown.
    """
    if sel_power_plants is None:
        sel_power_plants = ["OPR", "CON", "PLN", "DEL", "DEF"]

    df_platts = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "ALLUNITS_PLATTS_ISO.csv",
        sep=",",
        encoding="latin-1",
    )
    # Filtering selected power plants:
    return df_platts[df_platts.STATUS.isin(sel_power_plants)]


def fun_load_iea(sectors_required: Union[list, None] = None) -> pd.DataFrame:
    """This function loads the historical IEA data"""
    if sectors_required is None:
        sectors_required = [
            "Total final consumption",
            "Transport",
            "Industry",
            "Residential",
            "Commercial and public services",
            "Electricity output (GWh)",
            "Imports",
            "Exports",
        ]
    df_iea_h = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "Historical_data.csv",
        index_col=["ISO"],
        sep=",",
        encoding="utf-8",
    )
    ## Manually fixing country names
    df_iea_h.rename(columns={"COUNTRY": "COUNTRY"}, inplace=True)
    IEA_fuel_dict = fun_read_IEA_fuel_dict(
        CONSTANTS.INPUT_DATA_DIR / "IEA_Fuel_dict.csv"
    )  # IEA fuel dict (from IEA fuel name, to standardised fuel names)

    df_iea_all = fun_df_iea_all(CONSTANTS, IEA_fuel_dict)

    ## Automatically detects last year of hisorical data and create range of time list
    max_year = int(pd.to_numeric(df_iea_all.columns, errors="coerce").max())
    range_list = [str(x) for x in range(1960, max_year)]
    df_iea_melt = fun_df_iea_melt(df_iea_all, sectors_required, range_list)
    return df_iea_h, df_iea_all, df_iea_melt


def fun_load_df_iam_all(
    df_iam_all_input: Union[pd.DataFrame, None],
    list_of_models: list,
    list_of_targets: list,
    file: str,
) -> pd.DataFrame:
    """Loads IAM resulrs, returns df_iam_all and df_iam_all_models"""
    if df_iam_all_input is None:
        # df_iam_all = fun_read_df_iam_all(
        #     file=InputFile(CONSTANTS.INPUT_DATA_DIR / file),
        #     add_model_name_to_region=False,
        # )  ## READING IN IAMS RESULTS

        fun_read_df_iam_all_and_slice(
            list_of_models, list_of_targets, InputFile(CONSTANTS.INPUT_DATA_DIR / file)
        )
    else:
        df_iam_all = df_iam_all_input

    ## READ iam results and AJUSTING REGION NAME
    df_iam_all_models = df_iam_all.copy(deep=True)

    if df_iam_all_input is None:
        df_iam_all_models.loc[:, "REGION"] = (
            df_iam_all_models.loc[:, "REGION"] + "r"
        )  ## adjusting region name to avoid overlap with ISO codes
    return df_iam_all, df_iam_all_models


def fun_list_of_models(df_iam_all: pd.DataFrame, model_patterns: list) -> list:
    """Returns list of models present in `df_iam_all`, based on `models_patterns`"""
    models = [
        m
        for m in df_iam_all.MODEL.unique().tolist()
        if match_any_with_wildcard(m, model_patterns)
    ]
    if "Reference" in models:
        models.remove("Reference")
        models.sort(reverse=True)
    return models


# checking all fuels are distributed among energy carriers
def fun_check_iea_gases_list_fuels() -> None:
    """This function checks consistency in the list of iea_gases of fuels from fixtures.
    If the list of fuel is not consistent it will throw an error.
    """
    from downscaler.fixtures import iea_biomass, iea_coal, iea_gas, iea_gases, iea_oil

    # set should be empty
    main_list = iea_gases
    sel_list = (
        list(set(iea_gas).intersection(main_list))
        + list(set(iea_oil).intersection(main_list))
        + list(set(iea_gas).intersection(main_list))
        + list(set(iea_coal).intersection(main_list))
        + list(set(iea_biomass).intersection(main_list))
    )
    list1 = set(main_list) - set(sel_list)  ## differences in the two list
    list2 = set(sel_list) - set(main_list)

    if len(list1) != 0 or len(list2) != 0:
        raise ValueError(f"Check you list of fuels for {list1} and {list2}")


def fun_targets_regions_df_countries(
    model: str,
    df_iam_all_models: pd.DataFrame,
    region_patterns: list,
    target_patterns: list,
    pyam_mapping_file: str,
    df_countries: pd.DataFrame,
) -> Union[list, pd.DataFrame]:
    """This function returns lists of targets and regions for a given model"""

    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    targets = list(
        df_iam_all_models[df_iam_all_models.MODEL == model].SCENARIO.unique()
    )
    regions_all = list(
        df_iam_all_models[df_iam_all_models.MODEL == model].REGION.unique()
    )

    # NOTE: We only match regions that can be found for the corresponding model
    regions = [
        r
        for r in regions
        if any(
            r in r_df + "r" and match_any_with_wildcard(r_df, region_patterns)
            for r_df in regions_all
        )
    ]
    targets = [t for t in targets if match_any_with_wildcard(t, target_patterns)]
    return targets, regions, df_countries


def fun_slice_df_iam_all_models(
    model: str, region_loop: str, target: str, df_iam_all: pd.DataFrame
) -> pd.DataFrame:
    """Update df_iam_all and slices df_iam_all_models"""
    df_iam_all = fun_add_model_name_to_region(df_iam_all, model)
    df_iam_all_models = df_iam_all.copy(deep=True)

    if f"{model}|{region_loop}" in df_iam_all_models.REGION:
        df_iam_all_models = df_iam_all_models[
            (df_iam_all_models.REGION == region_loop)
            & (df_iam_all_models.SCENARIO == target)
        ]
    else:
        df_iam_all_models = df_iam_all_models[
            (df_iam_all_models.REGION == f"{model}|{region_loop}")
            & (df_iam_all_models.SCENARIO == target)
        ]
    return df_iam_all, df_iam_all_models


def make_input_data(
    project_file: IFT,
    list_of_models: List[str],
    list_of_regions: List[str],
    list_of_targets: List[str],
    random_electricity_weights: bool,
    csv_out: str,
    elc_trade_adj_input: bool,
    seed_range: range,
    model_in_region_name: bool,
    df_iam_all: pd.DataFrame,
    get_selection_dict: Callable,
    tc_max_range: List[int] = [2200],
    scen_dict: Union[dict, None] = None,
    run_sensitivity=False,
    default_ssp_scenario: str = "SSP2",
    long_term='ENLONG_RATIO',
    method='wo_smooth_enlong',
) -> List[dict]:
    """[This function creates an input_list for parallelisation of the simulation in step2]

    Args:
        # input_file (IFT): [input file with IAMs results]
        project_file (IFT): [project directory with IAMs results and regional mapping]
        list_of_models (List[str]): [List of selected models]
        list_of_regions (List[str]): [List of selected regions]
        list_of_targets (List[str]): [List of selected targets]
        random_electricity_weights (bool): [Generates random electricity weights]
        elc_trade_adj_input (bool): [Electricity trade minimisation]
        seed_range (range): [Seed range for random electricity weights]
        model_in_region_name (bool): [Scan only for regions that contain model name e.g. 'MESSAGE|Western Europe*' ]

    Returns:
        List[dict]: [Input list for parallel simulations]
    """

    input_file = CONSTANTS.INPUT_DATA_DIR / project_file / "snapshot_all_regions.csv"
    pyam_mapping_file = CONSTANTS.INPUT_DATA_DIR / project_file / "default_mapping.csv"
    selection_dict = get_selection_dict(
        InputFile(input_file), list_of_models, list_of_regions, list_of_targets
    )

    if random_electricity_weights == False:
        seed_range = range(
            1, 2, 1
        )  ## if Random electricity weights=False, then we only need 1 seed (no need to create multiple args_list for multiple seeds)

    if model_in_region_name == False:
        input_list = [
            {
                "file": input_file,
                "model_patterns": m,
                "region_patterns": r.rsplit("|")[1] + "r",  # f"{r}*",
                "target_patterns": t,
                "range_rand_elc_weights": range(seed, seed + 1, 1),
                "random_electricity_weights": random_electricity_weights,
                "csv_out": csv_out,
                "elc_trade_adj": elc_trade_adj_input,
                "pyam_mapping_file": pyam_mapping_file,
                "long_term":long_term,
                "method":method,
                # "regions":fun_match_wildcard(r, sel['regions'], model_in_region_name, m),
                "df_iam_all_input": df_iam_all[
                    (df_iam_all.MODEL == m)
                    & (df_iam_all.SCENARIO == t)
                    & (df_iam_all.UNIT == 'EJ/yr')
                    # &(df_iam_all.REGION.isin(fun_match_wildcard(r, sel['regions'], model_in_region_name, m ))
                    & (df_iam_all.REGION == r.rsplit("|")[1] + "r")
                ],
                "scen_dict": scen_dict,
                "run_sensitivity": run_sensitivity,
                "default_ssp_scenario": default_ssp_scenario,
            }
            for m, sel in selection_dict.items()
            for r in sel["regions"]
            for seed in seed_range
            for t in sel["targets"]
        ]
    else:
        input_list = [
            {
                "file": input_file,
                "model_patterns": m,
                "region_patterns": f"{r}*",
                "target_patterns": t,
                "range_rand_elc_weights": range(seed, seed + 1, 1),
                "random_electricity_weights": random_electricity_weights,
                "csv_out": csv_out,
                "elc_trade_adj": elc_trade_adj_input,
                "pyam_mapping_file": pyam_mapping_file,
                "long_term":long_term,
                "method":method,
                # "regions":fun_match_wildcard(r, sel['regions'], False, m), ## NOTE: model_in_region_name needs to be False here
                "df_iam_all_input": df_iam_all[
                    (df_iam_all.MODEL == m)
                    & (df_iam_all.SCENARIO == t)
                    & (
                        df_iam_all.REGION.isin(
                            fun_match_wildcard(r, sel["regions"], False, m)
                        )  ## NOTE: model_in_region_name needs to be False here
                    )
                ],
                "scen_dict": scen_dict,
                "run_sensitivity": run_sensitivity,
                "default_ssp_scenario": default_ssp_scenario,
            }
            for m, sel in selection_dict.items()
            for r in sel["regions"]
            for seed in seed_range
            for t in sel["targets"]
            if r.find(m) != -1
        ]

    print(f"input list {input_list}")  # Print full input_list

    ## Print selected variables out of the input_list
    vlist = [
        "region_patterns",
        "target_patterns",
        "range_rand_elc_weights",
        "random_electricity_weights",
    ]
    for i in range(0, len(input_list), 1):
        print(
            [input_list[i][vlist[(j[0])]] for j in enumerate(vlist)]
        )  ## j[0] (because enumerates creates a tuple)

    ## Print other info
    print(f"This is the lenght of input list: {len(input_list)}")

    if tc_max_range != [2200]:  ## This means we propagate tc_max from step1 to step2
        input_new = []
        for i in tc_max_range:
            "Propagating_step1_step2_max-tc_2300", "Propagating_step1_step2_max-tc_2100"  ## NOTE: what we want for csv_ouT
            input_list_mod = copy.deepcopy(input_list)

            for j in input_list_mod:
                j["csv_out"] = j["csv_out"] + "_max-tc_" + str(i)

            input_new = input_new + input_list_mod
        return input_new

    return input_list


def fun_concatenate_df_all_df_sect(
    df_all: pd.DataFrame, df_sect: pd.DataFrame
) -> pd.DataFrame:
    """This function concatenates two dataframes. It returns the updated datafrane"""
    # Without this block the LIST OF COUNTRIES in df_desired IS DIFFERENT over time: IN 2095 [USA] AND 2100 [USA, PRI]
    # Maybe also due to 5 years time steps in # .. years_all_countries.csv"
    fun_pd_sel(df_all, "", "")  ## setting index
    fun_pd_sel(df_sect, "", "")  ## setting index
    df_sect_index_list = df_sect.index.tolist()
    df_all_index_list = df_all.index.tolist()
    # missing countries (to be added to df_sect)
    add_df_sect = set(df_all_index_list) - set(df_sect_index_list)
    # creating a df with missing countries
    df_add_df_sect = pd.DataFrame(index=add_df_sect, columns=df_sect.columns)
    df_sect = pd.concat([df_sect,df_add_df_sect])
    m = ["TIME", "ISO"]
    setindex(df_sect, m)
    df_sect.sort_index(inplace=True)

    ## Concat df_sect to df_all (for all variables in var_list)
    setindex(df_all, m)
    setindex(df_sect, m)
    df_all.sort_index(inplace=True)

    # we drop columns already present in df_all
    drop_columns = [x for x in df_all.columns if x in df_sect.columns]
    df_all = pd.concat(
        [df_all, df_sect.drop(drop_columns, axis=1)],
        axis=1,
        join="inner",
    )
    df_all.sort_index(inplace=True)
    return df_all


def fun_add_gdp_pop_from_ssp(
    df_all: pd.DataFrame, df_ssp_melt: pd.DataFrame, gdp: str, pop: str
) -> pd.DataFrame:
    """This function adds gdp and population from df_ssp_melt to df_all. It return the updated df_all"""
    # same time horizon of df_all
    time_horizon = list(df_all.index.get_level_values(0).unique())
    # same countrylist of df_all
    c_list = list(df_all.index.get_level_values(1).unique())

    # append all data for all variables in var_list
    popdata = fun_pd_sel(df_ssp_melt, time_horizon, c_list, pop)[["VALUE"]]
    popdata = popdata.rename(columns={"VALUE": pop})

    gdpdata = fun_pd_sel(df_ssp_melt, time_horizon, c_list, gdp)[["VALUE"]]
    gdpdata = gdpdata.rename(columns={"VALUE": gdp})
    return pd.concat([df_all, popdata, gdpdata], axis=1)


def fun_minimize_elc_trade_and_clip_hydro_geoth(
    model: str,
    region: str,
    target: str,
    df_all: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    elc_trade_adj: bool,
    df_desired: pd.DataFrame,
    clipped_fuels_2010: list,
    var_list: list,
    min_iter: int = 3,
    conv_threshold: float = 0.001,
    long_term="ENLONG_RATIO",
) -> pd.DataFrame:
    """This function minimizes electricity trade and clips `Secondary Energy|Electricity|Hydro` and
    `Secondary Energy|Electricity|Geothermal` above 2010 values"""

    myvar = "Secondary Energy|Electricity"
    # 1) In a first step we try to minimize electricity trade while trying to avoid decling HYDRO and Geothermal
    fun_iterative_adj_electricity_hydro_geothermal(  ## we try to avoid decling HYDRO and Geothermal
        model,
        region,
        target,
        myvar,
        df_all,
        df_iam_all_models,
        elc_trade_adj,  ## Minimise trade true/false. If false this function will run iteratively only to try to stabilise geothermal and hydro above 2010 data (for min_iiter iterations)
        min_iter=min_iter,
        conv_threshold=conv_threshold,
    )
    fun_pd_sel(df_all, "", "")  # set correct index

    iamc_name_dict = {"GEO": "Geothermal", "HYDRO": "Hydro"}
    threshold_dict = {"GEO": 0.2, "HYDRO": 0.5}
    vars_dict = {f"{myvar}|{j}{long_term}": i for i, j in iamc_name_dict.items()}

    # Clip enlong_ratio as well using enshort_ref 2010 values:
    df_all = fun_clip_enlong_using_enshort2010_values(
        df_all, df_desired, threshold_dict, vars_dict, long_term=long_term
    )

    # 2) In a second step we clip geothermal and hydro to 2010 values.
    # Here we also make a disinction between countries with high/low shares
    threshold_dict2 = {iamc_name_dict[i]: j for i, j in threshold_dict.items()}
    for _ in range(3):
        df_all[clipped_fuels_2010] = df_all[clipped_fuels_2010].clip(
            df_all.loc[2010, clipped_fuels_2010], np.inf
        )
        ## we harmonise the results to match regional IAM results for geothermal and Hydro while try keeping the data above 2010 values
        for clip_ren, var in itertools.product(threshold_dict2, var_list):
            df_all = adjust_country_with_low_share(
                clip_ren,
                df_all,
                df_desired,
                model,
                target,
                df_iam_all_models,
                region,
                __threshold=threshold_dict2[clip_ren],
                _var=var,
            )

    return df_all


def fun_clip_enlong_using_enshort2010_values(
    df_all, df_desired, threshold_dict, vars_dict, long_term="ENLONG_RATIO"
):
    av_countries=df_all.reset_index().ISO.unique()
    for v, j in vars_dict.items():
        clist_sel = filter_country(j, df_desired, _threshold=threshold_dict[j])
        clist_sel=[x for x in clist_sel if x in av_countries]
        enshort_var_name = v.replace(long_term, "ENSHORT_REF")
        enshort = df_all.loc[[(2010, i) for i in clist_sel], enshort_var_name]
        df_all.loc[[(2010, i) for i in clist_sel], v] = enshort
    return df_all


def fun_run_step2b(
    df_all: pd.DataFrame,
    model: str,
    region: str,
    target: str,
    file: str,
    file_suffix: str,
    df_iam_electr: pd.DataFrame,
    df_iea_electr: pd.DataFrame,
    df_platts: pd.DataFrame,
    df_weights: pd.DataFrame,
    pyam_mapping_file: str,
    step1b: bool,
    random_electricity_weights: bool,
    _run_sensitivity: bool = False,
    _scen_dict: dict = None,
    read_cached_sensitivity: Union[None, pd.DataFrame] = None,
    default_ssp_scenario="SSP2",
    _func:str = 'log-log',
) -> pd.DataFrame:
    """This function runs step 2b (electricity downscaling) and returns the updated dataframe"""
    import downscaler.Electricity_2b

    # try:
    df_desired, df_sel_seeds = downscaler.Electricity_2b.main(
        "ENSHORT_REF",
        file,
        region,
        model,
        target,
        file_suffix,
        df_iam_electr,
        df_iea_electr,
        df_platts,
        df_weights,
        pyam_mapping_file,
        step1b=step1b,
        _scen_dict=_scen_dict,
        _run_sensitivity=_run_sensitivity,
        read_cached_sensitivity=read_cached_sensitivity,
        _func=_func,
    )
    # df_desired = df_desired.drop_duplicates(subset=None, keep="last")  #
    df_all_res = {
        x: fun_update_df_all_with_elc_results(
            df_all.copy(), random_electricity_weights, x, df_desired[x].copy()
        )
        for x in df_desired.keys()
    }
    return df_all_res, df_desired, df_sel_seeds
    # except Exception as e:
    #     print(e)
    #     print(traceback.format_exc())
    #     print(f"ERROR - Downscaling electricity  not working for {region}. Pass")

    # setindex(df_all, False)
    # df_all["TIME"] = pd.to_numeric(df_all["TIME"])
    # fun_pd_sel(df_all, "", "")

    # # Concatenates df_all with df_sect
    # df_all = fun_concatenate_df_all_df_sect(df_all, df_sect)
    # df_all = df_all.iloc[:, ~df_all.columns.duplicated(keep="last")]
    # df_all.sort_index(inplace=True)
    
    # return df_all, df_desired
    

def fun_update_df_all_with_elc_results(
    df_all: pd.DataFrame,
    random_electricity_weights: bool,
    ra: int,
    df_desired: pd.DataFrame,
) -> pd.DataFrame:
    """Updated df_all with electricity results

    Parameters
    ----------
    df_all : pd.DataFrame
        Initial dataframe
    random_electricity_weights : bool
        If running sensitivity analysis (random weights)
    ra : int
        seed used to randomly generate electricty criteria weights
    df_desired : pd.DataFrame
        Electricity results

    Returns
    -------
    pd.DataFrame
        Updated dataframe with electricity results
    """

    df_sect = fun_adjust_column_names_df_desired(
        df_desired,
    )
    df_sect = fun_pd_sel(df_sect, "", "")

    if random_electricity_weights:
        df_sect["Electricity Criteria"] = f"Random_{ra}"
    else:
        df_sect["Electricity Criteria"] = "Standard assumptions"

    setindex(df_all, False)
    df_all["TIME"] = pd.to_numeric(df_all["TIME"])
    fun_pd_sel(df_all, "", "")

    # Concatenates df_all with df_sect
    df_all = fun_concatenate_df_all_df_sect(df_all, df_sect)
    df_all = df_all.iloc[:, ~df_all.columns.duplicated(keep="last")]
    df_all.sort_index(inplace=True)
    return df_all


def fun_reshape_heat_reg(heat_reg: pd.DataFrame, targets: list) -> pd.Series:
    """Reshapes heat_reg and filters targets"""
    heat_reg.columns.names = ["TIME"]
    heat_reg.reset_index(inplace=True)
    heat_reg = heat_reg[heat_reg.TARGET.isin(targets)]
    heat_reg = pd.DataFrame(heat_reg.set_index(["TARGET", "UNIT"]).stack())
    return heat_reg.reset_index().set_index(["TIME", "TARGET"])[0]


def fun_hist_heat_res(
    df_iea_melt: pd.DataFrame,
    str_heat: str,
    _countrylist: list,
    iea_flow_dict: dict,
    m: list,
    t: int = 2010,
) -> dict:
    """Get historical heat and electricity data, for a given `_countrylist`"""

    # Country level hist 2010 data:
    hist_res = {}
    for x in ["Electricity", "Heat"]:
        res = (
            fun_pd_sel(
                df_iea_melt[
                    (df_iea_melt.PRODUCT == x)
                    & (df_iea_melt.FLOW.isin(iea_flow_dict[str_heat][0]))
                ],
                t,
                _countrylist,
            )
            .fillna(0)
            .groupby(m)
            .sum()
        )
        hist_res[x] = res["VALUE"]
    return hist_res


def fun_heat_enshort(
    df_iam: pd.DataFrame,
    model: str,
    region: str,
    targets: list,
    str_heat: str,
    heat_c: pd.DataFrame,
    ele_c: pd.DataFrame,
    df_iea_melt: pd.DataFrame,
    iea_flow_dict: dict,
    _countrylist: list,
    t: int = 2010,
) -> pd.DataFrame:
    """Calculates heat for short term projections,
    based on historical heat/el ratios at the country level.
    """
    levels = ["MODEL", "VARIABLE", "REGION"]
    m = ["TIME", "ISO"]

    # Get historical (2010) heat/electricity ratio
    hist_res = fun_hist_heat_res(df_iea_melt, str_heat, _countrylist, iea_flow_dict, m)
    _h_base_year_ratio = (hist_res["Heat"] / hist_res["Electricity"]).fillna(0)

    if _h_base_year_ratio.sum() == 0:
        # if we have no hist data, we use the same heat/elc share for all countries
        _h_base_year_ratio.loc[:, :] = 1 / len(_countrylist)

    _h_base_year_ratio = pd.DataFrame(_h_base_year_ratio)
    _h_base_year_ratio["TARGET"] = "target"
    single_target = _h_base_year_ratio.copy()
    for target in targets:
        single_target.loc[:, "TARGET"] = target
        _h_base_year_ratio =pd.concat([_h_base_year_ratio,single_target])
    _h_base_year_ratio = _h_base_year_ratio[_h_base_year_ratio.TARGET != "target"]
    setindex(_h_base_year_ratio, ["TIME", "TARGET", "ISO"])

    # Get regional heat data
    heat_reg = df_iam.xs((model, str_heat, region), level=tuple(levels))

    # Downscales heat based on historical base year shares
    heat_c = ele_c.loc[:, "ENSHORT_REF"] * _h_base_year_ratio.loc[t, "VALUE"]
    heat_c = pd.DataFrame(heat_c)
    heat_c = setindex(heat_c, ["TIME", "TARGET", "ISO"])[0]
    # NOTE: might need to sort the df by using df.sort_index()
    # Harmonize to match regional IAM results
    heat_c = heat_c / heat_c.groupby(["TIME", "TARGET"]).sum()
    heat_reg = fun_reshape_heat_reg(heat_reg, targets)
    return heat_c * heat_reg

    ## 2022_07_28 Careful
    return scen_str.replace(".csv", f"_{time_str}.xlsx")


def fun_check_get_ssp_input(allowed_ssp, default_ssp, _scen_dict):
    if default_ssp is None and _scen_dict is None:
        raise ValueError("Please specify a default_ssp and/or a scen_dict ")

    if default_ssp is not None and default_ssp not in allowed_ssp:
        raise ValueError(
            "default_ssp is only allowed to contain the following SSPs: "
            f"{allowed_ssp}. Your default_ssp is: {default_ssp} "
        )
    if _scen_dict is not None:
        for ssp in _scen_dict.values():
            if ssp not in allowed_ssp:
                raise ValueError(
                    "_scen_dict.values() is only allowed to contain the following "
                    f"SSPs: {allowed_ssp}. You provided: {ssp}."
                )


def fun_beccs_emission(
    file_input: str, model: str, selection_dict: dict, project_file, ra: Union[str, int]
) -> pd.DataFrame:
    """Calculates BECCS sequestration for Electricity, Liquids and Hydrogen, from a `file_input` based on secondary energy beccs.
    It returns a dataframe"""
    ## BECCS emissions by sector
    df_new = pd.read_csv(file_input)
    df_new = fun_select_criteria(df_new, ra)
    setindex(df_new, ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"])
    df_new = fun_convert_columns_to_type(df_new)

    beccs_dict = {
        "Carbon Sequestration|CCS|Biomass|Energy|Supply|Electricity": [
            "Secondary Energy|Electricity|Biomass"
        ],
        "Carbon Sequestration|CCS|Biomass|Energy|Supply|Liquids": [
            "Secondary Energy|Liquids|Biomass"
        ],
        "Carbon Sequestration|CCS|Biomass|Energy|Supply|Hydrogen": [
            "Secondary Energy|Gases|Biomass"
        ]  # We assume negative emissions from Hydrogen is produced by 'Secondary Energy|Gases|Biomass'
        # note: 'Carbon Sequestration|CCS|Biomass|Energy|Supply|Gases' is not available from IAMs.
    }

    df_new = fun_replace_d_dot_iso(df_new, "REGION")

    for i, j in beccs_dict.items():
        print(i, j)
        # setindex(df_new, ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"])
        df_new = fun_create_var_as_sum(df_new, i, j, unit="Mt CO2/yr")
        df_new.loc[:, list(range(2010, 2105, 5))] = df_new.loc[
            :, list(range(2010, 2105, 5))
        ].clip(1e-9, np.inf)
        df_new = fun_validation(
            CONSTANTS,
            Path("results/5_Explorer_and_New_Variables"),
            selection_dict,
            project_file,
            df_new,
            [f"*{model}*"],
            ["*"],
            ["*"],
            vars=[i],
            harmonize=True,
        )

    return df_new[df_new.index.get_level_values("VARIABLE").isin(beccs_dict.keys())]


def fun_append_beccs_emi(_df: pd.DataFrame, _df_beccs: pd.DataFrame) -> pd.DataFrame:
    """This function appends two dataframes. It returns a dataframe"""
    setindex(_df_beccs, False)

    _df_beccs.columns = [str(x) for x in _df_beccs.columns]
    _df_beccs = _df_beccs.set_index(_df.index.names)
    if "2005" in _df.columns:
        _df = _df.drop("2005", axis=1)
    return pd.concat([_df,_df_beccs])

def fun_mitigation(
    df: pd.DataFrame,
    ref_scen: str = "h_ndc",
    mit_scen: str = "o_1p5c",
) -> pd.DataFrame:
    """This function calculates mitigation across 2 scenarios. It return a dataframe)
    NOTE: We use absolute value because we also need to calculate mitigation from negative emissions
    (in this case we get negative mitigation because CCS captured =0 in the baseline scenario).
    """
    return np.abs(df.xs(ref_scen, level="SCENARIO") - df.xs(mit_scen, level="SCENARIO"))


def fun_top_mitigation(
    df_mitigation: pd.DataFrame,
    var_list: list,
    top: int = 20,
    _from: int = 2045,  # (included)
    _to: int = 2050,  # (included)
    base_year: Union[int, bool] = False,
    aggregate_countries: bool = False,
    aggregate_variables: bool = False,
    show_abs_emi: bool = True,  # If False -> % of World Mitigation
    verbose: bool = False,
) -> pd.Series:
    if type(base_year) is not int:
        base_year = 2010

    cols_diff = pd.Series(df_mitigation.columns).diff().dropna().unique()
    if len(cols_diff) == 1:
        dt = int(cols_diff[0])
    else:
        raise ValueError(
            f"The time step in your dataframe seem to differ over time. Time_step: {cols_diff}. Please use a dataframe with the same time years step (e.g. 5 years time steps) "
        )
    years = list(range(_from, _to + dt, dt))

    # replace D.ISO
    df_mitigation = fun_replace_d_dot_iso(df_mitigation, "REGION")

    # Average mitigation (angular coefficient approach)
    df_mitigation = df_mitigation[
        df_mitigation.index.get_level_values("VARIABLE").isin(var_list)
    ][years].mean(axis=1) / (np.mean([_to, _from]) - base_year)

    missing_eu_countries = list(
        set(fun_eu27()) - set(df_mitigation.reset_index().REGION.unique())
    )
    if len(missing_eu_countries):
        print(
            f"Cannot calculate total EU27, these coutries are missing: {missing_eu_countries}"
        )
    else:
        df_mitigation = fun_aggregate_countries(
            df_mitigation,
            "EU27",
            ["MODEL", "VARIABLE", "UNIT"],
            fun_eu27(),
            remove_single_countries=True,
        )

    top20_global_mitigation = (
        df_mitigation.sort_values(ascending=False).iloc[:top].sum()
    )
    if verbose:
        print(
            f"Top-{top} mitigation measures from {_from} to {_to} - excluding BECCS:",
            np.round(top20_global_mitigation, 1),
            "MtCO2",
        )

    if aggregate_countries:
        if aggregate_variables:
            raise ValueError(
                f"You can either aggregate by countries OR sectors, not both. Your selection: aggregate_countries={aggregate_countries}, aggregate_variables={aggregate_variables}."
            )
        df_mitigation = df_mitigation.groupby(["MODEL", "VARIABLE", "UNIT"]).sum()
    if aggregate_variables:
        df_mitigation = df_mitigation.groupby(["MODEL", "REGION", "UNIT"]).sum()

    if top is not None:
        df_top = df_mitigation.sort_values(ascending=False).iloc[:top]
    else:
        df_top = df_mitigation.sort_values(ascending=False)
    share = np.round(100 * df_top.sum() / df_mitigation.sum(), 1)

    if len(df_mitigation.index.get_level_values("REGION").unique()) == 184:
        # We print this info only if we run it for all (184) countries
        print(f"Percentage of global target: {share}%")
    if show_abs_emi:
        return df_top.sort_values(ascending=False)  # .iloc[:top]

    df_top = df_top / df_mitigation.sum() * 100
    units = df_top.index.get_level_values("UNIT").unique()
    unit_dict = {x: "%" for x in units}
    return df_top.rename(unit_dict).sort_values(ascending=False)


def fun_aggregate_countries(
    df_mitigation: pd.DataFrame,
    new_region_name: str,
    _groupby: Union[None, list],
    countrylist: Union[None, list],
    remove_single_countries,
    _var_name: Union[None, str] = None,
    _unit_name: Union[None, str] = None,
):
    if new_region_name in df_mitigation.reset_index().REGION.unique():
        raise ValueError(f'Cannot create aggregated region {new_region_name}, as it is already present in the dataframe. Otherwise we will double the values for this region')
    
    # drop duplicates to avoid possible double counting (summing up twice)
    if isinstance(df_mitigation, pd.DataFrame) and 'REGION' in df_mitigation.columns:
        df_mitigation=df_mitigation.set_index('REGION', append=True)
    df_mitigation = fun_drop_duplicates(df_mitigation)
    # add eu 27 numbers
    if countrylist is not None:
        eu27 = pd.DataFrame(
            df_mitigation[
                df_mitigation.index.get_level_values("REGION").isin(countrylist)
            ].fillna(0)
        )
        if _groupby is not None:
            eu27 = eu27.groupby(_groupby).sum()

    else:
        if _groupby is not None:
            eu27 = pd.DataFrame(df_mitigation.fillna(0).groupby(_groupby).sum())
        else:
            eu27 = pd.DataFrame({"All models": df_mitigation.fillna(0).sum()}).T
            for x in df_mitigation.index.names:
                eu27[x] = "All"
        if _var_name is not None and _groupby is not None:
            if "VARIABLE" in _groupby:
                raise ValueError(
                    f"if you want to overwrite the VARIABLE name using {_var_name}, please do not groupby by VARIABLE. Your current _groupby list: {_groupby}"
                )
            eu27["VARIABLE"] = _var_name

    if _unit_name is not None:
        if _groupby is not None and "UNIT" in _groupby:
            raise ValueError(
                f"if you want to overwrite the unit using {_unit_name}, please do not groupby by UNIT. Your current _groupby list: {_groupby}"
            )
        eu27["UNIT"] = _unit_name

    eu27["REGION"] = new_region_name
    eu27 = eu27.reset_index().set_index(df_mitigation.index.names)

    # remove individual eu 27 countries from df_mitigation
    if countrylist is None:
        countrylist = df_mitigation.reset_index().REGION.unique()
    if remove_single_countries:
        df_mitigation = df_mitigation[
            ~df_mitigation.index.get_level_values("REGION").isin(countrylist)
        ]

    idxnames = ["MODEL", "REGION", "VARIABLE", "UNIT"]
    cols = list(df_mitigation.columns) if isinstance(df_mitigation, pd.DataFrame) else []
    if "SCENARIO" in df_mitigation.index.names + cols:
        idxnames = idxnames + ["SCENARIO"]
    # add eu27 as an aggregate
    if type(df_mitigation) == pd.Series:
        return pd.concat([df_mitigation,eu27[0]])
    else:
        df_mitigation = pd.concat([df_mitigation,eu27]).groupby(idxnames).sum()
        # df_mitigation = df_mitigation.append(eu27).groupby(idxnames).sum()
        if 2005 in df_mitigation:
            df_mitigation = df_mitigation.drop(2005, axis=1)
        return df_mitigation


def fun_create_short_long_emi_table(
    df_co2_mitigation: pd.DataFrame,
    df_ghg_cum: pd.DataFrame,
    emi_list: list,
    time_dict: dict,
    _mitigation_vs_base_year: Union[int, bool] = False,
    _top: int = 20,
    _show_abs_emi: bool = False,
    _add_c_budget_calc=True,
) -> pd.DataFrame:
    """This function creates a table with the `top` mitigation measures in the short term and the long term"""
    res_dict = {
        x: fun_top_mitigation(
            df_co2_mitigation,
            emi_list,
            _top,
            time_dict[x]["from"],
            time_dict[x]["to"],
            _mitigation_vs_base_year,
            aggregate_countries=False,
            show_abs_emi=_show_abs_emi,
        )
        for x in ["short", "long"]
    }
    combi = fun_combine_short_and_long(time_dict, res_dict)

    if _add_c_budget_calc:
        f=fun_carbon_budget
        c_budget = pd.concat([f(df_ghg_cum, time_dict[x]['from'], time_dict[x]['to'] - 1) for x in ['short','long']],axis=1)
        c_budget=c_budget.drop("UNIT", axis=1) if 'UNIT' in c_budget.columns else c_budget
        # return pd.concat([combi, c_budget.loc[combi.index]], axis=1).set_index("UNIT", append=True)
        # return pd.concat([combi.set_index('REGION', append=True).dropna(how='all', axis=0),
        #   c_budget.reset_index().set_index(combi.index.names).loc[combi.index].dropna(how='all', axis=0)]).reset_index('REGION')
        df1=combi.dropna(how='all', axis=0)
        df2=c_budget.reset_index().set_index(combi.index.names).dropna(how='all', axis=0)
        return pd.concat([df1, df2], axis=1)
    return combi


def fun_combine_short_and_long(time_dict, res_dict):
    # combine long term and short term top mitigation
    short_col = f"{time_dict['short']['from']}-{time_dict['short']['to']}"
    long_col = f"{time_dict['long']['from']}-{time_dict['long']['to']}"

    combi = pd.concat([res_dict["short"], res_dict["long"]], axis=1)
    combi = combi.rename(
        columns={
            0: short_col,
            1: long_col,
        }
    )
    combi = combi.sort_values(
        by=[
            short_col,
            long_col,
        ],
        ascending=False,
    )
    return combi


def fun_carbon_budget(df_ghg_cum, _from, _to):
    # USE `fun_get_time_list_from_time_range_string()`
    fun=fun_get_time_list_from_time_range_string
    
    units=df_ghg_cum.reset_index().UNIT.unique()
    unexpected_units=[x for x in units if 'CO2' not in x]
    if len(unexpected_units):
        txt="We found some unexpected units in your dataframe"
        raise ValueError(f'{txt}: {unexpected_units}. Cannot calculate carbon budget.')

    # _from=time_dict['short']['to'] 
    # to=time_dict['long']['to'] - 1
    cols=fun(f"{_from}-{_to}")
    missing_cols=[x for x in cols if x not in df_ghg_cum.columns]
    if len(missing_cols):
        raise ValueError(f'Cannot calculate carbon budget, these columns are missing: {cols}')
    c_budget = df_ghg_cum[cols]
    c_budget=pd.DataFrame(c_budget.sum(axis=1), columns=[f"carbon_budget_{_from}_{_to}"])
    # c_budget.columns = [f"carbon_budget_until_{x}" for x in c_budget.columns]
    # unit_level = c_budget.index.names.index("UNIT")
    # combi = combi.reset_index(level=unit_level, drop=False)
    # c_budget = c_budget.reset_index().set_index(combi.index.names)

    return  c_budget


def fun_load_co2(
    get_selection_dict: dict,
    project_file: str,
    model: str,
    main_emi_sectors: dict,
    file_input_co2: str,
    file_input_energy: str,
    add_co2: bool = True,
    add_beccs: bool = True,
    ra: Union[str, int] = "standard",
):
    """This function loads preaviously downscaled CO2 emissions from the `results/5_Explorer_and_New_Variables/` folder.
    Then it calculates additional BECCS variables at the sectorial (Electricity, Liquids and Hydrogen) level, based on the energy data from `file_input_energy`.
    It returns: 1) a dataframe with mitigation and 2) a list of emissions variables.

    Essentially this function:
    1) Loads CO2 data
    2) Calculates BECCS emissions
    3) Calculates aggregated variables (as sum of sub sectors)
    4) Removes sub sectors from the dataset (only returns `emi_list` VARIABLES)
    5) Replace D.ISO with ISO
    """

    # file_input = CONSTANTS.INPUT_DATA_DIR / file_input_co2.replace("MODEL", model)
    folder=CONSTANTS.CURR_RES_DIR('step5') if callable(CONSTANTS.CURR_RES_DIR) else CONSTANTS.CURR_RES_DIR
    df_co2 = pd.DataFrame()
    if add_co2:
        # df_co2 = pd.read_csv(CONSTANTS.INPUT_DATA_DIR / file_input)
        # df_co2 = pd.read_csv(file_input)
        df_co2 = pd.read_csv(folder/file_input_co2)
        df_co2 = fun_select_criteria(df_co2, ra)
        setindex(df_co2, ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"])

        if isinstance(model, str) and '*' not in model:
            selection_dict={model:'trick to make fun_validation work'}
        else:
            selection_dict = get_selection_dict(
                InputFile(
                    CONSTANTS.INPUT_DATA_DIR
                    / project_file
                    / Path("snapshot_all_regions.csv")
                ),
                model=["*"],
                region=["*"],
                target=["*"],
                variable=["*"],
            )

    # 2) Calculates (add) BECCS emissions
    # if add_beccs:
    df_co2 = fun_add_beccs_emissions(
        project_file, model, file_input_energy, df_co2, selection_dict, add_beccs, ra
    )

    # 3) Calculates aggregated variables (as sum of sub sectors)
    emi_list = []
    if add_co2:
        for j, i in main_emi_sectors.items():
            df_co2 = fun_create_var_as_sum(df_co2, j, i, unit="Mt CO2/yr")
        df_co2 = fun_convert_columns_to_type(df_co2)

        emi_list += list(main_emi_sectors.keys()) + [
            "Emissions|CO2|Energy|Supply|Heat",
            "Emissions|CO2|Industrial Processes",
        ]
    if add_beccs:
        emi_list += [
            "Carbon Sequestration|CCS|Biomass|Energy|Supply|Liquids",
            "Carbon Sequestration|CCS|Biomass|Energy|Supply|Hydrogen",
            "Carbon Sequestration|CCS|Biomass|Energy|Supply|Electricity",
        ]
    # 4) Removes sub sectors from the dataset
    df_co2 = df_co2[df_co2.index.get_level_values("VARIABLE").isin(emi_list)]

    # 5) Replace D.ISO with ISO
    return fun_replace_d_dot_iso(df_co2, "REGION"), emi_list


def fun_add_beccs_emissions(
    project_file, model, file_input_energy, df_co2, selection_dict, add_beccs, ra
):
    folder=CONSTANTS.CURR_RES_DIR('step5') if callable(CONSTANTS.CURR_RES_DIR) else CONSTANTS.CURR_RES_DIR
    if add_beccs:
        df_beccs = fun_beccs_emission(
            folder/file_input_energy,
            model,
            selection_dict,
            project_file,
            ra,
        )
        # SELECT BECCS CRITERIA HERE??
        df_beccs = df_beccs.droplevel("REGION").rename_axis(index={"ISO": "REGION"})
        setindex(df_beccs, ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"])
    else:
        df_beccs = pd.DataFrame()

    if len(df_co2):
        df_co2 = fun_append_beccs_emi(df_co2, df_beccs) if len(df_beccs) else df_co2
    else:
        df_co2 = df_beccs
    return df_co2


def fun_calculate_mitigation(
    df_ghg: pd.DataFrame,
    _ref_scen: str,
    _mit_scen: str,
    mitigation_vs_base_year: Union[bool, int] = False,
) -> Union[pd.DataFrame, list]:
    """This function calculates the mitigation (`_ref_scen`-`_mit_scen`) and returns a dataframe with mitigation."""

    beccs_idx = df_ghg[
        df_ghg.index.get_level_values("VARIABLE").str.contains("Carbon Sequestration")
    ].index

    if len(beccs_idx) > 0:
        df_ghg.loc[beccs_idx, :] = -df_ghg.loc[beccs_idx, :]

    if mitigation_vs_base_year:
        df_mitigation = -df_ghg.xs(_mit_scen, level="SCENARIO").sub(
            df_ghg.xs(_mit_scen, level="SCENARIO")[mitigation_vs_base_year], axis=0
        )

    else:
        df_mitigation = df_ghg.xs(_ref_scen, level="SCENARIO") - df_ghg.xs(
            _mit_scen, level="SCENARIO"
        )
    return df_mitigation.clip(0)


def fun_calculate_ghg(
    get_selection_dict,
    project_file,
    model,
    main_emi_sectors,
    file_emi,
    file_energy,
    add_co2,
    add_beccs,
    add_non_co2,
    c,
    ra="standard",
    aggregate_by_sectors=True,
    **kwargs,
):
    df_ghg = pd.DataFrame()
    emi_list = []
    if add_co2 or add_beccs:
        df_co2, emi_list = fun_load_co2(
            get_selection_dict,
            project_file,
            model,
            main_emi_sectors,
            file_emi,
            file_energy,
            add_co2=add_co2,
            add_beccs=add_beccs,
            ra=ra,
        )
        df_ghg = df_co2

    if add_non_co2:
        df_non_co2 = fun_load_non_co2(
            project_file,
            CONSTANTS.INPUT_DATA_DIR,
            gains_non_co2_file="GAINS_nonCO2_Global_21Feb2022_byGas.csv",
            gains_iso_dict="GAINS_ISO3.csv",
            aggregate_by_sectors=aggregate_by_sectors,
        )
        df_ghg=pd.concat([df_ghg, df_non_co2])
        df_ghg = fun_replace_d_dot_iso(df_ghg, "REGION")
    if c is not None:
        return df_ghg[df_ghg.index.get_level_values("REGION").isin(c)]
    return df_ghg


def fun_reshape_gains_iamc(
    df_gains: pd.DataFrame,
    dict_gains_iso: dict,
    scenario_dict: dict = {"CLE": "h_ndc", "MFR": "o_1p5c"},
    aggregate_by_sectors=True,
    CO2eq: bool = True,
) -> pd.DataFrame:
    """This function converts GAINS data in an IAMC format. Data are in Mt (by default MtCO2e otherwise just Mt)
    It returns a dataframe with the baseline and stab scenarios.
    """
    CO2eq_str = "CO2eq" if CO2eq else ""
    # CONVERT TO ISO CODES
    df_gains["country"] = [x + "_WHOL" for x in df_gains["country"]]
    df_gains["ISO"] = [dict_gains_iso.get(x, x) for x in df_gains["country"]]

    # Results dictionary for baseline and stab scenarios
    if aggregate_by_sectors:
        res_dict = {
            x: df_gains.rename(
                columns={
                    "AGGRsec2": "VARIABLE",
                }
            )
            .set_index(["AGGRsec3", "ISO", "year", "VARIABLE", "pollutant"])
            .groupby(["ISO", "year", "VARIABLE"])
            .sum()
            .unstack("year")[f"{x}_kt{CO2eq_str}"]
            .rename_axis(index={"ISO": "REGION"})
            / 1e3
            for x in scenario_dict
        }
    else:
        res_dict = {
            x: df_gains.rename(
                columns={
                    "pollutant": "VARIABLE",
                }
            )
            .set_index(["AGGRsec3", "ISO", "year", "VARIABLE", "AGGRsec2"])
            .groupby(["ISO", "year", "VARIABLE"])
            .sum()
            .unstack("year")[f"{x}_kt{CO2eq_str}"]
            .rename_axis(index={"ISO": "REGION"})
            / 1e3
            for x in scenario_dict
        }

    for x in scenario_dict:
        res_dict[x]["SCENARIO"] = scenario_dict[x]
        res_dict[x]["MODEL"] = "GAINS"
        res_dict[x]["UNIT"] = "Mt CO2-eq/yr" if CO2eq else "Mt"
        res_dict[x] = setindex(
            res_dict[x], ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"]
        )

    # Combine baseline and stab scenarios in one dataframe
    combi = pd.concat([res_dict[list(scenario_dict.keys())[0]],
        res_dict[list(scenario_dict.keys())[1]]])
    # combi = res_dict[list(scenario_dict.keys())[0]].append(
    #     res_dict[list(scenario_dict.keys())[1]]
    # )
    combi.columns.name = "TIME"
    return combi


def fun_load_non_co2(
    project_file,
    folder_dir,
    gains_non_co2_file="GAINS_nonCO2_Global_21Feb2022_byGas.csv",
    gains_iso_dict="GAINS_ISO3.csv",
    aggregate_by_sectors=True,
    CO2eq=True,
):
    """This function loads non-co2 emissions from gains. It returns a dataframe"""
    df_gains = pd.read_csv(folder_dir / f"{project_file}" / f"{gains_non_co2_file}")
    df_gains_iso = pd.read_csv(folder_dir / f"{gains_iso_dict}")
    df_gains_iso.rename(
        columns={"Alpha-3 code": "ISO", "GAINS Region aggregated": "REGION"},
        inplace=True,
    )
    dict_gains_iso = df_gains_iso.set_index("REGION")["ISO"].to_dict()

    return fun_reshape_gains_iamc(
        df_gains, dict_gains_iso, aggregate_by_sectors=aggregate_by_sectors, CO2eq=CO2eq
    )


def run_top_measures(
    df_ghg,
    baseline,
    stab,
    time_dict,
    emi_list,
    top: Union[None, int] = 20,
    _show_abs_emi: bool = False,
    c: Union[bool, list] = None,  # list of countries to be considered
    _mit_vs_base_year: Union[int, bool] = False,
    _add_c_budget_calc=True,
    **kwargs,
):
    """Run top measures analysis. It returns a dataframe with the top measures"""

    if _mit_vs_base_year:
        _mit_vs_base_year = 2010

    df_mitigation = fun_calculate_mitigation(
        df_ghg,
        baseline,
        stab,
        mitigation_vs_base_year=_mit_vs_base_year,
    )

    # Cumulative ghg emissions for carbon budget calculations (stab scenario)
    df_ghg_cum = fun_cumulative_emissions(df_ghg, False, baseyear=2010)
    df_ghg_cum = df_ghg_cum.xs(stab, level="SCENARIO")
    missing_eu_countries = list(
        set(fun_eu27()) - set(df_ghg_cum.reset_index().REGION.unique())
    )
    if len(missing_eu_countries):
        print(
            f"Cannot calculate total EU27, these coutries are missing: {missing_eu_countries}"
        )
    else:
        df_ghg_cum = fun_aggregate_countries(
            df_ghg_cum,
            "EU27",
            ["MODEL", "VARIABLE", "UNIT"],
            fun_eu27(),
            remove_single_countries=True,
        )

    if c is not None:
        if type(c) is not list:
            raise ValueError(f"c must be a list. you have entered a {type(c)} {c}")
        df_mitigation = df_mitigation[
            df_mitigation.index.get_level_values("REGION").isin(c)
        ]

    mytable = fun_create_short_long_emi_table(
        df_mitigation,
        df_ghg_cum,
        emi_list,
        time_dict,
        _mit_vs_base_year,
        _top=top,
        _show_abs_emi=_show_abs_emi,
        _add_c_budget_calc=_add_c_budget_calc,
    )

    # Add World results
    if _show_abs_emi:
        world = fun_create_short_long_emi_table(
            df_mitigation,
            df_ghg_cum,
            emi_list,
            time_dict,
            _mit_vs_base_year,
            _top=None,
            _show_abs_emi=_show_abs_emi,
            _add_c_budget_calc=_add_c_budget_calc,
        )

        world = fun_aggregate_countries(
            world,
            "World",
            None,
            None,
            remove_single_countries=True,
            _var_name="All sectors",
            _unit_name="MT CO2e/yr",
        )

        world = (
            world.reset_index()
            .set_index(["MODEL", "VARIABLE", "UNIT", "REGION"])
            .xs("World", level="REGION", drop_level=False)
        )

        top_countries_vs_global_mitigation = fun_global_mitigation_overview(
            df_mitigation,
            df_ghg_cum,
            emi_list,
            time_dict,
            _mit_vs_base_year,
            _add_c_budget_calc=_add_c_budget_calc,
        ).dropna(how="all")
        return pd.concat([mytable,world]), top_countries_vs_global_mitigation

    else:
        # NOTE here we do not have `top_countries_vs_global_mitigation`,
        # we return an empty df
        return mytable, pd.DataFrame()


def fun_csv_string_top_20(
    file_energy: str, _top: int, time_dict: dict, input_dict: dict
) -> str:
    """Returns a final name (.xlsx), based on input dictionary"""
    time_str = f"{time_dict['short']['from']}_{time_dict['short']['to']}_and_{time_dict['long']['from']}_{time_dict['long']['to']}"

    if input_dict["_mit_vs_base_year"] is False:
        scen_str = file_energy.replace(
            ".csv",
            f"_top_{_top}_table_{input_dict['baseline']}_vs_{input_dict['stab']}.csv",
        )
    else:
        scen_str = file_energy.replace(
            ".csv",
            f"_top_{_top}_table_{input_dict['stab']}_vs_{input_dict['_mit_vs_base_year']}_emissions.csv",
        )

    return scen_str.replace(".csv", f"_{time_str}.xlsx")


def fun_get_ssp(
    scenario: str,
    default_ssp: str = "SSP2",
    _scen_dict: Optional[Dict] = None,
) -> str:
    """This function detects the SSP based on the name or a mapping.

    If `scenario` is not present in `_scen_dict`, and the `scenario` name does not
    contain any SSP information, it returns `default_ssp`. If `default_ssp` is not
    given, an error is returned.

    Parameters
    ----------
    scenario : str
        IAM scenario to be assinged an SSP scenario
    default_ssp : str, optional
        Fallback option in case no match is found, by default "SSP2"
    _scen_dict : Dict, optional
        Mapping between scenario names and SSP, by default None

    Returns
    -------
    str
        SSP scenario

    Raises
    ------
    ValueError
        Multiple SSP information is found in the scenario name
    ValueError
        No match is found in `_scen_dict`, no information in the scenario name and no
        default scenario is provided.
    """

    ALLOWED_SSP = {f"SSP{x}" for x in range(1, 6)}

    fun_check_get_ssp_input(ALLOWED_SSP, default_ssp, _scen_dict)

    _scen_dict = (
        {scen.upper(): ssp for scen, ssp in _scen_dict.items()} if _scen_dict else {}
    )

    scenario_upper = scenario.upper()

    if scenario_upper in _scen_dict:
        return _scen_dict[scenario_upper]
    # if the scenario is not in the dict, check if the name contains information
    found_ssps = sorted([ssp for ssp in ALLOWED_SSP if ssp in scenario_upper])
    if len(found_ssps) == 1:  # if 1 is found, return
        return found_ssps[0]
    elif len(found_ssps) > 1:  # if more than 1 is found, error
        raise ValueError(
            f"We detected multiple SSPs in the scenario name:{found_ssps}. We "
            "need to use a single ssp in the downscaling algorithm. Please "
            f"change your scenario name: {scenario}."
        )
    # if none and a default is given, return the default
    elif not found_ssps and default_ssp is not None:
        return default_ssp
    # if none of the above raise error
    else:
        raise ValueError(
            f"Unable to find SSPs info in the scenario name: {scenario} or _scen_dict"
        )





def fun_pd_extend_time(
    _df_start: pd.DataFrame, 
    _time_list: List[Union[int, str]], 
    _time_str: bool = False
) -> pd.DataFrame:
    """
    Extends a time series DataFrame by adding new time points from a specified list.

    This function takes an initial DataFrame and a list of time points, extending the 
    DataFrame by appending rows for each time point in the list (excluding the first). 
    It can handle time points as either integers or strings, depending on the value of 
    the `_time_str` parameter.

    Args:
        _df_start (pd.DataFrame): The initial DataFrame to be extended.
        _time_list (List[Union[int, str]]): A list of time points to be added to the DataFrame.
        _time_str (bool): A flag indicating whether the time points should be treated as strings. 
                          Defaults to False.

    Returns:
        pd.DataFrame: The extended DataFrame containing the original data plus additional rows 
                      for each time point in `_time_list`.
    """
    _t_start = _time_list[0]
    _time_list.remove(_time_list[0])
    _prev_t = 1

    for _t in _time_list:
        if _time_str:
            _t = str(_t)
            _t_start = str(_t_start)
            _df_add = fun_pd_sel(_df_start, _t_start, "")
        else:
            _df_add = fun_pd_sel(_df_start, _t_start, "")

        setindex(_df_start, False)
        setindex(_df_add, False)

        _df_add["TIME"] = _t
        _df_add["DT"] = _t - _prev_t

        _df_start = _df_start.append(_df_add, ignore_index=False)
        _prev_t = _t

    return _df_start


def fun_get_time_list_from_time_range_string(test_str: str) -> list:
    """Convert String ranges to list
    Example:
    `test_str =1, 4-6, 8-10, 11`
    returns `[1, 4, 5, 6, 8, 9, 10, 11]`

    Credit: https://www.geeksforgeeks.org/python-convert-string-ranges-to-list/

    Args:
        test_str (str): your text representing a time range

    Returns:
        list: Your full list of time range
    """
    return sum(
        (
            (
                list(range(*[int(b) + c for c, b in enumerate(a.split("-"))]))
                if "-" in a
                else [int(a)]
            )
            for a in test_str.split(", ")
        ),
        [],
    )


def fun_mark_high_baseline_scenarios(
    res_all_dict: dict, idx_below_baseline: list, idx_below_2010: list, mit_calc: str
):
    ## summary of results - mark as high baseline if measure is also present when comparing below h_ndc
    df_summary = res_all_dict[mit_calc]["ghg"].droplevel("UNIT")
    if mit_calc == "o_1p5c vs 2010":
        df_summary["high baseline"] = "*"
        df_summary.loc[
            [x for x in idx_below_2010 if x not in idx_below_baseline], "high baseline"
        ] = ""

    return df_summary


def fun_add_governance_and_mark_countries_below_threshold(
    df_summary: pd.DataFrame, threshold: float = 0.6, ssp_scenario: str = "SSP2"
) -> pd.DataFrame:
    countrylist = df_summary.index.get_level_values("REGION").unique()
    df_gov = fun_read_gov()
    time_sel = fun_get_time_list_from_time_range_string(
        f"{df_summary.columns[0]}, {df_summary.columns[1]}"
    )

    # Take an average of governance across the selected time periods `time_sel`
    df_gov_sel = fun_get_selected_governance_data(
        ssp_scenario, countrylist, df_gov, time_sel, sel_gov=None
    )

    # # Add EU27 governance indicators (median across all countries)
    # eu27_gov = fun_eu27_gov(ssp_scenario, df_gov, time_sel)

    df_gov_sel=df_gov_sel['governance']
    countries_with_low_governance = df_gov_sel[df_gov_sel < threshold].index.to_list()
    # if eu27_gov['governance'] < threshold:
    #     countries_with_low_governance = countries_with_low_governance + ["EU27"]

    df_summary["low governance"] = ""
    df_summary.loc[
        df_summary.index.get_level_values("REGION").isin(countries_with_low_governance),
        "low governance",
    ] = "*"

    return fun_add_governance_data(
        df_summary, ssp_scenario, countrylist, df_gov, time_sel
    )

def fun_read_gov()->pd.DataFrame:
    """Returns dataframe with Governance indicators

    Returns
    -------
    pd.DataFrame
        dataframe with Governance indicators
    """    
    df_gov= (
        pd.read_csv(
            CONSTANTS.INPUT_DATA_DIR / "master_proj_obs.csv",
            sep=",",
            encoding="latin-1",
        )
        .rename(
            columns={
                "gov.eff": "eff",
                "corr.cont": "corr",
                "countrycode": "ISO",
                "year": "TIME",
            }
        )
        .set_index("ISO")
    )

    # Add governance for R5 regions:
    grassi_reg_mapping = (
        pd.read_csv(CONSTANTS.INPUT_DATA_DIR / "Grassi_regional_mapping.csv")
        .dropna()
        .set_index(["R5_region", "ISO"])
    )
    grassi_dict=grassi_reg_mapping.reset_index().set_index('ISO')['R5_region'].to_dict()
    f=fun_add_governance_for_missing_countries_using_median
    df_gov=f(df_gov,fun_invert_dictionary({k:[v] for k,v in grassi_dict.items()}))
    
    # Add governance for Missig countries based on R5 regions
    df_gov=f(df_gov,{k:[v] for k,v in grassi_dict.items() if k in all_countries and k not in df_gov.index.unique()})

    # Add governance indicators for EU27
    df_gov=fun_add_governance_for_missing_countries_using_median(df_gov, {'EU27':fun_eu27()})
    return df_gov
    


def fun_eu27_gov(ssp_scenario, df_gov, time_sel):
    indicators=['governance', 'eff', 'corr', 'readiness']
    return (
        df_gov[(df_gov.TIME.isin(time_sel)) & (df_gov.scenario == ssp_scenario)]
        .loc[fun_eu27()][indicators]
        .median()
    )


def fun_add_governance_data(df_summary, ssp_scenario, countrylist, df_gov, time_sel):
    selected_periods = list(df_summary.columns[:2])
    combi = [", ".join(selected_periods)]
    gov_dict = {}
    for x in selected_periods + combi:
        time_sel = fun_get_time_list_from_time_range_string(x)
        gov_dict = fun_get_selected_governance_data(
            ssp_scenario, countrylist, df_gov, time_sel, sel_gov=None
        ).to_dict()

        ## HERE CHECK WHY WE ONLY HAVE governace as key in our dict
        for k,d in gov_dict.items():
        #     gov_dict["EU27"] = fun_eu27_gov(ssp_scenario, df_gov, time_sel)

            if x == combi[0]:
                # Combined (short/long period) governance indicators. We just call this 'gov'
                # d["EU27"] = fun_eu27_gov(ssp_scenario, df_gov, time_sel)[k]
                df_summary[k[:3]] = [
                    d.get(x, 0) for x in df_summary.index.get_level_values("REGION")
                ]

            else:
                # Governance data for each time period (short/long term) e.g: gov_2025_2030
                df_summary[f"{k[:3]}_{x}"] = [
                    d.get(x, 0) for x in df_summary.index.get_level_values("REGION")
                ]

    return df_summary


def fun_get_selected_governance_data(ssp_scenario, countrylist, df_gov, time_sel, sel_gov=["governance"]):
    allowed=['governance', 'eff','corr', 'readiness']
    sel_gov= sel_gov or allowed
    errors=[x for x in sel_gov if x not in allowed]
    if len(errors):
        raise ValueError(f'Cannot select: {errors}. Allowed indicators are: {allowed}')
    df=df_gov[(df_gov.TIME.isin(time_sel)) & (df_gov.scenario == ssp_scenario)]
    return (
        (
            df[df.index.get_level_values(0).isin(countrylist)]
            .dropna(how="all")[sel_gov]
        )
        .groupby("ISO")
        .mean()
    )


def fun_global_mitigation_overview(
    df_mitigation: pd.DataFrame,
    df_ghg_cum,
    emi_list: list,
    time_dict: dict,
    _mitigation_vs_base_year: Union[int, bool],
    _add_c_budget_calc=True,
) -> pd.DataFrame:  # sourcery skip: avoid-builtin-shadow
    """This function calculate % of Global mitigation as a function of number countries
     (top x countries considered)

    # NOTE: % of Mitigation here will not be the same as in the df_summary (main) table.
    Because the main table (both short ternm and long term colums) is sorted based on
    short term measures. This means that the long term measures are sorted based on
    the short-term measure. This function instead sorts global mitigation in the
    short term and in the long term individually.

    Args:
        df_mitigation (pd.DataFrame): Your dataframe with mitigation data
        emi_list (list): list of emissions included
        time_dict (dict): dictionary with the time period considered in the analysis
        _mitigation_vs_base_year (Union[int, bool]): Calculate mitigation below the base
         year (e.g. 2010) or the baseline scenario

    Returns:
        pd.DataFrame: Dataframe with % of global mitigation as a function of top x countries
    """

    df_all = pd.DataFrame()
    data = fun_create_short_long_emi_table(
        df_mitigation,
        df_ghg_cum,
        emi_list,
        time_dict,
        _mitigation_vs_base_year,
        _top=None,
        _show_abs_emi=False,
        _add_c_budget_calc=_add_c_budget_calc,
    )
    for x in data.columns:
        d = data.loc[:, x].sort_values(ascending=False).cumsum(axis=0).reset_index()[x]
        df_all[x] = d
    df_all.index = df_all.index + 1
    df_all.index.names = ["Top - Number of countries"]
    return df_all


def get_text_location(
    c, df, colsx, const=0.1, coly="gov_complement_to_one", where="upper left"
):
    datay = df[df.REGION == c][coly].unique()[0]
    # xmin=df[df.REGION == c][colsx].min()
    x_idx = max(0, df[df.REGION == c].index.min() - 1)
    xmin = df.loc[x_idx, colsx][0]
    # xmin = df[df.REGION == c][colsx].min() - df.iloc[1:, :][colsx].min()
    xmax = df[df.REGION == c][colsx].max()
    if where == "upper left":
        y = datay + 0.003
        x = xmin
    else:
        y = datay - const
        x = (xmax + xmin) / 2 - 10
    return (x, y, c)


def fun_order_legend(by_label, coldict):
    return {x: by_label[x] for x in coldict.keys() if x in by_label.keys()}


def fun_macc_graph(
    df, cols, step, coldict, sel_y="gov", sel_countries=None, complement_to_one:bool=True,
):
    plt.rcParams["figure.figsize"] = (20, 10)
    y=sel_y
    y_label=f'1 - {sel_y}' if complement_to_one else y
    if complement_to_one:
        y=f"{sel_y}_complement_to_one"
        df[y] = 0
            
        
    for col in cols:
        df.loc[f'{y}_{col}']=0
        if complement_to_one:
            df.loc[:, f'{y}_{col}'] = 1 - df.loc[:, f"{sel_y}_{col}"]
        # else:
        #     df.loc[:, f'{y}_{col}'] = df.loc[:, f"{sel_y}_{col}"]
        df_sel = df[df[col] > 0].copy(deep=True)
        allowed_cols = list(set([
            "VARIABLE",
            "REGION",
            col,
            f"{sel_y}_{col}",
            y,
            f"{col}_cum",
            f"{y}_{col}"
        ]))

        df_sel = fun_sort_y_values_and_cumsum_x_values(df_sel, x=col, y=f"{y}_{col}")[allowed_cols]

        upper = 0.6 if complement_to_one else None
        col = fun_simple_macc(step, coldict, f"{y}_{col}", plt, col, df_sel, y_label=y_label, upper=upper)

        if sel_countries is None:
            # All countries
            df_clist = (
                df_sel.groupby("REGION")
                .sum()
                .sort_values(col.replace("_cum", ""), ascending=False)[
                    col.replace("_cum", "")
                ]
            )
            # NOTE Top 10 countries
            c_list = df_clist[df_clist > 0].index[:10]
        if len(c_list) < 20:
            for c in c_list:
                a = get_text_location(c, df_sel, [col], where="upper left", coly=f"{y}_{col.replace('_cum','')}")
                plt.text(a[0], a[1], c)

    return plt


def fun_simple_macc(step, coldict, y, plt, col, df_sel, title_left:str="Mitigation under 1.5C pathway vs",
                    y_label:str="1 - gov", upper=None):
    title_dict={'gov':'governance',
                'eff':'efficiency',
                'rea':'readiness',
                'cor':'corruption'}
    
    title_left=f"{title_left} {title_dict.get(y_label,y_label)}"
    col = f"{col}_cum"
    # NOTE: The MACC curve should start from x = 0:
    df_sel.loc[:, col] = df_sel.loc[:, col] - df_sel[col].min()
    for i in df_sel.index[:]:
        color = [
            coldict.get("black", j) for j in [coldict[df_sel[: i + 1].VARIABLE[i]]]
        ]
        plt.fill_between(
            df_sel[i - 1 : i + 1][col],
            df_sel[i - 1 : i + 1][y],
            step=step,
            alpha=0.9,
            color=color[0],
            label=df_sel.loc[i].VARIABLE,
        )

        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = OrderedDict(zip(labels, handles))
        new_dict = fun_order_legend(by_label, coldict)
        plt.legend(new_dict.values(), new_dict.keys(), loc="upper left", ncol=1)

    plt.step(df_sel[:][col], df_sel[:][y], c="Black", alpha=0.5, where=step)
    col_name = col.replace("_cum", "")
    plt.title(f"{title_left} | {col_name} ")
    plt.xlabel("Mt CO2e/yr")
    plt.ylabel(title_dict.get(y_label,y_label))
    if upper:
        plt.ylim(top=upper)
    return col


def fun_sort_y_values_and_cumsum_x_values(df, x, y="gov_complement_to_one"):
    df=df.copy(deep=True)
    add = pd.DataFrame(df.iloc[0, :]).T
    add.loc[:, :] = 0
    # add.loc[:, 'gov_complement_to_one']=0.01
    df=pd.concat([add, df])
    # df = add.append(df)

    # Sort values and cumulative sum
    df = df.sort_values(y)
    df = fun_reset_index_and_fill(df)
    df[f"{x}_cum"] = df[x].fillna(0).cumsum()
    # df[f"{col}_cum"] = df[col].fillna(0).cumsum()+df[col]
    return df


def fun_reset_index_and_fill(df):
    if "level_0" in df.columns:
        df = df.drop("level_0", axis=1)
    df = df.reset_index()
    if "level_0" in df.columns:
        df = df.drop("level_0", axis=1)

    # we fill na with previous values (added on 3rd October)
    # NOTE: we need this, even if this will mess up the 2025-2030 or 2040-2050 column
    # (we don't use these columns anyway in the graph)
    # df = df.set_index(['MODEL','VARIABLE','REGION']).ffill()
    df = df.fillna(0)

    # reset index
    for x in ["level_0", "index"]:
        if x in df.columns:
            df = df.drop(x, axis=1)
        df = df.reset_index()
    return df


# NOTE: need to remove last row from dataset as it contains world data
def fun_plot_all_maccs_graphs(
    df_summary: pd.DataFrame, legend_dict: dict, _step="pre", complement_to_one:str=True, indicator:str='gov'
) -> plt.plot:
    """
    Creates a MACC-type  mitigation curve, by ranking mitigation measures based on governance indicators instead of costs.

    Args:
        df_summary : Dataframe with data
        legend_dict (dict): Color dictionary to be used for the legend
        _step : {'pre','post','mid'}, optional
    Define step if the filling should be a step function, i.e. constant in between x. The value determines where the step will occur:
       - 'pre': The y value is continued constantly to the left from every x position, i.e. the interval (x[i-1], x[i]] has the value y[i].
       - 'post': The y value is continued constantly to the right from every x position, i.e. the interval [x[i], x[i+1]) has the value y[i].
       - 'mid': Steps occur half-way between the x positions.
    Returns:
        plt.plot: MACC plot
    """
    for x in df_summary.columns[:2]:
        fun_macc_graph(df_summary.reset_index().iloc[:-1], [x], _step, legend_dict, sel_y=indicator
                       , complement_to_one=complement_to_one
                       )
        plt.show()


def fun_check_additional_long_term_measures(res_dir, res_all_dict):
    # Check common indexes in two tables (below h_ndc and below 2010):
    idx1, idx2 = fun_get_idx_mitigation(res_all_dict)
    print("===========")
    print(
        "Additional measures to be considered in the long term (not present in df_summary):"
    )
    print([x for x in idx1 if x not in idx2])
    df_additional_long_term_measures = (
        res_all_dict["o_1p5c vs h_ndc"]["ghg"]
        .droplevel("UNIT")
        .loc[[x for x in idx1 if x not in idx2]]
    )
    print(df_additional_long_term_measures)
    df_additional_long_term_measures.to_csv(
        res_dir / "Additional_long_term_measures.csv"
    )
    return df_additional_long_term_measures


def fun_get_idx_mitigation(res_all_dict):
    # Check common indexes in two tables (below h_ndc and below 2010):
    idx1 = res_all_dict["o_1p5c vs h_ndc"]["ghg"].droplevel("UNIT").index
    idx2 = res_all_dict["o_1p5c vs 2010"]["ghg"].droplevel("UNIT").index
    return idx1, idx2


def fun_check_path_lenght(file_path_lenght):
    if len(file_path_lenght) > 259:
        raise ValueError(
            f"xls_file_str string is too long. Full excel file path lenght needs to be smaller than 259 characters. Curent lenght: {file_path_lenght}"
        )


def fun_top_20_analysis(
    input_dict: dict,
    _mit_vs_base_year: Union[int, str],
    top: int = None,
    _save_to_xls: bool = False,
    input_gases: Union[bool, dict] = None,
    df_ghg: Union[bool, pd.DataFrame] = None,
    ra: Union[str, int] = "standard",
    **kwargs,
) -> dict:
    """Run analysis for a single mitigation setting (`_mit_vs_base_year`) and for all `input_gases` settings
    and saves results to excel.

    Parameters
    ----------
    input_dict : dict
        Dictionary with input parameters
    _mit_vs_base_year : Union[int, str]
        Mitigation setting (e.g. calculates mitigation below 2010 or below h_ndc scenarios)
    top : int, optional
        _description_, by default None
    _save_to_xls : bool, optional
        Saves results to excel, by default True
    input_gases : Union[bool, dict], optional
        Define your gases settings (e.g. runs the  analysis for all GHG emission, only CO2, etc.), by default None
    df_ghg : Union[bool, pd.DataFrame], optional
        Dataframe with GHG emissions data, by default None

    Returns
    -------
    dict
        Dictionary with results for all `input_gases` settings
    """
    from fixtures import time_dict_top20 as time_dict

    if input_gases is None:
        input_gases = {
            # "co2": {"add_co2": True, "add_beccs": True, "add_non_co2": False,},
            # "co2_excl_beccs": {"add_co2": True, "add_beccs": False, "add_non_co2": False,},
            # "non_co2": {"add_co2": False, "add_beccs": False, "add_non_co2": True,},
            # "beccs": {"add_co2": False, "add_beccs": True, "add_non_co2": False,},
            "ghg": {
                "add_co2": True,
                "add_beccs": True,
                "add_non_co2": True,
                # "_add_c_budget_calc":False,
            },
        }

    input_dict["_mit_vs_base_year"] = _mit_vs_base_year

    # Store results in a dictionary
    # NOTE: results unit is MtCO2/yr (average annual mitigation ovber selected time period) - this is also useful when calculating emissions below 2010
    df_ghg, res_dict = fun_run_analysis_all_gases_definition_settings(
        input_dict, input_gases, df_ghg, ra
    )

    if _save_to_xls:
        fun_save_analysis_to_excel(
            input_dict["file_energy"], top, input_dict, res_dict, time_dict
        )

    return res_dict


def fun_run_analysis_all_gases_definition_settings(input_dict, input_gases, df_ghg, ra):
    res_dict = {}
    input_dict["ra"] = ra
    for ig, value in input_gases.items():
        for x in value.keys():
            input_dict[x] = input_gases[ig][x]
        if df_ghg is None:
            df_ghg = fun_calculate_ghg(**input_dict)
        input_dict["df_ghg"] = df_ghg.copy(deep=True)
        input_dict["emi_list"] = list(
            df_ghg.index.get_level_values("VARIABLE").unique()
        )
        res_dict[ig], res_dict[ig + "_perc_global"] = run_top_measures(**input_dict)
    return df_ghg, res_dict


def fun_save_analysis_to_excel(
    file_energy, _top, input_dict, res_dict, time_dict_top20
):
    RESULTS_DATA_DIR = CONSTANTS.CURR_RES_DIR('step5') if callable(CONSTANTS.CURR_RES_DIR) else CONSTANTS.CURR_RES_DIR
    xls_file_str = fun_csv_string_top_20(file_energy, _top, time_dict_top20, input_dict)
    csv_out = RESULTS_DATA_DIR / xls_file_str
    fun_check_path_lenght(csv_out)
    # Save results to excel (in different sheets)
    with pd.ExcelWriter(csv_out) as writer:
        for i in res_dict:
            (res_dict[i]).to_excel(writer, sheet_name=i, index=True)


def fun_create_summary_table_and_save_to_csv(res_all_dict, res_dir, mit_calc):
    from fixtures import var_dict_top20

    idx1, idx2 = fun_get_idx_mitigation(res_all_dict)

    df_summary = fun_mark_high_baseline_scenarios(res_all_dict, idx1, idx2, mit_calc)
    df_summary = df_summary.rename(var_dict_top20)
    df_summary = fun_add_governance_and_mark_countries_below_threshold(df_summary)
    df_summary.to_csv(res_dir / "Main_table.csv")
    return df_summary


def fun_read_df_iea_act_and_reshape(
    gains_data_path: Union[Path, str], activity_iea_path: Union[Path, str]
) -> pd.DataFrame:
    """This function reads IEA activity data and reshape them, using ISO dictionary `dict_gains_iso`.

    Args:
        gains_data_path (Union[Path, str]): Path to Gains data (needed to create a dictionary)
        activity_iea_path (Union[Path, str]): Path to iea activity data

    Returns:
        pd.DataFrame: Contains IEA activity data
    """
    # Step 1 reading iso codes mapping of GAINS
    df_gains_iso = pd.read_csv(gains_data_path)
    df_gains_iso.rename(
        columns={"Alpha-3 code": "ISO", "GAINS Region aggregated": "REGION"},
        inplace=True,
    )
    dict_gains_iso = df_gains_iso.set_index("REGION")["ISO"].to_dict()

    # Step 2 reading IEA activity data
    df_iea_act = pd.read_csv(activity_iea_path)
    df_iea_act.rename(
        columns={
            "IDYEARS": "TIME",
        },
        inplace=True,
    )
    df_iea_act["REGION"] = [dict_gains_iso[x] for x in df_iea_act.EC_REGION]
    df_iea_act["LENISO"] = [len(x) for x in df_iea_act.REGION]
    aggregated_countries = df_iea_act[
        df_iea_act["LENISO"] != 3
    ].REGION.unique()  ## regions with aggregated data

    # Step 3: Reshape IEA activity data
    # Here we create Single data for aggregated countries (for each country we use the same data as in the region):
    # Example: [UZB, TKM, TJK] are part of the FSUA_WHOL region of GAINS.
    # For each country [UZB, TKM, TJK] we use the same data as the FSUA_WHOL region (we append the same regional data for each country).
    for x in aggregated_countries:
        mylist = [y.replace(" ", "") for y in x.split(",")]

        for y in mylist:
            df_single_iso = df_iea_act[df_iea_act.REGION.str.contains(x)]
            df_single_iso.loc[:, "REGION"] = y
            df_iea_act = pd.concat([df_iea_act,df_single_iso])
    df_iea_act.loc[:, "LENISO"] = [len(x) for x in df_iea_act.REGION]

    ## we only take data for countries (len(iso_code)==3)
    df_iea_act = df_iea_act[df_iea_act.LENISO == 3]
    df_iea_act.rename(columns={"EMISSION_CLOCK_SECTOR": "VARIABLE"}, inplace=True)
    df_iea_act = (
        df_iea_act.groupby(["TIME", "REGION", "VARIABLE"])
        .sum()["ACTIVITY"]
        .unstack("TIME")
    )
    tot = df_iea_act.groupby(["REGION"]).sum()  # e.g. tot transport data
    df_iea_act = df_iea_act / tot  # calculating activity as share for each country
    df_iea_act["MODEL"] = "your model"
    df_iea_act["SCENARIO"] = "your scenario"
    df_iea_act["UNIT"] = "share"
    return df_iea_act


def fun_get_downscaled_base_year_data(
    df_down: pd.DataFrame,
    _var: str,
    _baseyear: Union[int, str],
    list_countries: list,
    default_scen: str = "h_cpol",
) -> pd.Series:
    """
    Select base year `_baseyear` data from downscaled results `df_down` for a given list of countries `list_countries`,
    a given variable `var` and a default scenario `default_scen`.
    It returns a pd series
    """

    missing_countries_data = df_down[
        df_down.index.get_level_values("REGION").isin(list_countries)
    ]
    if len(missing_countries_data) > 0:
        if default_scen in df_down.index.get_level_values("SCENARIO").unique():
            pick_one_scen = default_scen
        else:
            pick_one_scen = df_down.index.get_level_values("SCENARIO").unique()[0]
            print(
                f"Warning: {default_scen} not present in df_downs. We use {pick_one_scen} scenario instead (for the base year value)"
            )

        return missing_countries_data.xs(
            (_var, pick_one_scen), level=("VARIABLE", "SCENARIO")
        )[_baseyear].droplevel(["MODEL", "UNIT"])


def fun_hist_iea_share(
    _var: str,
    _df_iea: pd.DataFrame,
    iea_var_dict: dict,
    _countrylist: list,
    _baseyear: str = "2010",
    df_down: pd.DataFrame = None,
    keep_data_if_missing_iea_data=True,
    conversion=1e3,
    aliased_variables_suffix=" v2",
) -> pd.Series:
    """Returns the (historical) share of a country in a given list of countries (countrylit) for a given
    variable (_var) and a given base year (_baseyear).
    If the `_baseyear` (e.g. 2020) is beyond the latest historical data, it returns the latest historical data (e.g. 2016)
    """

    # EXAMPLE: fun_hist_iea_share("Emissions|CO2|Energy excl BECCS", df_emi, iea_var_dict, ['USA','CAN'])

    ## Coerce to numeric data (at the base year, which is what we need)
    # You can do this for all columns in a loop: cols = [str(x) for x in range(1960, 2018, 1)]
    _df_iea[_baseyear] = pd.to_numeric(
        _df_iea[_baseyear], errors="coerce", downcast="float"
    )

    # NOTE _baseyear can be outside available historical data (e.g. 2025)
    # Below we scan for latest available data, to check if _baseyear is beyon last histoircal data.
    # If this is the case, we go back by one year, until we reach 2010
    country_data = fun_get_hist_data(
        _var, _df_iea, iea_var_dict, _countrylist, _baseyear
    )

    if keep_data_if_missing_iea_data:
        if df_down is None or len(df_down) == 0:
            raise ValueError(
                "The df_down is empty. Please provide a non-empty Dataframe if you want to keep previously downscaled data for countries without historical IEA data"
            )
        unit_downs_data = df_down.index.get_level_values("UNIT").unique()[0]
        if "CO2 fuel combustion" in _df_iea.reset_index().FLOW.unique():
            if unit_downs_data != "Mt CO2/yr" and conversion == 1e3:
                raise ValueError(
                    f"In the downscaled data the unit of {_var} is {unit_downs_data}, instead of Mt CO2/yr.\n"
                    f"Please check your conversio rate (currently we multiply the downscaled data by {conversion}"
                    f"to get KtCO2, which might to be wrong."
                )
        missing_iea_countries = [x for x in _countrylist if x not in country_data.index]

        # If countries are not available from IEA historical data we use the downscaled results at the base year (e.g.MAC not available in IEA data)
        if _var in fun_xs(df_down, {"REGION": missing_iea_countries}):
            missing_countries_data = fun_get_downscaled_base_year_data(
                df_down,
                _var.replace(aliased_variables_suffix, ""),
                _baseyear,
                missing_iea_countries,
                default_scen="h_cpol",
            )
            if missing_countries_data is not None:
                country_data = pd.concat([country_data,missing_countries_data * conversion])

    return country_data / country_data.sum()


def fun_get_hist_data(
    _var: str,
    _df_iea: pd.DataFrame,
    iea_var_dict: dict,
    _countrylist: list,
    _baseyear: str,
) -> pd.Series:
    """Get historical data for a selected `_baseyear` from `_df_iea` for a given variable `_var` and a `countrylist`.
    If `_baseyear` (e.g. 2020) is beyond latest historical data, it returns the data from the latest available
    historical data (e.g. 2016).

    Parameters
    ----------
    _var : str
        Variable
    _df_iea : pd.DataFrame
        IEA historical data
    iea_var_dict : dict
        Dictionary with IEA flow and Product associated to `_var`
    _countrylist : list
        List of countries (for which you want to get historical data)
    _baseyear : str
        The historical data that you are looking for (e.g. 2016).

    Returns
    -------
    pd.Series
        Historical data from IEA
    """
    my_range = list(range(int(_baseyear), 2009, -1))
    country_data = pd.Series([0])
    for t0 in my_range:
        if country_data.sum() == 0:
            t = t0
            if str(t) in _df_iea.columns:
                _df_iea[str(t)] = pd.to_numeric(
                    _df_iea[str(t)], errors="coerce", downcast="float"
                )
                country_data = (
                    _df_iea[
                        (
                            _df_iea.index.get_level_values("FLOW").isin(
                                iea_var_dict[_var]["flow"]
                            )
                        )
                        & (
                            _df_iea.index.get_level_values("PRODUCT").isin(
                                iea_var_dict[_var]["product"]
                            )
                        )
                        & (_df_iea.index.get_level_values("ISO").isin(_countrylist))
                    ][str(t)]
                    .groupby("ISO")
                    .sum()
                )

    return country_data


def hist_share_multiplied_by_growth_rate(
    _var: str,
    df_emi: pd.DataFrame,
    iea_var_dict: dict,
    countrylist: list,
    baseyear: Union[str, int],
    df_downs: pd.DataFrame,
) -> pd.DataFrame:
    """This function multiplies historical shares (at the county level) with growth rates from previosly downscaled
    results, for a given variable and a given countrylist (region). It applies to all targets.
    It returns the updated dataframe (sliced for variable _var)


    Parameters
    ----------
    _var : str
        variable
    df_emi : pd.DataFrame
        dataframe with historical emissions data (from IEA)
    iea_var_dict : dict
        Dictionary with IEA flow and product associated to a given `var`
    countrylist : list
        List of countries within a region
    baseyear : Union[str, int]
        base year for emissions harmonization (we replicate base year emissions)
    df_downs : pd.DataFrame
        Dataframe with downscaled results

    Returns
    -------
    pd.DataFrame
        Updated downscaled results
    """

    ## combine historical data and growth rates
    hist = fun_hist_iea_share(
        _var,
        df_emi,
        iea_var_dict,
        countrylist,
        _baseyear="2010",
        df_down=df_downs,
        keep_data_if_missing_iea_data=True,
    )

    growth = fun_growth_index(df_downs, baseyear)
    cols = growth.columns

    if "REGION" in list(growth.reset_index().columns):
        hist.index.names = ["REGION"]

    ## Same historical data for all time periods
    hist = pd.DataFrame(hist, columns=cols).apply(lambda x: hist, axis=0)

    # We set a maximum to the growth rates:
    ## Multuply hist data by gwoth rates
    updated_data = fun_xs((hist * growth),{"VARIABLE":_var})

    # Harmonised data to match regional iam results
    tot = updated_data.groupby(["MODEL", "SCENARIO", "UNIT"]).sum()
    if tot.max().max() !=0 :
        updated_data = updated_data / tot
        # NOTE As an alternative to fix division by zero error you can also do: `tot.replace(0, np.nan)`
    return updated_data


def fun_anchor_single_var_to_hist_data(
    _df_sect_harm: pd.DataFrame,
    var: str,
    df_emi: pd.DataFrame,
    iea_var_dict: dict,
    countrylist: list,
    baseyear: str = "2010",
    growth_rate_harmonization=True,
) -> pd.DataFrame:
    """This function updates the downscaled dataframe `_df_sect_harm`
    by anchoring emissions to historical data.
    At the `baseyear`, the variable (`var`) will be allocated to the country level using
    the historical data % allocation across countries in the region.
    After the base year, we apply groth rates (same groth rates as before), and adjust to match regional data (sum of country-level shares =1 in each time period).

    # NOTE: If a country does not have historical data, historical share data will be equal to nan. ()
    Example: fun_hist_iea_share("Emissions|CO2|Energy excl BECCS", df_emi, iea_var_dict, ['SSD','AUS']) => returns 'AUS'=1, 'SDN'=NaN

    The list of countries in a given region is defined by the `countrylist`.
    Historical emissions data are taken from `df_emi`.
    The conversion from IEA variables names and IAM variables nomenclature is given by `iea_var_dict`

    It returns the updated DataFrame.
    """

    # NOTE:IAM results might not contain info for our custum-made variable e.g."Emissions|CO2|Energy excl BECCS"
    # We trust that in the  _df_sect_harm the sum of cuntry level results matches IAM results
    # Below we multiply regional (sum of country level) results from _df_sect_harm by hist_share_multiplied_by_growth_rate

    reg_data = (
        fun_xs(_df_sect_harm, {"VARIABLE":var})
        .groupby(["MODEL", "SCENARIO", "UNIT", "VARIABLE"])
        .sum()
    ).sort_index()

    # Avoid division by zero errors by replacing 0 with a small value (otherwise creates problem in calculating the growth rates)
    # NOTE value should not be too small to avoid excessive growth rates in some countries. Changing this value can significantly affect the results!!
    # small_value = # 1e-9  # 0.001
    small_value = (
        0.01  # small thresholds (e.g. 1e-9) does not work for countries like EGY
    )

    if growth_rate_harmonization:
        df_with_adj_hist_data = hist_share_multiplied_by_growth_rate(
            var,
            df_emi,
            iea_var_dict,
            countrylist,
            baseyear,
            _df_sect_harm.replace(0, small_value),
        ).sort_index()

        updated_results = (
            df_with_adj_hist_data.reset_index().set_index(
                ["MODEL", "SCENARIO", "UNIT", "VARIABLE", "REGION"]
            )
            * reg_data
        )
    else:
        # use difference instead of growth rates below
        df_with_adj_hist_data = hist_share_plus_diff_in_downs_data(
            var,
            df_emi,
            iea_var_dict,
            countrylist,
            baseyear,
            _df_sect_harm.replace(0, small_value),
        ).sort_index()

        idx_names = ["MODEL", "SCENARIO", "UNIT", "VARIABLE", "REGION"]
        country_shares = (
            df_with_adj_hist_data.reset_index().set_index(idx_names)
            / df_with_adj_hist_data.reset_index()
            .set_index(idx_names)
            .groupby(["MODEL", "SCENARIO", "UNIT", "VARIABLE"])
            .sum()
        )

        updated_results = country_shares * reg_data

    # todo:
    # Harmonization using offeset instead of growth rates. With option to clip data to positive values only.

    # set index of _df_sect_harm
    updated_results = updated_results.reset_index().set_index(_df_sect_harm.index.names)

    # replace _df_sect_harm results with updated_results
    idx = updated_results.index
    if len(idx) > 0:
        cols = [x for x in updated_results if int(x) >= int(baseyear)]
        _df_sect_harm.loc[idx, cols] = updated_results.loc[idx, cols]
    return _df_sect_harm


def fun_fillna_multindex_level(
    df: pd.DataFrame,
    level_fillna: str,
    value: Union[str, float, int],
):
    """This function fillna on a given multindex level `level_fillna`, with a specific `value`
    It returns the updated `df`.
    """
    myidx = df.index.names
    df.reset_index(inplace=True)
    df[level_fillna] = df[level_fillna].fillna(value)
    return df.set_index(myidx)


col_iam_dict = {
    "ISO": "REGION",
    "TARGET": "SCENARIO",
    "SECTOR": "VARIABLE",
}
iamc_idx_names = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]

step1_col_dict = {
    "VARIABLE": "SECTOR",
    "REGION": "ISO",
    "SCENARIO": "TARGET",
}


def fun_setindex_iso_target_sector(df: pd.DataFrame) -> pd.DataFrame:
    return setindex(
        df.rename_axis(
            index={
                "VARIABLE": "SECTOR",
                "REGION": "ISO",
                "SCENARIO": "TARGET",
            }
        ),
        ["ISO", "TARGET", "SECTOR"],
    )


def fun_alias_var(my_list_duplicated_var: list, suffix: str = " v2") -> pd.DataFrame:
    """Create a dictionary of alias variable by adding a suffix, for all items in `my_list_duplicated_var`.

    Example:
    `my_list_duplicated_var` = ["Emissions|CO2|Energy excl BECCS v2"]
    `suffix` =  " v2"
    returns  {"Emissions|CO2|Energy excl BECCS" : "Emissions|CO2|Energy excl BECCS v2"}
    """
    return {x.replace(suffix, ""): x for x in my_list_duplicated_var}


def fun_list_duplicated_var_dict(new_var_dict: dict, mysuffix: str = " v2") -> dict:
    """This checks for duplicated variables in new_var_dict.keys().
    Duplicated values are new_var_dict.keys() that contain the `mysuffix` string.
    It returns a dictionary of duplicated (aliased) variables.
    Example: {"Emissions|CO2|Energy excl BECCS" : "Emissions|CO2|Energy excl BECCS v2"}


    """
    my_list_duplicated_var = [x for x in new_var_dict.keys() if x.find(mysuffix) != -1]
    return fun_alias_var(my_list_duplicated_var, suffix=mysuffix)


def fun_overvrite_values(
    df_sect_harm: pd.DataFrame, mydict: dict, scalar=1
) -> pd.DataFrame:
    """This function overwrites values in `df_sect_harm.values()` based on `df_sect_harm.keys()` multiplied by a `scalar`
    It returns the updated dataframe.
    """
    for i, j in mydict.items():
        idx_to_copy = fun_xs(df_sect_harm, {"SECTOR":i}).index
        idx_to_overwrite = fun_xs(df_sect_harm, {"SECTOR":j}).index

        cols = range(1990, 2200)
        cols = [str(x) for x in cols]
        cols = [x for x in cols if x in df_sect_harm.columns]

        df_mult = df_sect_harm[cols] * scalar
        if len(idx_to_copy) > 0:# and len(idx_to_overwrite) > 0:
            df_sect_harm.loc[idx_to_overwrite, cols] = df_mult.loc[
                idx_to_copy, cols
            ].droplevel("SECTOR")
        else:
            print(f"Warning: {i} or {j} not present in df_sect_harm")

    return df_sect_harm


class SliceableDict(dict):
    def slice(self, *keys):
        return {k: self[k] for k in keys}


def fun_split_emissions_by_fuel_and_mode(
    downs_res_file: str,
    csv_out: str,
    iamc_index: list,
    df_iam_all_models: pd.DataFrame,
    df_iea_act: pd.DataFrame,
    emi_factor: dict,
    eff_factor: dict,
    energy_dict: dict,
    model: str,
    # count: int,
    df_downs: pd.DataFrame,
    mymodel_dict: dict,
    countrylist: list,
    reg_name: str,
    target: str,
    var: str,
    cols: list,
) -> Union[pd.DataFrame, str, bool, str]:
    """This function splits total emissions into different fuels or mode, append csv ('write'/'append'), header (yes/no), name of csv file

    Args:
        downs_res_file (str): downscaled results file
        csv_out (str): name of csv file
        iamc_index (list): index of iamc format
        df_iam_all_models (pd.DataFrame): dataframe with regional IAMs results
        df_iea_act (pd.DataFrame): datafarme with IEA activity data
        emi_factor (dict): emissions factors by fuel
        eff_factor (dict): efficiency factors by fuel
        energy_dict (dict): energy dictionary
        model (str): model name
        df_downs (pd.DataFrame): dataframe with downscaled results
        mymodel_dict (dict): model dictionary
        countrylist (list): list of countries within a region
        reg_name (str): region
        target (str): scenario
        var (str): variable to be split by fuel/mode
        cols (list): list of columns

    Returns:
        Union[list, pd.DataFrame, str, str, str]: dataframe with emissions split by fuel/mode, and info how to save the csv file (header, mode),
    """
    ###################################
    ## regional level data from IAMs ##
    ###################################
    emi_var = fun_read_reg_value(model, reg_name, target, var[0], df_iam_all_models)[
        "VALUE"
    ]
    if var[1] is not None:
        seq_bio = fun_read_reg_value(
            model, reg_name, target, var[1], df_iam_all_models
        )["VALUE"].fillna(0)
        if len(seq_bio) == 0:
            seq_bio = 0
    else:
        seq_bio = 0

    if var[2] is not None:
        seq_fossil = fun_read_reg_value(
            model, reg_name, target, var[2], df_iam_all_models
        )["VALUE"].fillna(0)
        if len(seq_fossil) == 0:
            seq_fossil = 0
    else:
        seq_fossil = 0

    reg_var = emi_var + seq_bio + seq_fossil

    f_list = energy_dict[var[0]][1]

    df_fuel_all = (
        pd.DataFrame()
    )  # Dataframe that will contain the results for all variables
    if len(f_list) > 1:
        for f in f_list:
            df_fuel = pd.DataFrame()
            df_fuel = df_downs[
                (df_downs.VARIABLE == f"{energy_dict[var[0]][0]}|{f}")  # |w/o CCS'
                & (df_downs.REGION.isin(countrylist))
                & (df_downs.SCENARIO == target)
                & (df_downs.MODEL == model)
            ]

            df_fuel.loc[:, "VARIABLE"] = f"{var[3]}|{f}"
            df_fuel.loc[:, "UNIT"] = "Mt CO2/yr"

            setindex(
                df_fuel,
                iamc_index,
            )

            ########################################
            ## Emissions from Electricity by fuel ##
            ########################################
            # NOTE: apart from BECCS we allocate negative emissions to `Emissions|CO2|Energy|Supply|Electricity|Gas` , due to biogas blending
            if (
                f == "Gas"
                and var[-1] == "Emissions|CO2|Energy|Supply|Electricity"
                and reg_var.min() < 0
            ):  # NOTE: When calculating Electricity emission by fuel, Emissions from gas w/ccs might be negative due to some bleding with biogases.
                secondary_gases_biomass = df_downs[
                    (df_downs.VARIABLE == "Secondary Energy|Gases|Biomass")
                    & (df_downs.REGION.isin(countrylist))
                    & (df_downs.SCENARIO == target)
                    & (df_downs.MODEL == model)
                ]

                secondary_gases_tot = df_downs[
                    (df_downs.VARIABLE == "Secondary Energy|Gases")
                    & (df_downs.REGION.isin(countrylist))
                    & (df_downs.SCENARIO == target)
                    & (df_downs.MODEL == model)
                ]
                setindex(secondary_gases_biomass, iamc_index)
                setindex(secondary_gases_tot, iamc_index)

                try:
                    share_biomass_in_secondary_energy_gases = (
                        secondary_gases_biomass.xs(
                            "Secondary Energy|Gases|Biomass",
                            level="VARIABLE",
                        )
                        / secondary_gases_tot.xs(
                            "Secondary Energy|Gases",
                            level="VARIABLE",
                        )
                    )
                except:
                    cols = share_biomass_in_secondary_energy_gases.columns

                    myindex = [(model, target, x, "EJ/yr") for x in countrylist]
                    share_biomass_in_secondary_energy_gases = pd.DataFrame(
                        index=pd.MultiIndex.from_tuples(
                            myindex,
                            names=[
                                "MODEL",
                                "SCENARIO",
                                "REGION",
                                "UNIT",
                            ],
                        ),
                        columns=cols,
                    ).fillna(0)
                    print(
                        "Warning: no `Secondary Energy|Gases|Biomass`, found in the datafarme. We assume this is equal to zero."
                    )

                share_biomass_in_secondary_energy_gases["VARIABLE"] = f"{var[3]}|{f}"
                setindex(
                    share_biomass_in_secondary_energy_gases,
                    iamc_index,
                )
                share_biomass_in_secondary_energy_gases.rename(
                    index={"EJ/yr": "Mt CO2/yr"},
                    level="UNIT",
                    inplace=True,
                )

                share_ccs_in_electricity_gas = (
                    fun_read_reg_value(
                        model,
                        reg_name,
                        target,
                        "Secondary Energy|Electricity|Gas|w/ CCS",
                        df_iam_all_models,
                    )["VALUE"]
                    / fun_read_reg_value(
                        model,
                        reg_name,
                        target,
                        "Secondary Energy|Electricity|Gas",
                        df_iam_all_models,
                    )["VALUE"]
                )  # NOTE: taken from regional values
                share_ccs_in_electricity_gas.index = [
                    str(x) for x in share_ccs_in_electricity_gas.index
                ]
                df_fuel = (df_fuel * emi_factor[f] / eff_factor[f]) * (
                    1 - share_ccs_in_electricity_gas
                ) + df_fuel * emi_factor["Beccs"] / eff_factor[
                    "Beccs"
                ] * share_biomass_in_secondary_energy_gases * share_ccs_in_electricity_gas

            else:
                df_fuel = df_fuel * emi_factor[f] / eff_factor[f]
            df_fuel_all= pd.concat([df_fuel_all, df_fuel])
            # df_fuel_all = df_fuel_all.append(df_fuel)
    else:
        df_fuel = pd.DataFrame()
        df_fuel = df_downs[
            (df_downs.VARIABLE == f"{energy_dict[var[0]][0]}")  # |w/o CCS'
            & (df_downs.REGION.isin(countrylist))
            & (df_downs.SCENARIO == target)
            & (df_downs.MODEL == model)
        ]

        df_fuel.loc[:, "VARIABLE"] = f"{var[3]}"
        df_fuel.loc[:, "UNIT"] = "Mt CO2/yr"

        setindex(
            df_fuel,
            iamc_index,
        )
        df_fuel = df_fuel
        df_fuel_all = pd.concat([df_fuel_all, df_fuel])
        # df_fuel_all = df_fuel_all.append(df_fuel)

    df_fuel_all.rename(
        columns={x: int(x) for x in df_fuel_all.columns},
        inplace=True,
    )

    ###########################
    ## Special harmonisation ##
    ###########################
    # NOTE: we use a different harmonization rule for Emissions from Electricity by fuel, reason being that only `Emissions|CO2|Energy|Supply|Electricity|Gas` can go below zero.
    # Emissions from coal/oil need to be always above zero
    if var[-1] == "Emissions|CO2|Energy|Supply|Electricity" and reg_var.min() < 0:
        # NOTE: Below we define a list of fuels with emissions that need to be >0.
        elc_list_positive_emi = [
            "Emissions|CO2|Energy|Supply|Electricity|Coal",
            "Emissions|CO2|Energy|Supply|Electricity|Oil",
        ]

        tot_df_fuel_all = df_fuel_all.sum()  # Current total across all fuels

        ## NOTE: We Harmonize variables in ec_list, by imposing that values need to be positive (>0 values). We harmonize as follow: df_fuel_all/tot_df_fuel_all * np.maximum(0, reg_var)
        # We do this only when tot_df_fuel_all (sum across all fuels) is >0. time_positive is defined below:
        time_positive = list(tot_df_fuel_all[tot_df_fuel_all > 0].index)
        df_fuel_all.loc[
            df_fuel_all.index.get_level_values("VARIABLE").isin(elc_list_positive_emi),
            time_positive,
        ] = (
            df_fuel_all[
                df_fuel_all.index.get_level_values("VARIABLE").isin(
                    elc_list_positive_emi
                )
            ]
            * np.maximum(0, reg_var)
            / tot_df_fuel_all[tot_df_fuel_all > 0]
        )  ## Harminzation when sum across fuels >0.

        ## Now if total emissions go negative, we get a mismatch between sum of df_fuel_all and regional data  (because we only harmonize variables in `elc_list_positive_emi`).
        # We calculate the mismatch below:
        mismatch = reg_var - df_fuel_all.sum()

        # NOTE: We allocate the mismatch to Electricity|Gas (which can be negative due to some biomass blending).  Gas= Gas + mismatch * Gas / Gas.sum().
        # After implementing this we get df_fuel.sum() = reg_var.
        mygas=fun_xs(df_fuel_all, {"VARIABLE": "Emissions|CO2|Energy|Supply|Electricity|Gas"})
        
        resgas = mismatch* mygas/ mygas.sum()+ mygas
        #df_fuel_all.loc[resgas.index, 2010]
        df_fuel_all.loc[resgas.index]=resgas.loc[:]
        # df_fuel_all[
        #     df_fuel_all.index.get_level_values("VARIABLE")
        #     == "Emissions|CO2|Energy|Supply|Electricity|Gas"
        # ]= resgas
        ############################
        ## Standard harmonization ##
        ############################
    else:
        df_fuel_all = df_fuel_all * reg_var / df_fuel_all.sum()

        ##############################
        ## Rename IEA ACTIVITY DATA ##
        ##############################
    df_iea_act.loc[:, "UNIT"] = "Mt CO2/yr"
    df_iea_act.loc[:, "MODEL"] = model
    df_iea_act.loc[:, "SCENARIO"] = target

    setindex(
        df_fuel_all,
        iamc_index,
    )
    cols_iea=[x for x in df_iea_act.columns if x not in df_iea_act.index.names]
    df_iea_act=df_iea_act[cols_iea]
    setindex(
        df_iea_act,
        iamc_index,
    )

    ######################################
    ## Emissions from transport by mode ##
    ######################################
    if (
        var[-1] == "Emissions|CO2|Energy|Demand|Transportation"
    ):  ## we downscale to sub-sectors:
        df_append = (
            df_fuel_all.groupby(["MODEL", "SCENARIO", "REGION", "UNIT"]).sum()
            * df_iea_act
        )
        setindex(
            df_append,
            iamc_index,
        )
        # NOTE: list of variables in df_append = ['Air Travel', 'Buses/Trucks', 'Cars', 'Off-road', 'Ships', nan]. We want to exclude nan variable
        mask = [
            True if type(x) == str else False
            for x in df_append.index.get_level_values("VARIABLE")
        ]  ## NOTE Variables to be excluded (True/False list). We esclude variable name == nan. This means type is float instead of string
        df_append = df_append[
            mask
        ]  # We esclude variable == nan. This means type is float instead of string
        df_fuel_all= pd.concat([df_fuel_all, df_append.dropna(how="all")])
        # df_fuel_all = df_fuel_all.append(df_append.dropna(how="all"))
        rename_dict = {
            x: "Emissions|CO2|Energy|Demand|Transportation|" + x
            for x in df_append.index.get_level_values("VARIABLE").unique()
        }
        df_fuel_all.rename(index=rename_dict, level="VARIABLE", inplace=True)

    df_fuel_all.columns = [int(x) for x in df_fuel_all.columns]
    df_fuel_all=df_fuel_all[sorted(list(df_fuel_all.columns))]
    df_fuel_all = df_fuel_all.loc[:, 2005:]  ## Exclude 1990 data

    my_index_names = df_fuel_all.index.names
    df_fuel_all.reset_index(inplace=True)
    df_fuel_all["MODEL"] = [mymodel_dict.get(x, model) for x in df_fuel_all["REGION"]]
    df_fuel_all.set_index(my_index_names, inplace=True)

    return df_fuel_all


def fun_bottom_up_and_overwrite(
    df_harmo, reg_name, countrylist, model, df_iam, new_var_dict, _mysuffix=" v2"
):
    print("Anchor emissions to hist data")

    if new_var_dict:
        for j, i in new_var_dict.items():
            # check duplicated variables: 
            # [len(df_harmo.xs(x, level='SECTOR', drop_level=False)['2010']) for x in df_harmo.reset_index().SECTOR.unique()]
            df_sect_harm = fun_bottom_up_harmonization(
                model,
                reg_name,
                j,
                i,
                df_harmo,
                df_iam,
                add_model=model,
                add_unit="Mt CO2/yr",
                mandatory_harmonization=False,
            )

            df_harmo = df_sect_harm.copy()

        ## Overwrite values of 'Emissions|CO2|Energy excl BECCS v2' with 'Emissions|CO2|Energy excl BECCS' (to harmonize data using bottom up harmonization)=> sectorial consistency across sectors in each country
        my_list_duplicated_var_dict = fun_list_duplicated_var_dict(
            new_var_dict, _mysuffix
        )
        return fun_overvrite_values(df_sect_harm, my_list_duplicated_var_dict)


def fun_anchor_emi_to_hist_data(
    model_in_region_name: bool,
    col_iam_dict: dict,
    iamc_idx_names: list,
    step1_col_dict: dict,
    cols: list,
    df_iam: pd.DataFrame,
    df_emi: pd.DataFrame,
    model: str,
    region: str,
    countrylist: list,
    df: pd.DataFrame,
    emissions_harmo_dict_step5b,
    new_var_dict_step5b,
    iea_var_dict,
    _baseyear: int = 2010,
) -> pd.DataFrame:
    """This function anchors variables contained in emissions_harmo_dict_step5b.keys() to historical data (for a given `_baseyear`).
    It returns the updated datafrae

    Args:
        model_in_region_name (bool): True if the region contains also the model name (e.g. 'MESSAGE|Western Europe'), otherwise False
        col_iam_dict (dict): columns of iam data
        iamc_idx_names (list): iamc names of index
        step1_col_dict (dict): we use this to rename columns as in step1
        cols (list): list of columns
        df_iam (pd.DataFrame): dataframe with regional IAM results
        df_emi (pd.DataFrame): IEA historical dataset
        model (str): model name
        region (str): region name
        countrylist (list): list of countries within region
        df (pd.DataFrame): downscaled results
        emissions_harmo_dict_step5b (_type_): Dictionary with name of variables to be anchored (keys) and how we define this variable (values)
        new_var_dict_step5b (_type_): New variables name (dict.key) to be crated as sum of variables contained in dict.values
        _baseyear (int, optional): Base year for historical data harmonization. Defaults to 2010.

    Returns:
        pd.DataFrame: _description_
    """
    # slice for regional data
    # df[df.VARIABLE=='Emissions|CO2|Energy|Supply|Electricity|Gas']['2010']
    df = df[df.REGION.isin(countrylist)]
    setindex(df, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])

    if model_in_region_name:
        reg_name = f"{region}r"
    else:
        reg_name = region.replace(f"{model}|", "") + "r"

    print("Anchor emissions to hist data")
    harmo_dict = emissions_harmo_dict_step5b

    # NOTE here the order matters for top-down harmonization
    new_var_dict = new_var_dict_step5b
    if len(countrylist) > 0:
        df_harmo = fun_setindex_iso_target_sector(
            df[df.index.get_level_values("REGION").isin(countrylist)]
        )
        # fun_xs(df_harmo, {"SECTOR":'Emissions|CO2|Energy|Supply|Electricity|Gas'})['2010']
        df_sect_harm = fun_bottom_up_and_overwrite(
            df_harmo,
            reg_name,
            countrylist,
            model,
            df_iam,
            new_var_dict,
        )
        # fun_xs(df_sect_harm, {"SECTOR":'Emissions|CO2|Energy|Supply|Electricity|Gas'})['2010']
        if new_var_dict:
            for _ in range(3):
                for j, i in harmo_dict.items():
                    # blueprint for future improvement:
                    # list_to_be_updated_only_once= ["Emissions|CO2|Energy excl BECCS"]
                    # if _ > 0 and j in list_to_be_updated_only_once:
                    #     pass
                    # else:
                    # go ahed with the code below
                    setindex(df_sect_harm, False)
                    df_sect_harm.rename(
                        columns=col_iam_dict,
                        inplace=True,
                    )
                    setindex(
                        df_sect_harm,
                        iamc_idx_names,
                    )
                    df_sect_harm = df_sect_harm[cols].sort_index()

                    # if j=='Emissions|CO2|Energy|Supply|Electricity|Gas':
                    #     aaa=1
                    # Anchor emissions variables to historical data (% )
                    for t in [_baseyear, 2015]:
                        df_sect_harm = fun_anchor_single_var_to_hist_data(
                            df_sect_harm,
                            j,
                            df_emi,
                            iea_var_dict,
                            countrylist,
                            baseyear=str(t),
                        )

                        # NOTE: DO NOT DELETE! The code below almost work to anchor Final energy to historical data
                        # problem (we get na values)

                        # from downscaler.fixtures import iea_flow_dict
                        # df_iea_all =  pd.read_csv(INPUT_DATA_DIR / "Extended_IEA_en_bal_2019_ISO.csv",sep=',',  encoding='latin-1')
                        # fun_anchor_single_var_to_hist_data(
                        #     df.rename_axis(index={'ISO':'REGION'}).drop('CRITERIA',axis=1),
                        #     "Final Energy",
                        #     df_iea_all.set_index(['ISO', 'FLOW', 'PRODUCT']).loc[['AUS', 'JPN']],
                        #     fun_create_iea_dict_from_iea_flow_dict(["Final Energy"],iea_flow_dict),
                        #     ['AUS', 'JPN'],
                        #     baseyear=str(2010),
                        # ).xs('Final Energy', level = 'VARIABLE')

                    if list(df_sect_harm.index.names) == [None]:
                        df_sect_harm.set_index(
                            ["ISO", "TARGET", "SECTOR"], inplace=True
                        )

                    df_sect_harm = fun_setindex_iso_target_sector(df_sect_harm)

                    # NOTE: The block below does (overwrite values) does not have a huge impact on the results

                    ## We assume that the sum of sub sectorial emissions account for 80% of total emission if j == "Emissions|CO2|Energy excl BECCS v2"
                    myscalar = 0.8 if j == "Emissions|CO2|Energy excl BECCS v2" else 1

                    my_list_duplicated_var_dict = fun_list_duplicated_var_dict(
                        new_var_dict, " v2"
                    )
                    d = SliceableDict(my_list_duplicated_var_dict)

                    if j.replace(" v2", "") in d.keys():
                        df_sect_harm = fun_overvrite_values(
                            df_sect_harm,
                            d.slice(
                                j.replace(" v2", "")
                            ),  # we slice the dictionary as we want to update/overwrite only sector j (because myscalar differs across sectors)
                            scalar=myscalar,
                        )
                    # Remove duplicates
                    myidxn=df_sect_harm.index.names
                    df_sect_harm=df_sect_harm.reset_index().set_index(['MODEL', 'TARGET', 'ISO', 'SECTOR', 'UNIT'])
                    df_sect_harm=fun_drop_duplicates(df_sect_harm)
                    df_sect_harm=df_sect_harm.reset_index().set_index(myidxn)

                    df_sect_harm = fun_top_down_harmonization(
                        model,
                        reg_name,
                        j,
                        i,
                        df_sect_harm,
                        df_iam,
                        mandatory_harmonization=False,  # we do not always harmonize because we do not have emissions by fuel in the df_iam
                        unit="Mt CO2/yr",
                        postive_var=["Emissions|CO2|Energy|Supply|Heat"],
                        min_threshold=1e-6,
                        perc_threshold=0.8,  # None
                    )
                    df_sect_harm["MODEL"] = df_sect_harm["MODEL"].fillna(model)
                    df_sect_harm["UNIT"] = df_sect_harm["UNIT"].fillna("Mt CO2/yr")

                    df_sect_harm = df_sect_harm.reset_index().rename(
                        columns=col_iam_dict,
                    )

                    df_sect_harm.rename(
                        columns=step1_col_dict,
                        inplace=True,
                    )

                    # Copy updated values to df_harmo
            df_sect_harm=fun_drop_duplicates(fun_index_names(df_sect_harm).dropna())
            df_sect_harm=df_sect_harm.reset_index()
            idx1 = df_sect_harm.set_index(["ISO", "TARGET", "SECTOR"]).index
            idx2 = df_harmo.index
            idx_copy = idx1.intersection(idx2)
            df_harmo.loc[idx_copy] = df_sect_harm.set_index(
                    ["ISO", "TARGET", "SECTOR"]
                ).loc[idx_copy]


        df_harmo = df_harmo.reset_index()
        df_harmo["MODEL"] = df_harmo["MODEL"].fillna(model)
        df_harmo["UNIT"] = df_harmo["UNIT"].fillna("Mt CO2/yr")
        df_harmo = df_harmo.rename(columns=col_iam_dict).set_index(iamc_idx_names)

        return df_harmo
    else:
        return pd.DataFrame()


def fun_bottom_up_harmonization(
    model: str,
    region: str,
    main_sector: str,
    sub_sectors: Union[str,list],
    downscaled_data: pd.DataFrame,
    iam_data: pd.DataFrame,
    add_model: str = None,
    add_unit: str = None,
    mandatory_harmonization: bool = True,
) -> pd.DataFrame:
    """
    Create main_sector data as the sum of sub_sectors (for each country).
    Then it harmonizes the main_sector, so that the sum of country level data matches regional IAMs data.
    It returns the updated dataframe `downscaled_data`.

    NOTE: This function should be used only when creating new (`main_sector`) variables .
    If the variable (`main_sector`) is already present in the dataframe, you might consider using a top-down hamronization
    Reason: we trust more the data coming from the 'top':
    Example: Final Energy information should lead changes in the results for sub-sectors Final Energy|Industry, Final Energy|Transportation etc. ,
             and not vice-versa (unless Final Energy variable is not present in the dataframe. In that case we can use a bottom-up harmonization).


    """
    check_sector = downscaled_data[
        downscaled_data.index.get_level_values("SECTOR") == main_sector
    ]
    if len(check_sector) != 0:
        print(
            f"NOTE: The variable {main_sector} is already present in the downscaled country level results. Threfore we do not create a new variable using the bottom-up harmonization process. "
        )
        return downscaled_data
    else:
        sub_sectors_data = downscaled_data[
            (downscaled_data.index.get_level_values("SECTOR").isin(sub_sectors))
        ]
        sub_sectors_data_sum = sub_sectors_data.groupby(["ISO", "TARGET"]).sum()
        unit = add_unit if add_unit is not None else "EJ/yr"

        sub_sectors_data_sum["SECTOR"] = main_sector
        sub_sectors_data_sum.set_index("SECTOR", append=True, inplace=True)

        if main_sector in iam_data.index.get_level_values("VARIABLE").unique():
            ## We harmonize results to match regional IAM data only if the `main_sector` variable is avaiable from IAMs results
            iam_main_sector = iam_data.xs(
                (model, region, main_sector, unit),
                level=("MODEL", "REGION", "VARIABLE", "UNIT"),
                drop_level=True,
            )

            ## Harmonization of the main_sector to match regional IAMs results
            ratio = iam_main_sector / (
                sub_sectors_data_sum.groupby("TARGET").sum()
            )  ## ratio = Regional data divided by sum of downscaled (country level) data for each target
            # Select for correct SCENARIO in `ratio`
            ratio=ratio[ratio.index.isin(sub_sectors_data_sum.reset_index().TARGET.unique())]
            myd={"UNIT":add_unit, "MODEL":add_model}
            for k,v in myd.items():
                if v is not None:
                    sub_sectors_data_sum[k]=v
                    ratio[k]=v
            idxname=sub_sectors_data.index.names
            
            # Here we calculate the main sector as the sum of sub-sectors
            # Get correct order of index.names, to avoid errors when multiplying dataframes
            a=fun_index_names(sub_sectors_data_sum) # using sub_sectors_data_sum instead of sub_sectors_data 2024_01_10 h13.10
            b=fun_index_names(ratio).fillna(1)
            common=[x for x in a.index.names if x in b.index.names]
            tmp_idxn=common+[x for x in a.index.names if x not in b.index.names] #* ratio.fillna(1)
            # sub_sectors_data_sum =  fun_index_names(sub_sectors_data)*fun_index_names(ratio).fillna(1)
            sub_sectors_data_sum = a.reset_index().set_index(tmp_idxn)*b.reset_index().set_index(common)
            sub_sectors_data_sum=sub_sectors_data_sum.reset_index().set_index(idxname)
        if (
            main_sector not in iam_data.index.get_level_values("VARIABLE").unique()
            and mandatory_harmonization
        ):
            raise ValueError(
                f"The variable `{main_sector}` is not present in regional IAMs results.\n "
                f"If you still want to calculate this variable as the sum of sub_sectors, please set `mandatory_harmonization=False`"
            )

        if add_model is not None:
            sub_sectors_data_sum["MODEL"] = add_model
        if add_unit is not None:
            sub_sectors_data_sum["UNIT"] = add_unit
        cols = downscaled_data.columns
        
        if len(sub_sectors_data_sum)>0:
            # Exclude main sector from `downscaled_data` and replace it with the new data `sub_sectors_data_sum`
            return pd.concat([fun_xs(downscaled_data, {"SECTOR":main_sector}, exclude_vars=True), 
                            sub_sectors_data_sum[cols]])
        else:
            return downscaled_data
        # return pd.concat([downscaled_data, sub_sectors_data_sum[cols]])
        # return downscaled_data.append(sub_sectors_data_sum[cols])


def fun_top_down_harmonization(
    model: str,
    region: str,
    main_sector: str,
    sub_sectors: List,
    downscaled_data: pd.DataFrame,
    iam_data: pd.DataFrame,
    mandatory_harmonization: bool = True,
    unit: str = "EJ/yr",
    postive_var: list = [""],  # NOTE: this might break regional IAMs consistency
    min_threshold=1e-6,  #
    perc_threshold: Union[None, float] = None,
) -> pd.DataFrame:
    """Adjust sub-sectors to be consistent with the main_sector results, at the country level. (Country level consistency)
    Then it harmonizes all of the sub-sectors, so that the sum of country level data matches regional IAMs data. (Regional level consistency)

    All sub-sectors variables present in the `positive_var` list will be set as non-negative (clipped to a `min_threshold` as minimum value).

    Please note that if a sub-sector variable is not present in regional IAM results, no regional harmonization is possible. Therefore setting such variable as positive  could break conistency with regional IAM results.
    It returns the updated dataframe `downscaled_data` (with the updated data for the sub-sectors).

    NOTE: Given a subsector (sub):
    INITIAL DATAFRAME                              => downscaled_data[(downscaled_data.index.get_level_values('SECTOR')==sub)]
    1) DATAFRAME AFTER COUNTRY LEVEL CONSISTENCY   => sub_sectors_data.xs(sub, level='SECTOR')
    2) DATAFRAME AFTER REGIONAL LEVEL CONSISTENCY  => sub_data.loc[idx,:]
    Parameters
    ----------
    model : str
        Model
    region : str
        Region
    main_sector : str
        Main sector
    sub_sectors : List
        List of sub sectors
    downscaled_data : pd.DataFrame
        Dataframe with downscaled data
    iam_data : pd.DataFrame
        Regional IAM data
    mandatory_harmonization : bool, optional
        Wheter we want to always harmonize the data to match IAMs results, by default True
    unit : str, optional
        Unit of the variable, by default "EJ/yr"
    postive_var : list, optional
        List of variables that should be set as positive (non-negative) , by default [""]

    Returns
    -------
    pd.DataFrame
        dataframe with upadated results

    Raises
    ------
    ValueError
        Is a sub-sector is not present in IAMs results
    """

    allowed_cols = list(range(2000, 3000))
    allowed_cols += [str(x) for x in allowed_cols]
    # 1) Country level consistency harmonizaton
    main_sector_data = downscaled_data[
        (downscaled_data.index.get_level_values("SECTOR") == main_sector)
    ]
    sub_sectors_data = downscaled_data[
        (downscaled_data.index.get_level_values("SECTOR").isin(sub_sectors))
    ]

    allowed_cols = [x for x in allowed_cols if x in main_sector_data.columns]

    ratio = (
        fun_xs(main_sector_data.loc[:, allowed_cols], {"SECTOR":main_sector}).droplevel('SECTOR')
        / sub_sectors_data.loc[:, allowed_cols].groupby(["ISO", "TARGET"]).sum()
    )

    ratio = ratio.fillna(1)

    if perc_threshold is not None:
        # cols = list(ratio.columns)

        # [cols.remove(x) for x in ["MODEL", "UNIT"] if x in cols]
        ratio.loc[:, allowed_cols] = np.maximum(
            ratio.loc[:, allowed_cols], 1 - perc_threshold
        )
        ratio.loc[:, allowed_cols] = np.minimum(
            ratio.loc[:, allowed_cols], 1 + perc_threshold
        )
        ## Update sub_sectors results based on main_sector results
        if len(sub_sectors_data)>0:
            sub_sectors_data.loc[:, allowed_cols] = (
                sub_sectors_data[allowed_cols] * ratio[allowed_cols]
            )
    else:
        sub_sectors_data = sub_sectors_data * ratio
    idx = sub_sectors_data.index

    available_sub_sectors = list(
        sub_sectors_data.index.get_level_values("SECTOR").unique()
    )  ## sub sectors available from data

    # 2) Regional level consistency harmonization (for each sub sector)
    for sub in available_sub_sectors:
        if sub in postive_var:
            cols = range(2005, 2105)
            cols = [str(x) for x in cols if str(x) in sub_sectors_data.columns]
            idx = sub_sectors_data.xs(sub, level="SECTOR", drop_level=False).index
            sub_sectors_data.loc[idx, cols] = (
                sub_sectors_data.loc[idx, cols]
                .dropna(how="all", axis=1)
                .clip(min_threshold)
            )

        if (
            sub in postive_var
            and sub not in iam_data.index.get_level_values("VARIABLE").unique()
        ):
            print("#####################")
            print("WARNING:")
            print(
                f"{sub} is defined as a non-negative variables, but not present in regional IAM results. "
            )
            print(
                f"This could break consistency with regional IAMs results, especialy if `min_threshold` (currently set at: {min_threshold}) is large"
            )

        if (
            sub not in iam_data.index.get_level_values("VARIABLE").unique()
            and mandatory_harmonization
        ):
            raise ValueError(
                f"The variable `{sub}` is not present in regional IAMs results.\n "
                f"If you still want to calculate this variable as the sum of sub_sectors, please set `mandatory_harmonization=False`"
            )

        if sub in iam_data.index.get_level_values("VARIABLE").unique():
            ## new formulation
            iam_sub_sector = iam_data.xs(
                (model, region, sub, unit),
                level=("MODEL", "REGION", "VARIABLE", "UNIT"),
                drop_level=True,
            )
            ratio_sub = iam_sub_sector / (
                sub_sectors_data.xs(sub, level="SECTOR").groupby("TARGET").sum()
            )
            sub_data = sub_sectors_data.xs(
                sub, level="SECTOR", drop_level=True
            ) * ratio_sub.dropna(how="all")
            idx = sub_data.index
            sub_data["SECTOR"] = sub
            sub_data.set_index("SECTOR", append=True, inplace=True)
            setindex(sub_data, downscaled_data.index.names)
            idx = sub_data.index
            downscaled_data.loc[idx, :] = sub_data.loc[idx, :]

        else:  # no regional harmonization
            sub_data = sub_sectors_data.xs(sub, level="SECTOR", drop_level=False)
            idx = sub_data.index
            downscaled_data.loc[idx, :] = sub_data.loc[idx, :]

    return downscaled_data


def fun_down_revenues(
    downs_res_file: str,
    csv_out: str,
    model_in_region_name: bool,
    iamc_index: list,
    df_iam: pd.DataFrame,
    model: str,
    myiso_dict: dict,
    mymodel_dict: dict,
    region: str,
    countrylist: list,
    df: pd.DataFrame,
    revenue_var_list: dict,
) -> Union[pd.DataFrame, list]:
    df = df[df.REGION.isin(countrylist)]
    setindex(df, iamc_index)

    if model_in_region_name:
        reg_name = region + "r"
    else:
        reg_name = region.replace(model + "|", "") + "r"

    ## NOTE: {"variable to be downscaled":
    #               [variables to be used as proxi for downscaling (sum of these variables)]
    #         }
    # Example: we downscale "Revenue|Government|Tax|Carbon" based on the sum of  ["Emissions|CO2|Energy|Coal","Emissions|CO2|Energy|Gas","Emissions|CO2|Energy|Oil"], at the country level.
    # "Revenue|Government|Tax|Carbon" = regional value * emissions/emissions.sum(country)
    my_index_names = None
    df_rev_all = pd.DataFrame()
    if len(df) > 0:
        for revenue_var, proxi_vars in revenue_var_list.items():
            if (
                revenue_var
                in df_iam.loc[model].index.get_level_values("VARIABLE").unique()
            ):
                (
                    df_rev,
                    my_index_names,
                    file_name,
                ) = fun_downs_single_var_revenue(
                    downs_res_file,
                    csv_out,
                    iamc_index,
                    df_iam,
                    model,
                    mymodel_dict,
                    reg_name,
                    df,
                    revenue_var,
                    proxi_vars,
                )
                # df_rev_all = df_rev_all.append(df_rev)
                df_rev_all = pd.concat([df_rev_all,df_rev])

    return df_rev_all.rename(index=myiso_dict).dropna(how="all"), my_index_names


def fun_downs_single_var_revenue(
    downs_res_file: str,
    csv_out: str,
    iamc_index: list,
    df_iam: pd.DataFrame,
    model: str,
    mymodel_dict: dict,
    reg_name: str,
    df: pd.DataFrame,
    revenue_var: str,
    proxi_vars: list,
) -> Union[pd.DataFrame, list, str]:
    reg_var = df_iam[df_iam.index.get_level_values("VARIABLE") == revenue_var].xs(
        (model, reg_name),
        level=("MODEL", "REGION"),
        drop_level=False,
    )
    reg_var.index.rename(
        names=iamc_index,
        inplace=True,
    )

    ## country % of emissions within region
    ratio = (
        np.abs(df[df.index.get_level_values("VARIABLE").isin(proxi_vars)])
        / np.abs(df[df.index.get_level_values("VARIABLE").isin(proxi_vars)])
        .groupby(["MODEL", "SCENARIO", "UNIT"])
        .sum()
    )

    df_rev = ratio.groupby(["MODEL", "REGION", "SCENARIO"]).sum() * reg_var.xs(
        reg_name, level="REGION"
    )

    try:
        assert_frame_equal(
            reg_var.groupby(["MODEL", "SCENARIO"]).sum(),
            df_rev.groupby(["MODEL", "SCENARIO"]).sum(),
        )
    except:
        ## Fill missing values with previous year (if values are missing)
        mask = df_rev.eq(0).copy()
        df_rev2 = df_rev.mask(mask).ffill(axis=1).copy(deep=True)
        df_rev2 = df_rev2.reset_index().set_index(
            ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"]
        )
        df_rev3 = df_rev2.loc[df_rev2.index.dropna()]
        ratio_corr = reg_var / df_rev3.groupby(["MODEL", "SCENARIO"]).sum().copy()
        ratio_corr = ratio_corr.xs(reg_name, level="REGION").copy()
        # NOTE BELOW might cause RecursionError: maximum recursion depth exceeded while calling a Python object
        # because index names are different
        # this one works: ratio_corr.reset_index().set_index(df_rev3.index.names[:3]).drop('UNIT', axis=1)*df_rev3
        # df_rev = (ratio_corr * df_rev3).dropna(how="all")
        df_rev = (
            ratio_corr.reset_index()
            .set_index(df_rev3.index.names[:3])
            .drop("UNIT", axis=1)
            * df_rev3
        )

    setindex(
        df_rev,
        iamc_index,
    )

    my_index_names = df_rev.index.names
    df_rev.reset_index(inplace=True)

    df_rev["MODEL"] = [mymodel_dict.get(x, model) for x in df_rev["REGION"]]

    df_rev.set_index(my_index_names, inplace=True)

    file_name = downs_res_file.replace("MODEL", model).replace(".csv", csv_out) + ".csv"
    return df_rev, my_index_names, file_name


def fun_check_sector_consistency(
    df_final: pd.DataFrame, main_sector: str, sub_sectors: list
) -> float:
    """Calculates ratio between sum of `sub_sector` and `main_sector` in a given dataframe `df_final`.

    Parameters
    ----------
    df_final : pd.DataFrame
        Dataframe
    main_sector : str
        main sector variable e.g. `Final Energy`
    sub_sectors : list
        List of sub-sectors e.g. ['Final Energy|liquids','Final Energy|gases' ... ]

    Returns
    -------
    _type_
        Ratio between sum of `sub_sector` and `main_sector`

    Raises
    ------
    ValueError: if sectors are not present in a give dataframe

    """
    sub_sectors = list(sub_sectors)

    df_final, primary_ren = fun_add_non_biomass_ren_nomenclature(df_final)
    primary_dict = {
        f"Primary Energy|{x}": f"Primary Energy|Non-Biomass Renewables|{x}"
        for x in primary_ren
    }
    sub_sectors = [primary_dict.get(x, x) for x in sub_sectors]

    # Use lower case
    sub_sectors = [x.lower() for x in sub_sectors if x is not None]
    main_sector = main_sector.lower()
    df_final = df_final.rename(
        index={x: x.lower() for x in df_final.reset_index().VARIABLE}
    )

    # Check for sectors
    missing = [
        x
        for x in sub_sectors + [main_sector]
        if x not in df_final.index.get_level_values("VARIABLE").unique()
    ]
    if len(missing) > 0:
        raise ValueError(
            f"These sub-sectors are missing (please check typos): {missing} for these regions: {df_final.reset_index().REGION.unique()}"
        )
    if main_sector not in df_final.index.get_level_values("VARIABLE"):
        fun_create_var_as_sum(df_final, main_sector, sub_sectors)

    num = np.maximum(
        df_final.xs(main_sector, level="VARIABLE")
        .groupby(["MODEL", "SCENARIO", "REGION", "UNIT"])
        .sum(),
        1e-5,
    )
    den = np.maximum(
        df_final[df_final.index.get_level_values("VARIABLE").isin(sub_sectors)]
        .groupby(["MODEL", "SCENARIO", "REGION", "UNIT"])
        .sum(),
        1e-5,
    )
    ratio = den / num
    c_max = ratio.max()
    ratio[ratio.eq(c_max).any(1)]
    c_list = list(
        ratio[ratio.eq(c_max).any(1)].index.get_level_values("REGION").unique()
    )
    return np.round((ratio).max().max(), 5), c_list


def fun_sectoral_consistency_check(
    df_downs: pd.DataFrame,
    df_iam: Union[pd.DataFrame, None],
    main_sector: str,
    sub_sectors: list,
    max_threshold: float = 1,  # can be >1 if you want to add a bit of tolerance
    coerce_errors=False,
) -> tuple:
    """Check is sum of `sub_sectors` exceeds the `main_sector` both in the downscaled  (`df_downs`) and IAM (`df_iam`) data.
    Normally this function checks the data at the regional level (df_downs and df_iam nomally refer to one region and one model).
    If `df_iam` is None, it just checks all country-level data available in the downscaled results `df_downs` (for all countries/model/scenarios available in the `df_downs`),
    checking the `df_iam` results.

    It Raises a value error if:
    1) the sum of `sub_sectors` exceeds the `main_sector` in the downscsaled results and
    2) this is not the case in the regional IAM (`df_iam`) results (when `df_iam` is not None) and
    3) coerce_errors is False

    It returns a tuple of including: (ratio of `sub_sectors/main_sector` and  a list of countries where the sum of `sub_sectors` exceeds the `main_sector`)

    NOTE: We can add a bit of tolerance by changing `max_threshold`.
    Example. max_threshold=1.01 means that the sum of sub-sector can be 1% higher than the `main_sector`

    Parameters
    ----------
    df_downs : pd.DataFrame
        downscaled results
    df_iam : pd.DataFrame
        IAMs results
    main_sector : str
        main sector variable e.g. `Final Energy`
    sub_sectors : list
        List of sub-sectors e.g. ['Final Energy|liquids','Final Energy|gases' ... ]
    max_threshold : float, optional
        Maximum threshold ratio between the sum of `sub_sectors` and `main_sector`, by default 1

    Returns
    -------
    Tuple
        Tuple including: (ratio of `sub_sectors/main_sector` and  a list of countries where the sum of `sub_sectors` exceeds the `main_sector`)


    Raises
    ------
    ValueError
        _description_
    """
    if df_iam is not None:
        try:
            iam_data = fun_check_sector_consistency(
                df_iam.rename_axis(index={"TARGET": "SCENARIO"}),
                main_sector,
                sub_sectors,
            )
        except:
            iam_data = [0, 0]
    else:
        iam_data = [0, 0]
    downscaled_data = fun_check_sector_consistency(df_downs, main_sector, sub_sectors)
    print("=============================")
    print(f"Sectorial consistency - {main_sector}")
    print(f"Sub sectors: {[x.split('|')[-1] for x in sub_sectors]}")
    print(f"IAM data: {iam_data[0]}")
    print(f"Downscaled_data: {downscaled_data}")

    if downscaled_data[0] > max_threshold:
        if iam_data[0] < max_threshold:
            msg = f" Sectorial consistency for {main_sector} is not good: sum of sub sectors is above total in the downscaled results, for {downscaled_data[1]} "
            if coerce_errors:
                print(msg)
            else:
                raise ValueError(msg)
        if df_iam is not None and iam_data[0] > max_threshold:
            print(
                f"{main_sector}: all good, because sum of sub-sectors is above total both in the downscaled and in the IAM data."
            )
        elif df_iam is None:
            print(
                f"{main_sector},  sum of sub-sectors is above total in the downscaled results (we do not check IAM data because df_iam is None)."
            )
        else:
            print(
                f"{main_sector},  sum of sub-sectors is above total in the downscaled results."
            )

    return downscaled_data


def hist_share_plus_diff_in_downs_data(
    _var: str,
    df_emi: pd.DataFrame,
    iea_var_dict: dict,
    countrylist: list,
    baseyear: Union[str, int],
    df_downs: pd.DataFrame,
) -> pd.DataFrame:
    """This function multiplies historical shares (at the county level) with growth rates from previosly downscaled results, for a given variable and a given countrylist (region).
    It applies to all all targets.
    It returns the updated dataframe (sliced for variable _var)
    """

    ## combine historical data and growth rates
    hist = fun_hist_iea_share(
        _var,
        df_emi,
        iea_var_dict,
        countrylist,
        _baseyear="2010",
        df_down=df_downs,
        keep_data_if_missing_iea_data=True,
    )

    cols = df_downs.columns

    if "REGION" in list(df_downs.reset_index().columns):
        hist.index.names = ["REGION"]

    ## Same historical data for all time periods
    hist = pd.DataFrame(hist, columns=cols).apply(lambda x: hist, axis=0)

    # We calculate the difference compared to 2010 data (df.diff and then cumsum)
    diff = (
        df_downs.xs(_var, level="VARIABLE", drop_level=False)
        .diff(axis=1)
        .fillna(0)
        .cumsum(axis=1)
    )

    # We add difference in df_downs to hist data
    updated_data = hist + diff

    # Harmonised data to match regional iam results
    tot = updated_data.groupby(["MODEL", "SCENARIO"]).sum()
    if tot.max().max() !=0 :
        updated_data = updated_data / tot
    return updated_data


def fun_check_if_variables_in_dict_values_are_present_in_df_iam(
    var_dict: dict,
    df_iam_all_models: pd.DataFrame,
    harmonize: bool,
    coerce_error: bool = False,
) -> list:
    """Check if variables in `var_dict.values` are present in `df_iam_all_models`.
    If some variables are missing it throws an error, otherwise it returns the list of variables that have been checked.
    """
    dict_list = [list(x.keys())[0] for x in var_dict.values() if type(x) == dict]

    # type of values is a list:
    values_list = [
        x
        for x in [inner for outer in var_dict.values() for inner in outer]
        if type(x) == str
    ]
    checklist = unique(dict_list + values_list)

    blacklist = []
    for x in checklist:
        if x not in df_iam_all_models.index.get_level_values("VARIABLE"):
            if x in var_dict.keys() and x in values_list:
                # this means x is not (yet) present in df_iam_all_models, but it will be (as this is also present in our var_dict.keys)
                pass
            else:
                txt = (
                    f"{x} is  not available in df_iam_all_models but present in `step5b_sect_consistency_reg_iam_data.values()`"
                    f"Please revise `step5b_sect_consistency_reg_iam_data.values()` (use only data present in df_iam_all_models) such as"
                    f"{df_iam_all_models.xs('Mt CO2/yr', level='UNIT').index.get_level_values('VARIABLE').unique()}"
                )
                if harmonize:
                    if not coerce_error:
                        raise ValueError(txt)
                    else:
                        blacklist = blacklist + [x]
                else:
                    print(txt)

    return [x for x in checklist if x not in blacklist]

    # df_iam_all_models moved to iamc format to create new variable


def fun_convert_df_iam_all_models_to_iamc_format(
    df_iam_all_models: pd.DataFrame, sel_var_list: Union[List, None] = None
) -> pd.DataFrame:
    myindex = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]

    if list(df_iam_all_models.index.names) != [None]:
        df_iam_all_models.reset_index(inplace=True)

    if sel_var_list is not None:
        df_iam_all_models = df_iam_all_models[
            df_iam_all_models.VARIABLE.isin(sel_var_list)
        ]

    df_iam_all_models.set_index(myindex, inplace=True)

    return df_iam_all_models.set_index("TIME", append=True).unstack("TIME")["VALUE"]


def fun_create_var_as_sum_df_iam_all_models(
    df_iam_all_models: pd.DataFrame, new_var_dict: dict
) -> pd.DataFrame:
    """Create variable in df_iam_all_models with a long format.
    Creates variables in `new_var_dict.keys` as sum of `new_var_dict.values`"""
    df_iam_all_models = fun_convert_df_iam_all_models_to_iamc_format(
        df_iam_all_models, None
    )

    for i, j in new_var_dict.items():
        if i not in df_iam_all_models.index.get_level_values("VARIABLE"):
            df_iam_all_models = fun_create_var_as_sum(df_iam_all_models, i, j)

    # Convert df_iam_all_models back to melted format
    df_iam_all_models = df_iam_all_models.reset_index().melt(
        ["MODEL", "SCENARIO", "UNIT", "VARIABLE", "REGION"]
    )
    df_iam_all_models.columns = [x.upper() for x in df_iam_all_models.columns]

    return df_iam_all_models


def fun_read_df_iam_all_and_slice(
    list_of_models, list_of_targets, input_file, _add_model_name_to_region=False
):
    df_iam_all = fun_read_df_iam_all(
        file=InputFile(input_file), add_model_name_to_region=_add_model_name_to_region
    )

    # Solve memory problems by filtering out selected model, scenario
    models = [
        x
        for x in df_iam_all.MODEL.unique()
        if match_any_with_wildcard(x, list_of_models)
    ]
    targets = [
        x
        for x in df_iam_all.SCENARIO.unique()
        if match_any_with_wildcard(x, list_of_targets)
    ]
    if len(targets)==0:
        raise ValueError(f'Unable to find list of targets {list_of_targets}')
    df_iam_all = df_iam_all[
        (df_iam_all.SCENARIO.isin(targets)) & (df_iam_all.MODEL.isin(models))
    ]

    return df_iam_all


def fun_calculate_share(
    df_iam_all_models: pd.DataFrame,
    num_var: str,
    den_var: str,
    model: str,
    target: str,
    region: str,
):
    """Calculates share in df_iam_all_models. Returns a share

    Parameters
    ----------
    df_iam_all_models : pd.DataFrame
        Dataframe with IAM results
    num_var : str
        numerator
    den_var : str
        denominator
    model : str
        model
    target : str
        target
    region : str
        region

    Returns
    -------
    _type_
        Share of num_var/den_var for a given model, scenario, region
    """
    idxcol = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "TIME"]
    df_iam_all_models = df_iam_all_models.reset_index().set_index(idxcol)
    num = df_iam_all_models.xs(num_var, level="VARIABLE")
    den = df_iam_all_models.xs(den_var, level="VARIABLE")
    res = num["VALUE"] / den["VALUE"]
    return res.loc[model].loc[target].loc[region]


def fun_calculate_electricity_ccs(
    df: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    model: str,
    region: str,
    target: str,
) -> pd.DataFrame:
    """Calculate electricity generation with and without carbon capture and storage (CCS) based on regional CCS share data.

    This function considers only technologies that emit CO2 and adjusts their 
    values according to the calculated share of CCS. The resulting DataFrame 
    includes the adjusted electricity values for coal, gas, oil, and biomass.

    Parameters
    ----------
    df : pd.DataFrame
        A DataFrame containing the electricity generation data for various fuels. 
        The columns represent different types of electricity generation.
        
    df_iam_all_models : pd.DataFrame
        A DataFrame containing the IAM (Integrated Assessment Model) data that includes 
        the variable names for different technologies and their CCS shares.
        
    model : str
        The name of the model being used to calculate CCS shares.
        
    region : str
        The geographical region for which the CCS share is being calculated.
        
    target : str
        The specific target scenario or timeframe for the calculations.

    Returns
    -------
    pd.DataFrame
        A DataFrame with the updated electricity generation values for the fuels 
        that have been adjusted according to their CCS shares. The resulting DataFrame 
        will include only the columns for COAL, GAS, OIL, and BIO.

    Notes
    -----
    The function looks for specific variable names in the `df_iam_all_models` DataFrame
    that correspond to each fuel type and calculates the CCS share based on that. 
    If a variable is not found, it assumes the CCS share to be 1 (no CCS impact).
    """
    # NOTE: only technologies that emit CO2 are considered
    mydict = {
        "COAL": [
            "Secondary Energy|Electricity|Coal|w/o CCS",
            "Secondary Energy|Electricity|Coal",
        ],
        "GAS": [
            "Secondary Energy|Electricity|Gas|w/o CCS",
            "Secondary Energy|Electricity|Gas",
        ],
        "OIL": [
            "Secondary Energy|Electricity|Oil|w/o CCS",
            "Secondary Energy|Electricity|Oil",
        ],
        # NOTE: only for gas we consider w/ccs (negative emissions)
        "BIO": [
            "Secondary Energy|Electricity|Biomass|w/ CCS",
            "Secondary Energy|Electricity|Biomass",
        ],
    }
    
    for i, j in mydict.items():
        # Check if the CCS variable exists in the IAM models DataFrame
        if j[0] in df_iam_all_models.VARIABLE.unique():
            ccs_share = fun_calculate_share(
                df_iam_all_models, j[0], j[1], model, target, f"{model}|{region}"
            )
        else:
            ccs_share = 1  # Default to no CCS impact if variable not found
            
        # Adjust the electricity generation data according to the CCS share
        df[i] = ccs_share * df[i]
    
    return df[list(mydict.keys())]


def fun_mulitply_df_by_dict(df: pd.DataFrame, mydict: dict):
    """Multiplies dataframe columns by a dictionay

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe to be multiplied by mydict
    mydict : dict
        Dictionary with multipliers value for each column of the dataframe

    Returns
    -------
    _type_
        Dataframe multiplied by mydict values
    """
    return pd.DataFrame({x: df[x].apply(lambda x: x * i) for x, i in mydict.items()})


def fun_calculate_electricity_emissions(
    df: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    model: str,
    region: str,
    target: str
) -> pd.Series:
    """
    Calculates the electricity emissions based on the carbon capture and storage 
    (CCS) output and specific emission factors for different fuel types.

    This function first calls another function to calculate the electricity output 
    considering CCS technology, and then multiplies the result by predefined emission 
    factors (adjusted for efficiency) for various fuel types. The final result is 
    returned as the total emissions.

    Args:
        df (pd.DataFrame): DataFrame containing the electricity generation data.
        df_iam_all_models (pd.DataFrame): DataFrame containing IAM model outputs.
        model (str): The specific IAM model being used for calculations.
        region (str): The geographical region for which emissions are calculated.
        target (str): The target year or condition for emissions assessment.

    Returns:
        pd.Series: A Series representing the total electricity emissions over time.
    """
    res = fun_calculate_electricity_ccs(
        df.copy(deep=True), df_iam_all_models, model, region, target
    )
    
    # Emission Factors (same as `dict_prova` in step3), divided by efficiency
    dict_emi: Dict[str, float] = {
        "OIL": 18.4 / 0.25,
        "GAS": 15.3 / 0.5,
        "COAL": 26.1 / 0.3,
        "BIO": -20 / 0.3,
    }
    
    return fun_mulitply_df_by_dict(res, dict_emi).sum(axis=1)


def fun_weighted_criteria_and_finalise(
    var: str,
    df_platts: pd.DataFrame,
    df_weights: pd.DataFrame,
    countrylist: List[str],
    df_demand: pd.DataFrame,
    fuel_list: List[str],
    df_gov: pd.DataFrame,
    df_criteria_not_weighted: pd.DataFrame,
) -> pd.DataFrame:
    """
    Combines weighted criteria and finalizes the DataFrame of energy demand 
    by adding total demand and applying governance and nuclear criteria.

    This function first combines weighted criteria with non-weighted criteria, 
    adds the total demand, checks for nuclear energy eligibility, and ensures 
    the resulting DataFrame sums up to one for the specified fuels.

    Args:
        var (str): The variable name representing the specific demand or energy metric.
        df_platts (pd.DataFrame): DataFrame containing pricing or cost data relevant to fuels.
        df_weights (pd.DataFrame): DataFrame containing weights for the criteria.
        countrylist (List[str]): List of countries for which the criteria are being evaluated.
        df_demand (pd.DataFrame): DataFrame containing demand data for different fuels.
        fuel_list (List[str]): List of fuel types to be considered in the calculations.
        df_gov (pd.DataFrame): DataFrame containing governance data relevant to energy policies.
        df_criteria_not_weighted (pd.DataFrame): DataFrame containing criteria not adjusted by weights.

    Returns:
        pd.DataFrame: A DataFrame containing the finalized energy criteria, including total demand
                      and checks for nuclear eligibility.
    """
    df_all_criteria = fun_combine_weighted_criteria(
        df_weights, df_criteria_not_weighted
    )

    df_all_criteria = fun_add_total_demand_other(
        var, df_demand, df_all_criteria, fuel_list
    )

    # Creating list of countries allowed to have NUCLEAR
    df_all_criteria = fun_allowed_nuclear(
        df_platts, countrylist, df_gov, df_all_criteria
    )
    df_all_criteria = fun_drop_duplicates(df_all_criteria)

    # Check if sum of df_all_criteria is equal to one
    fun_check_adding_up_to_one(df_all_criteria[fuel_list])

    return df_all_criteria


def fun_eu28():
    """Returns list of EU28 ISO codes"""
    return [
        "BGR",
        "CYP",
        "DNK",
        "IRL",
        "EST",
        "AUT",
        "CZE",
        "FIN",
        "FRA",
        "DEU",
        "GRC",
        "HRV",
        "HUN",
        "ITA",
        "LVA",
        "LTU",
        "SVK",
        "MLT",
        "BEL",
        "LUX",
        "NLD",
        "POL",
        "PRT",
        "ROU",
        "SVN",
        "ESP",
        "SWE",
        "GBR",
    ]


def fun_eu27():
    """Returns list of EU28 ISO codes"""
    eu27 = fun_eu28()
    eu27.remove("GBR")
    return eu27

def fun_add_governance_for_missing_countries_using_median(df_gov, missing_country={'EU27':fun_eu27()}):
    """_summary_

    Parameters
    ----------
    ssp_scenario : _type_
        _description_
    df_gov : _type_
        _description_
    missing_country : dict, optional
        A dictionary with missing ISO as keys and values as countrylist from which we take the median, by default {'EU27':fun_eu27()}

    Returns
    -------
    _type_
        _description_
    """    
    cols=df_gov.columns
    scenarios=[x for x in df_gov.scenario.unique() if isinstance(x, str)]
    d=pd.DataFrame()
    for s in scenarios:
        for k,v in missing_country.items():
            a=df_gov[ (df_gov.scenario == s)].loc[v].set_index('TIME', append=True)
            # for i in ['governance','eff', 'readiness','corr']:
            b=a.set_index('scenario', append=True).groupby(['TIME','scenario']).median()
            c=b.assign(ISO=k).set_index('ISO', append=True)
            d=pd.concat([d,c.reset_index().set_index('ISO')])
    return pd.concat([df_gov, d])[cols]

def downscale_tot_revenues_harmonization(
    project_file,
    model_patterns,
    region_patterns,
    target_patterns,
    downs_res_file,
    iamc_index,
    RESULTS_DATA_DIR,
    selection_dict,
    model,
    file_csv,
    ra,
):
    if isinstance(downs_res_file, pd.DataFrame):
        df_downs = downs_res_file
    else:
        file_name = str(RESULTS_DATA_DIR / downs_res_file).replace("MODEL", model)
        df_downs = pd.read_csv(file_name)
    df_downs = fun_select_criteria_if_available(ra, df_downs)
    ## ISO dictionary mapping to be used later:
    myiso = df_downs.REGION.unique()
    myiso_dict = {x.replace("D.", ""): x for x in myiso}
    mymodel_dict = df_downs.set_index("REGION")["MODEL"].to_dict()

    ## NGFS file: contains Revenues data but not Emissions by sectors
    df_rev_all = pd.read_csv(file_csv)  # .replace(".csv", f"{csv_out}.csv")
    df_rev_all = fun_select_criteria_if_available(ra, df_rev_all)
    ## Block below is Creating varibles using a bottom up approach
    df_rev_all = fun_create_var_as_sum(
        df_rev_all.set_index(iamc_index),
        "Revenue|Government|Tax|Carbon",
        [
            "Revenue|Government|Tax|Carbon|Demand|Buildings",
            "Revenue|Government|Tax|Carbon|Demand|Industry",
            "Revenue|Government|Tax|Carbon|Supply",
            "Revenue|Government|Tax|Carbon|Demand|Transport",
        ],
    )
    df_rev_all = fun_validation(
        CONSTANTS,
        RESULTS_DATA_DIR,
        selection_dict,
        project_name=project_file,
        model_patterns=model_patterns,
        region_patterns=region_patterns,
        target_patterns=target_patterns,
        vars=["Revenue|Government|Tax|Carbon"],
        # csv_str=f"MODEL_{csv_str}",
        pd_dataframe_or_csv_str=df_rev_all,  # .drop("2005", axis=1),
        harmonize=True,
    )

    return df_downs, myiso_dict, mymodel_dict, df_rev_all


def downs_sectorial_revenues(
    downs_res_file,
    csv_out,
    model_in_region_name,
    iamc_index,
    df_iam,
    model,
    myiso_dict,
    mymodel_dict,
    region,
    countrylist,
    df,
):
    from downscaler.fixtures import revenue_var_list

    if len(df[df.REGION.isin(countrylist)]) == 0:
        msg = f"*** No downscaled data found for {model}_{region}. We skip this region *** "
        print(msg)
        return pd.DataFrame(), []
    else:
        df.loc[:, "MODEL"] = [x.replace("_downscaled", "") for x in df.MODEL]
        df.loc[:, "REGION"] = [x.replace("D.", "") for x in df.REGION]

        df_rev, my_index_names = fun_down_revenues(
            downs_res_file,
            csv_out,
            model_in_region_name,
            iamc_index,
            df_iam,
            model,
            myiso_dict,
            mymodel_dict,
            region,
            countrylist,
            df,
            revenue_var_list,
        )

    return df_rev  # , my_index_names


def anchor_emi_to_hist_data(
    df,
    csv_out,
    model_in_region_name,
    cols,
    df_iam,
    df_emi,
    model,
    region,
    countrylist,
):
    from downscaler.fixtures import (
        col_iam_dict,
        emissions_harmo_dict_step5b,
        iamc_idx_names,
        iea_var_dict,
        new_var_dict_step5b,
        step1_col_dict,
    )

    df.columns = [str(x) if type(x) == int else x for x in df.columns]

    # read downscaled results
    df.loc[:, "MODEL"] = [x.replace("_downscaled", "") for x in df.MODEL]
    df.loc[:, "REGION"] = [x.replace("D.", "") for x in df.REGION]

    ############################
    # Anchor emissions to hist data
    ############################
    # df[df.VARIABLE=='Emissions|CO2|Energy|Supply|Electricity|Gas']['2010']
    df_harmo = fun_anchor_emi_to_hist_data(
        model_in_region_name,
        col_iam_dict,
        iamc_idx_names,
        step1_col_dict,
        cols,
        df_iam,
        df_emi,
        model,
        region,
        countrylist,
        df,
        emissions_harmo_dict_step5b,
        new_var_dict_step5b,
        iea_var_dict,
        _baseyear=2010,
    )

    return df_harmo


def fun_emi_by_fuel(
    downs_res_file,
    csv_out,
    cols,
    iamc_index,
    df_iam_all_models,
    df_iea_act,
    emi_factor,
    eff_factor,
    model,
    df_downs,
    myiso_dict,
    mymodel_dict,
    countrylist,
    reg_name,
    target,
):
    from downscaler.fixtures import energy_dict_step5b as energy_dict
    from downscaler.fixtures import var_list_step5b as var_list

    # NOTE: var_list = list of tuples (total emi, beccs sequestation, fossil sequestration, 'how you want to call this variable')
    # tup[3]= tup[0] + tup[1] + tup[2]

    df_combi = pd.DataFrame()
    for var in var_list:
        if var == ('Emissions|CO2|Energy|Supply|Electricity', 'Carbon Sequestration|CCS|Biomass|Energy|Supply|Electricity', None, 'Emissions|CO2|Energy|Supply|Electricity'):
            here=1
        df_fuel_all = fun_split_emissions_by_fuel_and_mode(
            downs_res_file,
            csv_out,
            iamc_index,
            df_iam_all_models,
            df_iea_act,
            emi_factor,
            eff_factor,
            energy_dict,
            model,
            df_downs,
            mymodel_dict,
            countrylist,
            reg_name,
            target,
            var,
            cols,
        )

        df_fuel_all = df_fuel_all.rename(index=myiso_dict)
        df_combi= pd.concat([df_combi, df_fuel_all])
        # df_combi = df_combi.append(df_fuel_all)

        setindex(df_iea_act, False)

    return df_combi


def fun_finalise_and_drop_duplicates(
    add_only_revenues,
    cols,
    model,
    df_downs,
    myiso_dict,
    mymodel_dict,
    my_index_names,
    df_rev_all,
):
    if len(df_rev_all):
        df_rev_all = df_rev_all.droplevel("REGION").rename_axis(index={"ISO": "REGION"})
        df_rev_all = df_rev_all.reset_index()

        if add_only_revenues:
            # We filter revenues data (unit = 'billion US$2010/yr')
            df_rev_all = df_rev_all[
                (df_rev_all.MODEL != "MODEL")
                & (df_rev_all.UNIT == "billion US$2010/yr")
            ]
        else:
            # Add both emissions and revenues
            df_rev_all = df_rev_all[df_rev_all.MODEL != "MODEL"]

        df_rev_all["MODEL"] = [mymodel_dict.get(x, model) for x in df_rev_all["REGION"]]

        ## solutions:
        # 1 try/except

        # 2 using missing keys:
        # missing_keys=[x for x in aa.index.get_level_values('REGION').unique() if x not in myiso_dict.keys()]
        # myiso_dict.update({x:x for x in missing_keys})
        # does not work...

        # 3 create function to easily handle explorer and downloadable file

        df_rev_all["REGION"] = [myiso_dict.get(x, x) for x in df_rev_all.REGION]

        # NOTE: At this stage for native countries (e.g. India, Japan of REMIND) we could get different results df_downs and df_rev_all
        # Reason: df_rev_allm re-harmonize the results for "Revenue|Government|Tax|Carbon" only if differences are above a given threshold (e.g. 0.01)
        # Therefore results might be slightly different. The more accurate are the one coming directly from IAM result which are available in the `df_downs` (which only contains revenues for native countries).
        # Solution: we check for results that are present both in `df_downs` and `df_rev_all` => `idx_intersection`. We drop these results (`idx_intersection`) from df_rev_all

        idx_intersection = list(
            set(df_downs.set_index(my_index_names).index).intersection(
                df_rev_all.set_index(my_index_names).index
            )
        )
        df_rev_all = df_rev_all.set_index(my_index_names)
        if len(idx_intersection) > 0:
            df_rev_all = df_rev_all.drop(idx_intersection)

        iso_names_to_be_adj = [
            x
            for x in myiso_dict.keys()
            if x in df_rev_all.index.get_level_values("REGION")
            if x not in myiso_dict.values()
        ]

        if len(iso_names_to_be_adj) > 0:
            df_rev_all = df_rev_all.rename(index=myiso_dict)
    df_final = pd.concat([df_downs.set_index(my_index_names)[cols], df_rev_all])
    # df_final = df_downs.set_index(my_index_names)[cols].append(df_rev_all)
    df_final = fun_drop_duplicates(df_final)
    return df_final


def check_sector_value_error(
    df,
    sectors_list,
    exp_value,
    i,
    jj,
    list_of_reg_harmo_var,
    list_of_top_down_harmo_var,
):
    if len(df[df.index.get_level_values("VARIABLE").isin(sectors_list)]) > 0:
        tot = np.round(
            df[df.index.get_level_values("VARIABLE").isin(sectors_list)]
            .groupby(["MODEL", "SCENARIO", "UNIT"])
            .sum()["2010"][0]
        )
        if tot != exp_value:
            print(
                f"{sectors_list} equal to {tot}, keys {i}, values {jj}"
                f"{list_of_reg_harmo_var}, {list_of_top_down_harmo_var}"
            )
            return True


def fun_dict_without_keys(d, keys):
    return {x: d[x] for x in d if x not in keys}


def fun_reg_harmo_step5b(
    project_file,
    models,
    region_patterns,
    target_patterns,
    df_or_file_path,
    selection_dict,
    _harmonize,
    project_name,
):
    if len(models) != 1:
        raise ValueError(f"We expect only one model in models, you provided: {models}")
    model = [x for x in selection_dict if models[0].replace("*", "") in x]

    from downscaler.fixtures import (
        step5b_sect_consistency_downs_data,
        step5b_sect_consistency_reg_iam_data,
    )

    if type(df_or_file_path) != pd.DataFrame:
        csv_str = f"{df_or_file_path}"
        df = pd.read_csv(
            CONSTANTS.RES_DIR
            / "5_Explorer_and_New_Variables"
            / f"Explorer_{model}_{csv_str}.csv"
        )
    else:
        df = df_or_file_path
    myindex = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]
    df.set_index(myindex, inplace=True)

    vars_downs = list(step5b_sect_consistency_downs_data.keys())

    additional_var_to_be_harmo = [
        "Emissions|CO2|Industrial Processes",
        "Emissions|CO2|Energy|Supply|Heat",
    ]

    all_vars = additional_var_to_be_harmo + vars_downs

    # Reading df_iam_all_models
    input_file = CONSTANTS.INPUT_DATA_DIR / project_name / "snapshot_all_regions.csv"
    df_iam_all_models = fun_read_df_iam_all(
        file=InputFile(input_file),
        add_model_name_to_region=False,
        categorical_index=False,
    )
    df_iam_all_models.loc[:, "REGION"] = df_iam_all_models.loc[:, "REGION"] + "r"
    df_iam_all_models.set_index(myindex, inplace=True)

    iam_var_list = fun_check_if_variables_in_dict_values_are_present_in_df_iam(
        step5b_sect_consistency_reg_iam_data, df_iam_all_models, _harmonize
    )

    df_iam_all_models = df_iam_all_models[
        df_iam_all_models.index.get_level_values("VARIABLE").isin(
            iam_var_list + all_vars
        )
    ]

    # Create new variables in downscaled results
    list_of_new_variables = []
    for _ in range(0, 2):
        for i, j in step5b_sect_consistency_downs_data.items():
            if i not in df.index.get_level_values("VARIABLE"):
                # NOTE: we do not create variables as sum for aliased sectors (sectors that constain the string ' v2').
                # reason: because here we use the `step5b_sect_consistency_downs_data` which is also used in the top-down harmonization below (here the order of dictionaries matters - therefore we cannot change it)
                # With this keys order, however, we cannot create variable as sum for sectors that contain ' v2' because some variable have not been created yet.
                # Therefore we skip all the sectors that contain ' v2'.
                # if " v2" not in i:
                try:
                    list_of_new_variables += [i]
                    df = fun_create_var_as_sum(df, i, j, unit="Mt CO2/yr")
                except:
                    pass

        missing = [
            x
            for x in step5b_sect_consistency_downs_data.keys()
            if x not in df.index.get_level_values("VARIABLE")
            and x in df_iam_all_models.index.get_level_values("VARIABLE")
        ]
        if len(missing) > 0:
            raise ValueError(
                f"{missing} are available in IAMs results, but missing in downscaled results"
            )

    # Create new variables in df_iam_all_models (melted_format)
    df_iam_all_models = fun_create_var_as_sum_df_iam_all_models(
        df_iam_all_models,
        step5b_sect_consistency_reg_iam_data,
    )

    if "2005" in df.columns:
        df = df.drop("2005", axis=1)

    check_sector = "Emissions|CO2|Energy|Supply|Electricity EXCL BECCS"
    val_sector = 300

    # check_sector = "Emissions|CO2|Energy EXCL BECCS v2"
    # val_sector = 300

    if not _harmonize:
        # we do not check the v2 sectors because we have not harmonized them (on purpose, because we do not need it), as
        # main sector results are always read by removing ' v2' when we do the top-down harmonization
        all_vars = [x for x in all_vars if "v2" not in x]

    df = fun_validation(
        CONSTANTS,
        Path("results/5_Explorer_and_New_Variables"),
        selection_dict,
        project_name=project_file,
        model_patterns=models,
        region_patterns=region_patterns,
        target_patterns=target_patterns,
        vars=all_vars,
        pd_dataframe_or_csv_str=df,
        harmonize=_harmonize,
        save_to_csv=False,
        skip_regions=None,  # ["MESSAGEix-GLOBIOM 1.1-M-R12|Eastern Europer"],
        df_iam_all_models=df_iam_all_models,
    )

    if _harmonize:
        df = (
            df.reset_index()
            .drop("REGION", axis=1)
            .rename(columns={"ISO": "REGION"})
            .set_index(myindex)
        )

        list_of_reg_harmo_var = []
        list_of_top_down_harmo_var = []

        # Top down harmonization
        for i, j in step5b_sect_consistency_downs_data.items():
            # blueprint:
            # 0) Final regional harmonization for the main sector as top downs harmo can break consistency with regional IAMs results
            df = fun_simple_reg_harmo(
                df,
                project_file,
                models,
                region_patterns,
                target_patterns,
                selection_dict,
                _harmonize,
                myindex,
                df_iam_all_models,
                i,
                _replace_main_sector=" v2",
            )
            for jj in j:
                # 1) we do a top-down harmonization for the sub_sectors
                # if jj not in iam_var_list:
                df = fun_simple_top_down(
                    df,
                    i,  # e.g. "Emissions|CO2|Energy EXCL BECCS v2"
                    j,  # e.g. [ "Emissions|CO2|Energy|Supply|Electricity EXCL BECCS", "Emissions|CO2|Energy|Demand|Industry" ]
                    jj,  # e.g.  "Emissions|CO2|Energy|Demand|Industry"
                    list(step5b_sect_consistency_downs_data.keys()),
                    # _replace_main_sector="",  # " v2",
                    _replace_main_sector=" v2",
                    _scalar=0.5,  # needed to enhance sectorial harminization
                )
                list_of_top_down_harmo_var += [jj]
                # 2) we  regionally harmonize the sub_sectors
                if jj in iam_var_list + list_of_new_variables:
                    df = fun_simple_reg_harmo(
                        df,
                        project_file,
                        models,
                        region_patterns,
                        target_patterns,
                        selection_dict,
                        _harmonize,
                        myindex,
                        df_iam_all_models,
                        jj,  # sector to be harmo
                        _replace_main_sector=" v2",
                    )
                    list_of_reg_harmo_var += [jj.replace(" v2", "")]

        # NOTE we remove list of newly created data (in fun_create_var_as_sum).
        # Otherwise next time we validate the results it will not do a real validation
        # (it will not re-calculate the sum based on sub-sectors)
        df = df[~df.index.get_level_values("VARIABLE").isin(list_of_new_variables)]
        return df


def fun_simple_reg_harmo(
    df,
    project_file,
    models,
    region_patterns,
    target_patterns,
    selection_dict,
    _harmonize,
    myindex,
    df_iam_all_models,
    jj,
    _replace_main_sector=" v2",
):
    df = fun_validation(
        CONSTANTS,
        Path("results/5_Explorer_and_New_Variables"),
        selection_dict,
        project_name=project_file,
        model_patterns=models,
        region_patterns=region_patterns,
        target_patterns=target_patterns,
        # NOTE below we replace ' v2' so that we always read the same results for the main sector.
        # otherwise results can be different at the country level
        vars=[jj.replace(_replace_main_sector, "")],
        pd_dataframe_or_csv_str=df,
        harmonize=_harmonize,
        save_to_csv=False,
        skip_regions=None,  # ["MESSAGEix-GLOBIOM 1.1-M-R12|Eastern Europer"],
        df_iam_all_models=df_iam_all_models,
    )

    df = (
        df.reset_index()
        .drop("REGION", axis=1)
        .rename(columns={"ISO": "REGION"})
        .set_index(myindex)
    )

    return df


def fun_simple_top_down(
    df, i, j, jj, list_all_main_sectors, _replace_main_sector: str = " v2", _scalar=0.05
):
    print(f"top-down harmonization: {jj}")
    sector = df[df.index.get_level_values("VARIABLE").isin(j)]
    sector_sum = sector.groupby(["MODEL", "SCENARIO", "REGION", "UNIT"]).sum()
    ratio = sector / sector_sum

    # NOTE below we replace ' v2' so that we always read the same results for the main sector.
    # otherwise results can be different at the country level
    main_sector = df.xs(i.replace(_replace_main_sector, ""), level="VARIABLE")

    updated_results = ratio * main_sector

    if jj in list_all_main_sectors:
        # We muliply by an 2010 ratio value to enhance regional consistency harmonization
        # Results l be then re-harmonized later, as this sector `jj` is in the  `list_all_main_sectors`
        # (all main sectors will be always re-harmonized to match iam results)
        if sector.xs(jj, level="VARIABLE")["2010"].sum() == 0:
            print("use main sector data or aggregated data in 2010")
            # If we do not have 2010 data it means we rely on ENLONG_RATIO data.
            # The _scalar here is the percentage of total emissions. When we use ENLONG_RATIO we assume the same share (_scalar) with respect to
            # total in all countries.
            # enlong_results = (
            #     updated_results.xs(jj, level="VARIABLE", drop_level=False) * _scalar
            # )
            idx_enlong = updated_results.xs(
                jj, level="VARIABLE", drop_level=False
            ).index

            updated_results.loc[idx_enlong, :] = 0.05 * main_sector

    # NOTE update results only for a single jj sector (not for all, otherwise breaks regional consistency)
    # updated_results = updated_results.xs(
    #     jj, level="VARIABLE", drop_level=False
    # )
    updated_results = updated_results.reset_index().set_index(df.index.names)

    # try:
    #     df.loc[updated_results.index] = updated_results
    # except:
    idx_to_be_updated = list(set(updated_results.index).intersection(df.index))
    df.loc[idx_to_be_updated] = updated_results.loc[idx_to_be_updated]
    return df


def fun_get_step5b_models_and_data(project_file, model_patterns, file):
    # Reading model mapping
    pyam_mapping_file: IFT = (
        CONSTANTS.INPUT_DATA_DIR / project_file / "default_mapping.csv"
    )

    df_iam_all_models, df_iea_act = fun_get_df_iam_and_iea_activity_data(file)

    # read regional iam results
    input_file = CONSTANTS.INPUT_DATA_DIR / project_file / "snapshot_all_regions.csv"
    df_iam, df_emi = fun_input_data_step5b(input_file)

    # List of models
    models = df_iam_all_models.MODEL.unique().tolist()
    if "Reference" in models:
        models.remove("Reference")
    models = [m for m in models if match_any_with_wildcard(m, model_patterns)]
    return pyam_mapping_file, df_iam_all_models, df_iea_act, df_iam, df_emi, models


def fun_get_step5_results(
    downs_res_file: Union[pd.DataFrame, str], model: str, RESULTS_DATA_DIR: Path
):
    ## read previously downscaled results:
    if isinstance(downs_res_file, pd.DataFrame):
        df_downs = downs_res_file
    else:
        downs_res_file = RESULTS_DATA_DIR / downs_res_file
        df_downs = pd.read_csv(str(downs_res_file).replace("MODEL", model))

    ## ISO dictionary mapping to be used later:
    myiso = df_downs.REGION.unique()
    myiso_dict = {x.replace("D.", ""): x for x in myiso}
    mymodel_dict = df_downs.set_index("REGION")["MODEL"].to_dict()

    df_downs.loc[:, "MODEL"] = [x.replace("_downscaled", "") for x in df_downs.MODEL]

    df_downs.loc[:, "REGION"] = [x.replace("D.", "") for x in df_downs.REGION]
    return df_downs, myiso_dict, mymodel_dict


def fun_get_targets_df_countries_regions(selection_dict, pyam_mapping_file, model):
    targets = list(selection_dict[model]["targets"])

    df_countries = fun_read_df_countries(
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv"
    )

    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    regions = [i for i in selection_dict[model]["regions"]]
    return targets, df_countries, regions


def fun_get_df_iam_and_iea_activity_data(file):
    df_iam_all_models = fun_read_df_iam_all(
        file=InputFile(CONSTANTS.INPUT_DATA_DIR / file), add_model_name_to_region=False
    )

    df_iam_all_models.loc[:, "REGION"] = df_iam_all_models.loc[:, "REGION"] + "r"

    df_iea_act = fun_read_df_iea_act_and_reshape(
        CONSTANTS.INPUT_DATA_DIR / "GAINS_ISO3.csv",  # gains path
        CONSTANTS.INPUT_DATA_DIR  # iea activity path
        / "Emission_Clock_transport_act_data_aggregation_WEO2021_STEPS_V2.csv",
    )
    return df_iam_all_models, df_iea_act


def fun_read_df_iam_iamc(input_file: str) -> pd.DataFrame:
    """[This function reads IAMs results from a given file path `input_file`]

    Args:
        input_file ([str]): [Path of file with IAMs results]

    Returns:
        [pd.DataFrame]: [Dataframe with IAM results in IAMc format]
    """  ## Reading iam results
    df_iam = pd.read_csv(input_file)
    df_iam = fun_clean_up_df_iam(df_iam)
    return df_iam


def fun_clean_up_df_iam(df_iam, replace_scenario_w_target=True, add_r=True):
    idx = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]

    df_iam.columns = [x.upper() for x in df_iam.columns]
    df_iam = fun_remove_np_in_df_index(
        df_iam.set_index(idx), sel_type=str
    ).reset_index()
    # Slice for allowed regions (region must be a string)
    allowed_regions = [x for x in df_iam.REGION.unique() if type(x) is str]
    df_iam = df_iam[df_iam.REGION.isin(allowed_regions)]

    df_iam = df_iam[
        (~(df_iam.MODEL == "Reference"))
    ]  ### Get only Energy and emissions variables (2021_03_02)
    if add_r:
        ## adjusting region name to avoid overlap with ISO codes
        df_iam.loc[:, "REGION"] = [x + "r" for x in df_iam.REGION]
    if replace_scenario_w_target:
        df_iam.rename(columns={"SCENARIO": "TARGET"}, inplace=True)
        idx = ["MODEL", "TARGET", "REGION", "VARIABLE", "UNIT"]
    return fun_index_names(df_iam, True, int, idx)
    # df_iam.set_index(idx, inplace=True)
    # cols = [x for x in df_iam.columns if str(x[0]) in {"1", "2"}]
    # df_iam = df_iam[cols]
    # df_iam.columns = [int(x) for x in df_iam.columns]
    # return df_iam


def fun_input_data_step5b(input_file: str) -> pd.DataFrame:
    """This function returns IAMs results and historical IEA data.
    It also crates a new variable in the dataframe with IAMs results called "Emissions|CO2|Energy|Supply|Electricity and heat excl BECCS",
    defined as the sum of electricity, heat and BECCS from the electricity sector.

    Args:
        input_file (str): input file path of IAM results

    Returns:
        pd.DataFrame: dataframe with IAM results and historical emissions data from IEA
    """
    df_iam = fun_read_df_iam_iamc(input_file)
    df_iam.columns = [str(x) for x in df_iam.columns]

    vars_dict = {
        "Emissions|CO2|Energy|Supply|Electricity": 1,  # 1 positive sign (+). -1 means negative sign (-1)
        "Emissions|CO2|Energy|Supply|Heat": 1,
        "Carbon Sequestration|CCS|Biomass|Energy|Supply|Electricity": 1,  # CCS is reported as positive value. Ok to use 1 (+ sign) here.
    }
    # Create additonal variables in df_iam (if all elements in vars_dict.keys() are included in df_iam.VARIABLE)
    if all(x in df_iam.reset_index().VARIABLE.unique() for x in vars_dict):
        df_iam = fun_create_var_as_sum(
            df_iam,
            "Emissions|CO2|Energy|Supply|Electricity and heat excl BECCS",
            vars_dict,
            _level="VARIABLE",
        )

    # read historical emissions data
    df_emi = fun_read_hist_emissions()

    return df_iam, df_emi


def fun_read_hist_emissions():
    df_emi = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "IEA 2019 CO2 emissions from fuels_ISO.csv",
        sep=",",
        encoding="latin",
        dtype={"REGION": str, "ISO": str},
    )

    df_emi = df_emi.set_index(["ISO", "FLOW", "PRODUCT"])
    df_emi=fun_drop_columns(df_emi, case_sensitive=False, 
                            col_names=["Unnamed", "COUNTRY", "TIME", "REGION", "FUEL_IAM"])

    return df_emi


def fun_save_to_excel(
    project_file: Union[str, None],
    downs_res_file: str,
    model: str,
    df_final: pd.DataFrame,
    input_arguments: dict,
):
    # Writing the output to excel

    with pd.ExcelWriter(
        downs_res_file.replace("MODEL", model).replace(".csv", "_FINAL.xlsx")
    ) as writer:
        df_final.reset_index().to_excel(writer, sheet_name="data", index=False)

        # NOTE could do a for loop for all projects (and all models) e.g. if project_file is a list (and append all version_info)
        if (
            project_file
            and (CONSTANTS.INPUT_DATA_DIR / project_file / "versions.csv").is_file()
        ):
            version_info = pd.read_csv(
                CONSTANTS.INPUT_DATA_DIR / project_file / "versions.csv"
            )
            version_info = version_info[
                (version_info["model"] == model)
                & (
                    version_info["scenario"].isin(
                        df_final.index.get_level_values("SCENARIO").unique()
                    )
                )
            ]
            # Rename as agreed with Philip on 20230426
            version_info["model"] = [f"Downscaling[{x}]" for x in version_info.model]
            version_info = version_info.rename(columns={"version": "iam_version"})
            version_info["COMMIT_HASH"] = get_git_revision_short_hash()
            version_info.to_excel(writer, sheet_name="meta", index=False)
        else:
            print(
                "Did not find version information at "
                f"{CONSTANTS.INPUT_DATA_DIR / project_file / 'versions.csv'}"
            )
        if input_arguments is not None:
            downs_conf = pd.DataFrame([input_arguments]).T[0]
            downs_conf.to_excel(
                writer,
                sheet_name="downscaling_config",
                index=True,
            )


def fun_consistency_check(
    df: pd.DataFrame, check_until_2050: bool = True
) -> Union[float, float, pd.DataFrame]:
    """This function checks how many countries have sectorial emissions higher than total emissions.

    NOTE: How do we count for countries? We count for countries while also taking into account how large is the mismatch. We do a weighted sum (where the weight is the mismatch between sectorial emissions and total emissions)

    EXAMPLES:
    - if a country has twice as much of sectorial emissions compared to total emissions, it will be counted as twice (e.g. we assume we have 2 countries above the threshold).
    - if a country has sectorial emissions above 10% of total emissions, it will be counted as 1.1 (e.g. we have 1.1 countries abobe the threshold).
    - if a country has sectorial emissions below total emissions, it will be counted as zero.

    Args:
        df (pd.DataFrame): your dataframe
        check_until_2050 (bool, optional): Checks data until 2050 (True/False). Defaults to True.

    Returns:
        Union[float,float, pd.DataFrame.index]: 1) % of countries with sectorial emissions above total (standardised value), 2) Absolute number of countries above threshold (weighted sum), 3) Index of the dataframe associated with countries where we found the largest mismatch.
    """
    from downscaler.fixtures import emissions_harmo_dict_step5b

    # Create 'main sectors'  variables
    for i in sorted(emissions_harmo_dict_step5b.keys(), reverse=True):
        df = fun_create_var_as_sum(
            df, i, emissions_harmo_dict_step5b[i], unit="Mt CO2/yr"
        )

    # Check if the sum of sub sectors is greater than the main sector
    check = df.xs("Emissions|CO2|Energy excl BECCS v2", level="VARIABLE") / df.xs(
        "Emissions|CO2|Energy excl BECCS", level="VARIABLE"
    )
    # We filter data where the sum of sectoral data is above total (if this is not the case, we get np.nan)
    check = check[check > 1]

    # We check data only until 2050
    if check_until_2050:
        cols = [str(x) for x in range(2010, 2051)]
        check = check.iloc[:, check.columns.isin(cols)]

    value = check.sum().sum()
    value_standardised = value / np.prod(check.shape)
    no_of_countries_with_sectoral_emi_above_total = check.count().sum()

    # Below we identify the column with the maximum value (where the biggest problem is)
    max_val = check.dropna(how="all").max().max()
    if max_val > 0:
        col = (
            check[check == max_val]
            .dropna(how="all")
            .dropna(how="all", axis=1)
            .columns[0]
        )

        # Sort by column where we found the maximum value
        # idx_with_emi_above_total = check.dropna(how="all").head().index  # just a selection
        idx_with_emi_above_total = (
            check.dropna(how="all").sort_values(col, ascending=False).head().index
        )
    else:
        # e.g. max_val is none
        idx_with_emi_above_total = None
    return (
        value_standardised,
        no_of_countries_with_sectoral_emi_above_total,
        idx_with_emi_above_total,
    )


def fun_consistency_check_above_threshold(
    df: pd.DataFrame,
    max_threshold: float = 0.01,
    check_until_2050: bool = True,
):
    """This function checks sectorial consistency in the `df`.
    It checks if the sum of sectorial emissions is abothe the total emissions.
    This check passes if less than `max_threshold` (e.g 1%) of countries have sectorial emissions above thresholds.
    Otherwise it raises a value error.
    NOTE: How do we count for countries? We count for countries while also taking into account how large is the mismatch.

    EXAMPLES:
    - if a country has twice as much of sectorial emissions compared to total emissions, it will be counted as twice (e.g. we assume we have 2 countries above the threshold).
    - if a country has sectorial emissions above 10% of total emissions, it will be counted as 1.1 (e.g. we have 1.1 countries abobe the threshold).
    - if a country has sectorial emissions below total emissions, it will be counted as zero.

    Args:
        df (pd.DataFrame): Your dataframe
        max_threshold (float, optional): Max % of countries allowed to have sectorial emissions above total emissions . Defaults to 0.01 (1%).
        check_until_2050 (bool, optional): Checks data until 2050 (True/False). Defaults to True.

    Raises:
        ValueError: Raise error if this check fails and print a selection of countries/targets wher we found the largest discrepancy between secorial emissions and total.
    """
    warning_threshold = max_threshold / 2

    val, abs_val, idx = fun_consistency_check(df, check_until_2050=check_until_2050)

    if val > max_threshold:
        raise ValueError(
            f"{np.round(val*100,5)}% of countries/targets combinations in your dataframe contain data where the sum of sectorial emissions is above the main sector. This is above your chosen maximum threshold of {max_threshold*100}%."
            f"\n Example of countries with the sum of sectorial emissions above total: \n {idx}"
            "\n Please check your emissions downscaling method or increase the percentage of your max_threshold to silent this error"
        )
    if val > warning_threshold:
        print(
            f"WARNING: more than {np.round(val*100,5)}% of countries/targets combinations in your dataframe contain data where the sum of sectorial emissions is above the main sector"
        )
    else:
        print(
            f"Sectorial consistency is good - {np.round(val*100,3)}% of countries have sectorial emissions above total"
        )


def fun_step5b_input_data(
    project_file, model_patterns, region_patterns, target_patterns, get_selection_dict
):
    file: IFT = CONSTANTS.INPUT_DATA_DIR / project_file / "snapshot_all_regions.csv"
    try:
        selection_dict = get_selection_dict(
            InputFile(file),
            model=model_patterns,
            region=region_patterns,
            target=target_patterns,
            variable=["*"],
        )
    except ValueError as e:
        raise ValueError(
            f"{e}\nConsider using the asterisk '*' for the pattern."
        ) from e

    cols = [str(x) for x in range(2010, 2105, 5)]

    iamc_index = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]

    (
        pyam_mapping_file,
        df_iam_all_models,
        df_iea_act,
        df_iam,
        df_emi,
        models,
    ) = fun_get_step5b_models_and_data(project_file, model_patterns, file)

    return (
        selection_dict,
        cols,
        iamc_index,
        pyam_mapping_file,
        df_iam_all_models,
        df_iea_act,
        df_iam,
        df_emi,
        models,
    )


def fun_get_countrylist_reg_name(model_in_region_name, model, df_countries, region):
    countrylist = df_countries[
        df_countries.REGION == region.replace(model + "|", "") + "r"
    ].ISO.unique()
    if model_in_region_name:
        reg_name = region + "r"
    else:
        reg_name = region.replace(model + "|", "") + "r"
    return countrylist, reg_name


def downscale_emissions_all_regions_targets(
    downs_res_file,
    csv_out,
    model_in_region_name,
    cols,
    iamc_index,
    df_iam_all_models,
    df_iea_act,
    model,
    countrylist,
    df_downs,
    myiso_dict,
    mymodel_dict,
    targets,
    reg_name,
    region,
    emi_factor,
    eff_factor,
    df_iam,
    df_emi,
):
    df_all = pd.DataFrame()
    df_hanchored_all = pd.DataFrame()

    if len(df_downs[(df_downs.REGION.isin(countrylist))]) == 0:
        msg = f"*** No downscaled data found for {model}_{region}. We skip this region *** "
        print(msg)
    else:
        for target in targets:
            # Downscale emissions by fuel/mode and save to csv
            df = fun_emi_by_fuel(
                downs_res_file,
                csv_out,
                cols,
                iamc_index,
                df_iam_all_models,
                df_iea_act,
                emi_factor,
                eff_factor,
                model,
                # count,
                df_downs,
                myiso_dict,
                mymodel_dict,
                countrylist,
                reg_name,
                target,
            )
            # fun_xs(df, {"VARIABLE":'Emissions|CO2|Energy|Supply|Electricity|Gas'})[2010]
            df_all = pd.concat([df_all, df])
        # fun_xs(df_all, {"VARIABLE":'Emissions|CO2|Energy|Supply|Electricity|Gas'})
        df_anchored = anchor_emi_to_hist_data(
            df_all.reset_index(),
            csv_out,
            model_in_region_name,
            cols,
            df_iam.xs(reg_name, level="REGION", drop_level=False),
            df_emi[df_emi.index.get_level_values(0).isin(countrylist)],
            model,
            region,
            countrylist,
        )
        df_hanchored_all = pd.concat([df_hanchored_all, df_anchored])
    return df_hanchored_all


def fun_downscale_emi_and_save_csv(
    project_file,
    region_patterns,
    target_patterns,
    downs_res_file,
    csv_out,
    model_in_region_name,
    selection_dict,
    cols,
    iamc_index,
    df_iam_all_models,
    df_iea_act,
    df_iam,
    df_emi,
    model,
    df_downs,
    myiso_dict,
    mymodel_dict,
    targets,
    df_countries,
    regions,
    csv_name_out,
    emi_factor,
    eff_factor,
    ra,  # standard, upper, lower criteria
    add_only_revenues,
):
    count = 0
    for region in regions:
        countrylist, reg_name = fun_get_countrylist_reg_name(
            model_in_region_name, model, df_countries, region
        )
        # NOTE We downscale emissions only for regions that are non-native (more that 1 country in the countrylist).
        # Otherwise we get multiple values for the same variable (downscaled vs native) e.g. we had this issue for ('GCAM 6.0 NGFS', 'RUS', 'Emissions|CO2|Energy|Supply|Heat', 'Mt CO2/yr', 'd_delfrag')
        if len(countrylist) > 1:
            df = downscale_emissions_all_regions_targets(
                downs_res_file,
                csv_out,
                model_in_region_name,
                cols,
                iamc_index,
                df_iam_all_models,
                df_iea_act,
                model,
                countrylist,
                df_downs,
                myiso_dict,
                mymodel_dict,
                targets,
                reg_name,
                region,
                emi_factor,
                eff_factor,
                df_iam,
                df_emi,
            )

            # NOTE: We write to CSV so that we do not keep data in memory
            df = fun_add_criteria(ra, df)
            # here 'Emissions|CO2|Energy|Supply|Heat' is twice as much as the regional data

            # Drop 2005 data
            cols_drop = [2005]
            cols_drop = cols_drop + [str(x) for x in cols_drop]
            for x in cols_drop:
                if x in df.columns:
                    df = df.drop(x, axis=1)

            # if count == 0 and ra == "standard":
            if count == 0:
                df.to_csv(csv_name_out, mode="w")
            else:
                df.to_csv(csv_name_out, mode="a", header=False)
            if len(df) > 0:
                count = count + 1

    # NOTE: Read data with all regions
    df = pd.read_csv(csv_name_out)
    # Select criteria "ra" if available
    df = fun_select_criteria_if_available(ra, df)

    df_harmo = fun_reg_harmo_step5b(
        project_file,
        [model],
        region_patterns,
        target_patterns,
        df,
        selection_dict,
        _harmonize=True,
        project_name=project_file,
    )
    if not add_only_revenues:
        # We check consistency only if we want to report the emissions variables
        fun_consistency_check_above_threshold(df_harmo, max_threshold=0.05)
    df_harmo = fun_add_criteria(ra, df_harmo)
    df_harmo.to_csv(csv_name_out, mode="w")
    return df_harmo


def fun_select_criteria_if_available(ra, df):
    # Add standard criteria only if "CRITERIA" column is not already available from df
    df = fun_add_criteria("standard", df)
    # Otherwise we select current `ra` criteria
    df = fun_select_criteria(df, ra)
    return df


def fun_append_regional_iam(model, df_countries, df, native_countries, df_iam):
    reg_native_dict, df_missing_iam_native_regions = fun_native_iam_countries(
        model, df_countries, native_countries, df_iam, df=df
    )

    df = pd.concat([df, df_missing_iam_native_regions])
    # df = df.append(df_missing_iam_native_regions)
    index_name = list(df.index.names)
    setindex(df, False)
    df["REGION"] = [reg_native_dict.get(i, i) for i in df.REGION]
    setindex(df, index_name)

    df = df[~df.index.duplicated()]
    return df


def fun_interpolate(
    df_input: pd.DataFrame,
    time_cols_string: bool,
    time_range: range = range(2005, 2105, 5),
    interpolate_columns_present=False,
) -> pd.DataFrame:
    """Interpolates the dataframe (if provided using an IAMC format), but only if columnns are missing. Returns the
    interpolated dataframe for a given `time_range`

    Args:
        df_input (pd.DataFrame): Dataframe to be intepolated
        time_cols_string (bool): Choose if the time columns of the dataframe should be
         reported as string (e.g. '2010') or int (e.g. 2010)
        time_range (range, optional): Range of columns to be interpolated. Defaults to
         range(2005, 2105, 5).

    Returns:
        pd.DataFrame: Interpolated dataframe
    """
    df = df_input.copy(deep=True)
    # Check if df is in iamc format:
    fun_check_iamc_index(df)

    df = fun_index_names(df, True, int)
    # Insert missing columns
    cols = [int(i) for i in time_range]
    missing_cols = [x for x in cols if x not in df.columns]
    for x in missing_cols:
        df.insert(1, x, np.nan)
    df = df.reindex(sorted(df.columns), axis=1)

    # Interpolate dataframe
    df_interpolated = df.copy()
    if interpolate_columns_present:
        df_interpolated.loc[:, cols] = df[cols].interpolate(axis=1).loc[:, cols]
    else:
        # Interpolates missing colums only
        df_interpolated.loc[:, missing_cols] = (
            df[missing_cols].interpolate(axis=1).loc[:, missing_cols]
        )

    # interp_columns=[x for x in df_interpolated.columns.copy() if x not in df.columns.copy()]

    # [df.insert(1, x, np.nan) for x in interp_columns]
    # df.loc[:, interp_columns]=df_interpolated.loc[:,interp_columns]

    if time_cols_string:
        df_interpolated.columns = [str(i) for i in df_interpolated.columns]
    return df_interpolated


def fun_cumulative_emissions(df_ghg, time_cols_string: bool, baseyear):
    _to = int(df_ghg.columns[-1])
    time_range = range(baseyear, _to)
    # Interpolate results using annual time steps
    df_interpolated = fun_interpolate(df_ghg, time_cols_string, time_range=time_range)

    if time_cols_string:
        time_range = [str(i) for i in time_range]
    return df_interpolated.cumsum(axis=1)[time_range]


def fun_pick_one_blending(
    df: pd.DataFrame, conv="MED", filter_var="Final", filter_dict=None
) -> pd.DataFrame:
    """Pick selected converge criteria for a given variable

    Args:
        df (pd.DataFrame): dataframe with a range of projections
        conv (str, optional): Selected convergence criteria. Defaults to "MED".
        filter_var (str, optional): Selected variables (e.g. Final energy variables).
         Defaults to "Final".
        filter_dict (_type_, optional): Dictionary with convergence criteria for a given
         variable. Defaults to None.

    Returns:
        pd.DataFrame: Dataframe with selected converge criteria for a given variable
    """

    if filter_dict is None:
        from downscaler.fixtures import fun_conv_settings

        filter_dict = fun_conv_settings(False)  # we run it with sensitivity = False

    # filter
    df = df.xs("EJ/yr", level="UNIT", drop_level=False)  # energy variables only
    df = df[
        (df.index.get_level_values("VARIABLE").str.contains(filter_var))
        & (
            df.index.get_level_values("VARIABLE").str.contains(
                str(filter_dict[filter_var][conv])
            )
        )
    ]

    idxnames = df.index.names
    df = df.reset_index()
    df.loc[:, "VARIABLE"] = [
        x.replace(f"{str(filter_dict[filter_var][conv])}_BLEND", "")
        for x in df.VARIABLE
    ]
    return df.set_index(idxnames)


def fun_finalize_step2_results(df: pd.DataFrame, _conv: str) -> pd.DataFrame:
    """Append selected 'Final', 'Secondary', 'Primary' data to existing dataframe

    Args:
        df (pd.DataFrame): Dataframe with a range of projections
        _conv (str): selected convergence criteria

    Returns:
        pd.DataFrame: Combined dataframe with Final, Secondary and Primary energy variables
    """

    df_all = pd.DataFrame()
    for var in ["Final", "Secondary", "Primary"]:
        df_append = fun_pick_one_blending(df, conv=_conv, filter_var=var)
        df_all = pd.concat([df_all, df_append])
    return df_all


def fun_finalize_step2_all_targets(
    df: pd.DataFrame,
    standard_conv: Union[str, None],
    target_dict: Union[dict, None],
    check_taget_dict_keys: bool = True,
) -> pd.DataFrame:
    """This function finalized step2 results (Final, Secondary and Primary), by picking one convergence criteria for each target, based on a `target_dict`.
    If `target_dict` is not provided then we use a `standard_conv` assumption for all targets.
    It returns the updated dataframe with a single pathway (chosen convergence criteria).

    NOTE: This function only considers variables with `unit= EJ/yr` and that contain  the sting 'Final', 'Secondary', 'Primary', because those are the variables that we expect from steps 1 and 2.
    Any other variable (e.g. 'Oil price') will be dropped from the dataframe.

    Args:
        df (pd.DataFrame): dataframe from Step 2 with a range of projections for each convergence criteria
        standard_conv (Union[str, None]): Convergence assumption to be used if target_dict is None
        target_dict (Union[dict, None]): Dictionary with convergence assumptions (values) for each target (keys)
        check_taget_dict_keys(bool): Whether to check if all keys in `target_dict` are present in the df targets

    Returns:
        pd.DataFrame: Dataframe with a single pathway (chosen convergence criteria)
    """
    fun_check_input(standard_conv, target_dict)
    fun_check_iamc_index(df)

    # NOTE: Example target_dict={'SSP5-Baseline_marker_scenario':"MED"}
    allowed_targets = list(df.index.get_level_values("SCENARIO").unique())
    allowed_conv = ["SLOW", "MED", "FAST"]

    if check_taget_dict_keys:
        targets = allowed_targets if target_dict is None else list(target_dict.keys())
    else:
        # we just use targets available from previously downscaled results
        targets = allowed_targets

    df_all = pd.DataFrame()
    for target in targets:
        fun_check_target(allowed_targets, target)
        conv = standard_conv if target_dict is None else target_dict[target]
        fun_check_conv(allowed_conv, conv)
        print(f"{target}: {conv}")
        df_append = fun_finalize_step2_results(
            df.xs(target, level="SCENARIO", drop_level=False), conv
        )
        df_all = pd.concat([df_all, df_append])

    return df_all


def fun_check_input(standard_conv, target_dict):
    if standard_conv is not None and target_dict is not None:
        raise ValueError(
            "`standard_conv` must be None if you want to use a `target_dict`"
        )
    if standard_conv is None and target_dict is None:
        raise ValueError("Please provide either a `standard_conv` or a `target_dict`")


def fun_check_conv(allowed_conv, conv):
    if conv not in allowed_conv:
        raise ValueError(
            f"_conv should contain one of the following: {allowed_conv}. You typed: {conv}"
        )


def fun_check_target(allowed_targets, target):
    if target not in allowed_targets:
        raise ValueError(
            f"{target} is not present in the df. df contains: {allowed_targets} "
            f"\n Please check your target_dict keys or your df"
        )

def fun_select_criteria_with_max_range(
    countrylist: list, full_range: list, df_desired_dict: dict
) -> Union[list, dict]:
    """Select criteria with maximum range of emissions compared to standard assumption for ENSHORT
    By default we do this by checking
    - absolute emissions from electricity (not percentage variation),
    - by checking using 'dynamic_recursive' calculations
    - by checking data only until 2050 (here we do short term projections).

    NOTE: in the future this could be also done to check max deviation compared to ENLONG beyond 2050 (see blueprint in
    `fun_select_max_range_across_all_countries` )


    Parameters
    ----------
    countrylist : list
        _description_
    full_range : list
        Criteria range of sensitivity analysis
    df_desired_dict : dict
        dictionary with results

    Returns
    -------
    _type_
        List of selected criteria (across all countries) and a dictionary with individual max/min criteria for each country
    """
    
    # SUGGEST TO USE COMMENTED BLOCK BELOW which takes min/max for each single fuels, not just for electricity related emissions
    # res={}
    # # Get results for time (t), country (c) and fuel (f)
    # for c in ['AUS','NZL','JPN']:
    #     for f in fuels:
    #         res_single_fuel_country=df_desired.xs((t, c), level=('TIME','ISO'))[[f]].describe().T[['min','max']]
    #         for x in ['min','max']:
    #             res[f"{c}-{f}-{x}"]=df_desired[df_desired.eq(res_single_fuel_country[x].iloc[0]).any(1)].CRITERIA.iloc[0]

    # # List of all criteria associated to min/max across all countries
    # all_criteria=set(list(res.values())+['standard'])
    
    
    
    # NOTE: 'emi' refers to emissions from electricity
    all_seeds_by_country, max_occ_seeds = select_criteria_single_variable(countrylist, full_range, df_desired_dict, 'emi')
    return fun_flatten_list(max_occ_seeds, _unique=True), all_seeds_by_country
    # USE The below for the full set by each fuel
    res={x:select_criteria_single_variable(countrylist, full_range, df_desired_dict, x) for x in ['emi','SOL']}
    fuel_list=['BIO', 'COAL',  'GAS', 'GEO',  'HYDRO', 'NUC', 'OIL','SOL', 'WIND','emi']
    fullset=fun_flatten_list([fun_flatten_list(list(select_criteria_single_variable(countrylist, full_range, df_desired_dict, i)[0].values()), _unique=True ) for i in fuel_list], _unique=True)
    bycountry={i:select_criteria_single_variable(countrylist, full_range, df_desired_dict, i)[0] for i in fuel_list}
    return fullset, bycountry

def select_criteria_single_variable(countrylist, full_range, df_desired_dict, myvar):
    all_seeds = fun_select_max_range_across_all_countries(
        df_desired_dict, countrylist, myvar, full_range
    )
    # Contains min/max seeds for each country (just for information purposes)
    all_seeds_by_country = dict(zip(countrylist, all_seeds))

    # Select criteria (max/min) across all countries
    max_occ_seeds = fun_select_max_occurrency_in_list_of_tuples(1, all_seeds)
    return all_seeds_by_country,max_occ_seeds


def fun_read_cached_sensitivity(
    read_cached_sensitivity: pd.DataFrame, 
    range_random_weights: List[int]
) -> List[int]:
    """
    Reads cached sensitivity data and updates the random weights based on the
    available ranges from the cached results.

    This function evaluates the length of each sensitivity range, sorts them
    by length, and modifies the `range_random_weights` to exclude already
    evaluated ranges. It replaces them with selected minimum and maximum
    criteria from the cached data.

    Args:
        read_cached_sensitivity (pd.DataFrame): DataFrame containing sensitivity 
            ranges and selected min/max criteria.
        range_random_weights (List[int]): List of random weights to be filtered 
            and updated based on cached results.

    Returns:
        List[int]: Updated list of random weights after excluding cached ranges 
            and including selected min/max values.
    """
    read_cached_sensitivity["length"] = [
        eval(f"len({range(eval(x)[0], eval(x)[1])})")
        for x in read_cached_sensitivity.full_range
    ]

    read_cached_sensitivity = read_cached_sensitivity.sort_values(
        "length", ascending=False
    )
    available_res = []
    x_range_list_all = []

    for x in read_cached_sensitivity.full_range:
        # Available sensitivity range
        available_res = [list(range(eval(x)[0], eval(x)[1]))]
        # Check for intersections
        if (
            len(
                set(fun_flatten_list(available_res)).intersection(
                    set(fun_flatten_list(x_range_list_all))
                )
            )
            <= 1
        ):
            # 1) Remove those sensitivity range (cached results) from range_random_weights
            range_random_weights = [
                x
                for x in range_random_weights
                if x not in fun_flatten_list(available_res)
            ]

            # 2) Replace them with the optimal selected min/max criteria
            sel_min_max = [
                eval(i)
                for i in read_cached_sensitivity[
                    read_cached_sensitivity.full_range == x
                ].sel_min_max
            ]

            x_range_list_all += available_res
            range_random_weights += fun_flatten_list(sel_min_max)

    range_random_weights = unique(range_random_weights)
    range_random_weights.sort()
    return range_random_weights


def fun_read_cache_simple(
    read_cached_sensitivity: Optional[pd.DataFrame], 
    range_random_weights: List[int]
) -> List[int]:
    """
    Reads cached sensitivity data and updates random weights simply based on 
    the available ranges.

    If the cached sensitivity data is provided, it removes those ranges from 
    `range_random_weights` and adds selected min/max criteria based on already 
    evaluated simulations.

    Args:
        read_cached_sensitivity (Optional[pd.DataFrame]): DataFrame containing 
            cached sensitivity ranges and selected min/max criteria, or None.
        range_random_weights (List[int]): List of random weights to be filtered 
            and updated based on cached results.

    Returns:
        List[int]: Updated list of random weights after excluding cached ranges 
            and including selected min/max values.
    """
    if read_cached_sensitivity is not None:
        # Remove cached results from range_random_weights
        available_res = [
            list(range(eval(x)[0], eval(x)[1]))
            for x in read_cached_sensitivity.full_range
        ]
        available_res = fun_flatten_list(available_res)
        range_random_weights = [
            x for x in range_random_weights if x not in available_res
        ]

        # Add selected min/max criteria
        sel_min_max = [eval(x) for x in read_cached_sensitivity.sel_min_max]
        sel_min_max = fun_flatten_list(sel_min_max)
        range_random_weights = unique(range_random_weights + sel_min_max)
        range_random_weights.sort()

    return range_random_weights


def convert_string_to_list(string: str) -> list:
    """Separate all words contained in string (separated by a comma) and convert them to a list

    Parameters
    ----------
    string : str
        your initial string

    Example
    -------
        convert_string_to_list('this is an example') ->['this', 'is', 'an', 'example']

    Returns
    -------
    _type_
        List of words (separated by comma) contained in your initial string
    """

    if isinstance(string, str):
        li = list(string.split(" "))
        li = [l.replace(",", "") for l in li]
        return li
    return string


def fun_list_time_of_conv(conv_settings: dict, var: str = "Final") -> list:
    """Returns a list of time of convergence for 'Final', 'Secondary' and 'Primary' energy variables, based on 'conv_settings'

    Parameters
    ----------
    conv_settings : dict
        Convergence settings
    var : str, optional
        Variable e.g. ('Final', 'Secondary', 'Primary'), by default "Final"

    Returns
    -------
    range
        List of timing of convergerence
    """
    return list(conv_settings[var].values())


def fun_blending(
    _df_all: pd.DataFrame,
    sensitivity: bool,
    standard_range: range = None,
    secondary_primary_range: range = None,
    _from: str = "ENSHORT_REF",
    _to: str = "ENLONG_RATIO",
    _over: str = "TIME",
    iam_base_year: int = 2010,
) -> pd.DataFrame:  # blending function 2020_12_03
    from downscaler.fixtures import fun_conv_settings

    conv_settings = fun_conv_settings(sensitivity)  # we run it with sensitivity = False

    if standard_range is None:
        standard_range = fun_list_time_of_conv(conv_settings, var="Final")

    if secondary_primary_range is None:
        secondary_primary_range = fun_list_time_of_conv(conv_settings, var="Secondary")
    ## 1) We create a list of variables for blending (We check available variables for both ENSHORT and ENLONG).
    enlong_list = _df_all.iloc[:, _df_all.columns.str.endswith(_to)].columns
    enshort_list = _df_all.iloc[:, _df_all.columns.str.endswith(_from)].columns
    enlong_list = [x.replace(_to, "") for x in enlong_list]
    enshort_list = [x.replace(_from, "") for x in enshort_list]

    ## list of variables available for both enlong and enshort
    blend_list = set(enlong_list).intersection(set(enshort_list))

    # 2) Calculating Blended projections
    setindex(_df_all, False)

    # TO CALCULATE weights based on Regional gdpcap, you can do something alonge these lines
    if _over == "TIME":
        _df_all["TIME2"] = _df_all.TIME
        _over = "TIME2"

    iam_base_year_values = set(
        _df_all[_df_all.TIME == iam_base_year]
        .set_index(["ISO"])[_over]
        .to_dict()
        .values()
    )
    txt = "seems to differ across countries. For `_over` we need the same value for all countries to calculate"
    if not len(iam_base_year_values) == 1:
        raise ValueError(
            f"{_over}{txt} blended projections in line with regional IAMs results: we found {iam_base_year_values}"
        )
    iam_base_year = list(iam_base_year_values)[0]

    # iam_base_year=_df_all[_df_all.TIME==iam_base_year].set_index('ISO')[_over].iloc[0]

    for s in blend_list:
        # Define the yearx_range
        if s.find("Secondary") != -1 or s.find("Primary") != -1:
            yearx_range = secondary_primary_range
        else:
            yearx_range = standard_range
        for yearx in yearx_range:
            yearx_values = set(
                _df_all[_df_all.TIME == yearx]
                .set_index(["ISO"])[_over]
                .to_dict()
                .values()
            )
            if not len(iam_base_year_values) == 1:
                raise ValueError(
                    f"{_over}{txt} blended projections in line with regional IAMs results: we found {iam_base_year_values}"
                )
            if len(yearx_values) == 0:
                myval = _df_all.set_index("TIME")[_over].to_dict()
                yearxv = float(
                    fun_spline_interpolation(
                        yearx, list(myval.keys()), list(myval.values())
                    )
                )
            else:
                yearxv = list(yearx_values)[0]
            ## 2.1) We compute the Standard (Final Energy level) weights for the uncertainty range
            _df_all[_over]=[int(x) for x in _df_all[_over]]
            _df_all[str(yearx)] = (
                -1 / (yearxv - iam_base_year) * _df_all[_over]
                + (1 / (yearxv - iam_base_year) * iam_base_year + 1)
            ).clip(0, 1)
            ## 2.2.) We compute the uncertainty range based on OPTIMAL ratio from LEARNING (blended scenario from PINK line and ENLONG_RATIO)
            _df_all[s + str(yearx) + "_BLEND"] = _df_all[s + _from].fillna(0) * _df_all[
                str(yearx)
            ] + _df_all[s + _to].fillna(0) * (
                1 - _df_all[str(yearx)]
            )  ## Normal convergence
    return fun_pd_sel(_df_all, "", "")


def fun_read_config_file(config_file_path):
    df_config_scen = pd.read_csv(config_file_path)
    df_config_scen.columns = [x.upper() for x in df_config_scen.columns]
    for x in ["SSP", "CONVERGENCE"]:
        df_config_scen.loc[:, x] = [
            x.upper().replace("MEDIUM", "MED") for x in df_config_scen[x]
        ]
    if "MODEL" in df_config_scen.columns:
        df_config_scen = df_config_scen.drop("MODEL", axis=1)
    
    # Scan for duplicated entries in the config_scen file. If so, it throws an error
    scenarios= list(df_config_scen.SCENARIO)
    df=df_config_scen
    duplicated=[x for x in scenarios if not (len(df[df.SCENARIO==x].SSP.unique())==1 and len(df[df.SCENARIO==x].CONVERGENCE.unique())==1 )] 
    if len(duplicated):
        txt1='The `scenario_config.csv` file contains duplicated entries for '
        txt2='Please check your `scenario_config.csv` file: each scenario should have only one entry (only one row/value) associated to each scenario in the csv file. '
        txt3='All models should share the same assumptions.'
        raise ValueError(f'{txt1} {duplicated}. {txt2} {txt3}')
    try:
        df_config_scen = fun_drop_duplicates(
            df_config_scen.set_index("SCENARIO").dropna(how="all", axis=1)
        )
    except:
        df_config_scen = df_config_scen.drop_duplicates(keep="first").set_index(
            "SCENARIO"
        )
    res = {}
    ssp_list = [f"SSP{x}" for x in range(1, 6)]
    val_type = {"CONVERGENCE": ["SLOW", "MED", "FAST"], "SSP": ssp_list}

    for i, j in val_type.items():
        mydict = {x: list(df_config_scen[df_config_scen[i] == x].index) for x in j}
        res[i] = mydict

    conv_dict = {
        "slow": [],
        "med": [],
        "fast": [],
    }

    for x in conv_dict:
        if x.upper() in res["CONVERGENCE"]:
            conv_dict[x] = res["CONVERGENCE"][x.upper()]

    ssp_dict = res["SSP"]
    scen_dict = {x: sspscen for sspscen in ssp_list for x in ssp_dict[sspscen]}

    if "SENSITIVITY" in df_config_scen.columns:
        df_config_scen.loc[:, "SENSITIVITY"] = [
            convert_string_to_list(x) for x in df_config_scen.SENSITIVITY
        ]
        sensitivity_dict = (
            df_config_scen.loc[:, "SENSITIVITY"].dropna().to_dict() or None
        )
    else:
        sensitivity_dict = None
    return conv_dict, scen_dict, sensitivity_dict




def fun_run_analysis_all_mitigation_settings(
    top: int,
    read_ghg_from_csv: bool,
    input_dict: dict,
    input_spec: dict,
    default_mitigation: Union[str, int],
    ra: Union[str, int] = "standard",
    res_dir = CONSTANTS.CURR_RES_DIR('step5')/ "emissions_clock",
) -> pd.DataFrame:
    """This function run the top 20 mitigation analysis for all mitigation settings
     defined in `input_spec`. It returns a dataframe

    Parameters
    ----------
    top : int
        How many (e.g. top 20) measures shold be considered
    read_ghg_from_csv : bool
        Reading GHG emissions file from csv
    input_dict : dict
        Dictionary with input data
    input_spec : dict
        Dictionary with mitigation settings
    default_mitigation : Union[str, int]
        Default mitigation settings (e.g. 2010 -> calculates mitigation below 2010 data)

    Returns
    -------
    pd.DataFrame
        Dataframe with top mitigation measures

    Raises
    ------
    ValueError
        Raises error if `default_mitigation` is not found in `input_spec`
    """
    if default_mitigation not in input_spec:
        raise ValueError(
            f"Cannot find {default_mitigation} in your input_spec: {input_spec.keys()}"
        )

    df_ghg = None
    
    if read_ghg_from_csv:
        myidx = ["MODEL", "SCENARIO", "VARIABLE", "UNIT", "REGION"]
        file = "MESSAGEix-GLOBIOM 1.1-M-R12_all_ghg_emissions.csv"
        df_ghg = pd.read_csv(res_dir / file).set_index(myidx)
        df_ghg.columns = [int(x) for x in df_ghg.columns]

    # Run analysis for all mitigation settings (input_spec):
    res_all_dict = {
        i: fun_top_20_analysis(
            input_dict, _mit_vs_base_year=j, top=top, df_ghg=df_ghg, ra=ra
        )
        for i, j in input_spec.items()
    }

    fun_check_additional_long_term_measures(res_dir, res_all_dict)
    df_summary = fun_create_summary_table_and_save_to_csv(
        res_all_dict, res_dir, default_mitigation
    )

    print("top measures calculations done")
    return df_summary


def fun_index(_df: pd.DataFrame, _col: str) -> pd.DataFrame:
    return _df.index.get_level_values(_col)


def fun_multindex(_df: pd.DataFrame, _tuple: tuple) -> pd.DataFrame:
    """
    This function allows for accessing a df with multiindex (based on a list of tuples [('index,'val')] )

    """
    # POSSIBLE EXTENSION If _not=True it returns data that do not contain value
    for i in range(len(_tuple)):
        if type(_tuple[i][1]) in [range, np.ndarray]:
            #             print('this is range')
            _df = _df[fun_index(_df, _tuple[i][0]).isin(list(_tuple[i][1]))]
        if type(_tuple[i][1]) == list:
            #             print('this is list')
            _df = _df[fun_index(_df, _tuple[i][0]).isin(_tuple[i][1])]
        else:
            _df = _df[fun_index(_df, _tuple[i][0]) == _tuple[i][1]]
    return _df


def fun_create_variable_sum(
    _df: pd.DataFrame,
    _new_var_name: str,
    _models_list: list,
    _add_var_list: list,
    _subtract_var_list: list,
) -> pd.DataFrame:
    """
    This function creates a new variable in the dataframe by adding up and/or subtracting variables for a list of models. Created on 2021_05_25

    Parameters
    ==========
    _df: pd.DataFrame
        CSV File name with both downscaled IAMs results (and native country results).
    _new_var_name: str
        The name of the new variable that we want to create
    _models_list: list
        List of models for which we want to create a new variable (note:  model contains only native countries, model+"_downscaled" will contain only downscaled results)
    _add_var_list: list
        We sum up all variables contained in this list
    _subtract_var_list: list
        We sum up all variables contained in this list and subtract  from the previous sum (_add_var_list)

    Returns
    =======
    pd.DataFrame
        Contains the dataframe with the new variable that we have created.
    """
    # _df = df
    setindex(_df, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    _df2 = pd.DataFrame()
    if len(_subtract_var_list) == 0 or _subtract_var_list == [""]:
        ## We sum all variables contained in the list _add_var_list  (_subtract_var_list is empty, therefore we do not subtract anything)
        _df2 = (
            fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    ("VARIABLE", _add_var_list),
                ],
            )
            .droplevel("VARIABLE")
            .groupby(["MODEL", "SCENARIO", "REGION", "UNIT"])
            .sum()
        )

    else:
        ## We do the same as above and then we subtract the sum of variables contained in _subtract_var_list
        _df2 = fun_multindex(
            _df,
            [
                ("MODEL", _models_list),  ## Selecting only native regions
                ("VARIABLE", _add_var_list),
            ],
        ).droplevel("VARIABLE").groupby(["MODEL", "SCENARIO", "REGION", "UNIT"]).sum(
        ) - (
            fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    ("VARIABLE", _subtract_var_list),
                ],
            )
            .droplevel("VARIABLE")
            .groupby(["MODEL", "SCENARIO", "REGION", "UNIT"])
            .sum()
        )

    _df2["VARIABLE"] = _new_var_name

    setindex(_df2, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    return pd.concat([_df,_df2])
    # return _df.append(_df2)


def fun_create_variable_ratio(
    _df: pd.DataFrame,
    _new_var_name: str,
    _models_list: list,
    _num_var_list: list,
    _den_var_list: list,
    _iso_list: list,
    _den_sum_all_countries: bool,
    _unit: str = "",
) -> pd.DataFrame:
    """
    This function creates a new variable in the dataframe as the ratio of existing variables. Created on 2021_05_25

    Parameters
    ==========
    _df: pd.DataFrame
        CSV File name with both downscaled IAMs results (and native country results).
    _new_var_name: str
        The name of the new variable that we want to create
    _models_list: list
        List of models for which we want to create a new variable (note:  model contains only native countries, model+"_downscaled" will contain only downscaled results)
    _iso_list: list
        List of countries for which we want to create a new variable
    _num_var_list: list
        Numerator of our new variable
    _den_var_list: list
        Denominator of our new variable
    _den_sum_all_countries:bool
        If True we sum up all country-level results in the denominator variable (for example to calculate share of country within region)

    Returns
    =======
    pd.DataFrame
        Contains the dataframe with the new variable that we have created.
    """
    # _df = df
    setindex(_df, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    _df2 = pd.DataFrame()
    if len(_iso_list) == 0:  ## Ratio
        if _den_sum_all_countries == False:
            _df2 = fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    ("VARIABLE", _num_var_list),
                ],
            ).droplevel("VARIABLE") / fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    ("VARIABLE", _den_var_list),
                ],
            ).droplevel(
                "VARIABLE"
            )
        else:
            print(
                "CAREFUL: Apply ratios to all countries, maybe from different regions. Please consider providing an _iso_list as input to this function"
            )

            #             print((fun_multindex(df,[('MODEL',_models_list), ## Selecting only native regions
            #                                   ('VARIABLE',_den_var_list)]).groupby(['MODEL','SCENARIO']).sum(axis=0)))

            _df2 = fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    ("VARIABLE", _num_var_list),
                ],
            ).droplevel("VARIABLE") / (
                1e-20
                + fun_multindex(
                    _df,
                    [
                        ("MODEL", _models_list),  ## Selecting only native regions
                        ("VARIABLE", _den_var_list),
                    ],
                )
                .groupby(["MODEL", "SCENARIO"])
                .sum(axis=0)
            )

    else:
        if _den_sum_all_countries == False:
            _df2 = fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    (
                        "REGION",
                        _iso_list,
                    ),  ## List of countries for which we want to create our new variable
                    ("VARIABLE", _num_var_list),
                ],
            ).droplevel("VARIABLE") / fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    ("REGION", _iso_list),
                    ("VARIABLE", _den_var_list),
                ],
            ).droplevel(
                "VARIABLE"
            )
        else:
            _df2 = fun_multindex(
                _df,
                [
                    ("MODEL", _models_list),  ## Selecting only native regions
                    (
                        "REGION",
                        _iso_list,
                    ),  ## List of countries for which we want to create our new variable
                    ("VARIABLE", _num_var_list),
                ],
            ).droplevel("VARIABLE") / (
                1e-20
                + fun_multindex(
                    _df,
                    [
                        ("MODEL", _models_list),  ## Selecting only native regions
                        (
                            "REGION",
                            _iso_list,
                        ),  ## List of countries for which we want to create our new variable
                        ("VARIABLE", _den_var_list),
                    ],
                )
                .groupby(["MODEL", "SCENARIO"])
                .sum()
            )

    _df2["VARIABLE"] = _new_var_name

    if _unit != "":
        _df2 = _df2.droplevel("UNIT")
        _df2["UNIT"] = _unit

    setindex(_df2, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    return pd.concat([_df,_df2])
    # return _df.append(_df2)


def fun_create_variable_multiply(
    _df: pd.DataFrame,
    _new_var_name: str,
    _models_list: list,
    _num_var_list: list,
    _den_var_list: list,
) -> pd.DataFrame:
    """
    This function creates a new variable in the dataframe as the ratio of existing variables. Created on 2021_05_25

    Parameters
    ==========
    _df: pd.DataFrame
        CSV File name with both downscaled IAMs results (and native country results).
    _new_var_name: str
        The name of the new variable that we want to create
    _models_list: list
        List of models for which we want to create a new variable (note:  model contains only native countries, model+"_downscaled" will contain only downscaled results)
    _num_var_list: list
        Numerator of our new variable
    _den_var_list: list
        Denominator of our new variable

    Returns
    =======
    pd.DataFrame
        Contains the dataframe with the new variable that we have created.
    """
    df = _df
    # _df = df
    setindex(_df, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    _df2 = pd.DataFrame()

    _df2 = fun_multindex(
        df,
        [
            ("MODEL", _models_list),  ## Selecting only native regions
            ("VARIABLE", _num_var_list),
        ],
    ).droplevel("VARIABLE") * fun_multindex(
        df,
        [
            ("MODEL", _models_list),  ## Selecting only native regions
            ("VARIABLE", _den_var_list),
        ],
    ).droplevel(
        "VARIABLE"
    )

    _df2["VARIABLE"] = _new_var_name

    setindex(_df2, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    return pd.concat([_df,_df2])
    # return _df.append(_df2)


def fun_native_countries(model, df_countries, regions):
    native_countries = [
        df_countries[df_countries.REGION == r.replace(f"{model}|", "") + "r"].ISO[0]
        for r in regions
        if len(
            df_countries[df_countries.REGION == r.replace(f"{model}|", "") + "r"].ISO
        )
        == 1
    ]

    return native_countries


def fun_get_csv_path_suf(
    csv_str, step4, run_sensitivity, sensitivity_suffix, PREV_STEP_RES_DIR, model
):
    suf = sensitivity_suffix if run_sensitivity else ""
    if step4:
        suf = f"{suf}_step4_FINAL"
        my_path = PREV_STEP_RES_DIR
    else:
        my_path = CONSTANTS.PREV_RES_DIR(
            str(Path(os.path.abspath("")) / "Policies_emissions_4.py")
        )
        print("read results from step3")
    csv_suffix = f"{model}_{csv_str}{suf}.csv"
    return csv_suffix, suf, my_path


def fun_get_target_regions_step5(pyam_mapping_file, selection_dict, model):
    targets = [i for i in selection_dict[model]["targets"]]

    df_countries = fun_read_df_countries(
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv",
    )
    df_countries, regions = load_model_mapping(
        model, df_countries, pyam_mapping_file.file
    )

    regions = [i for i in selection_dict[model]["regions"]]
    return targets, df_countries, regions


def fun_create_df_test_all(selection_dict):
    test_variables = [
        "Final Energy",
        "GDP|PPP",
        "Population",
        "Emissions|CO2|Energy",
        "Price|Carbon",
    ]
    models = list(selection_dict.keys())
    df_test_all = pd.DataFrame(index=models, columns=test_variables)
    return test_variables, df_test_all


def fun_add_gdp_step5(csv_suffix, model):
    df_gdp = pd.read_csv(csv_suffix, sep=",")

    ## We keep only ISO codes with len==3 (3 ISO CODES DIGITS)
    df_gdp = df_gdp[df_gdp.ISO.isin([i for i in df_gdp.ISO.unique() if len(i) == 3])]

    setindex(df_gdp, ["MODEL", "SCENARIO", "ISO", "VARIABLE", "UNIT"])

    df_gdp=fun_drop_columns(df_gdp, case_sensitive=False, col_names=["unnamed"])
    # if "Unnamed: 0" in df_gdp.columns:
    #     df_gdp = df_gdp.drop("Unnamed: 0", axis=1)
    fun_multindex(df_gdp, [("MODEL", model)])
    return df_gdp


def fun_create_input_data_step5(
    project_name,
    add_model_in_region_name,
    region_patterns,
    model_patterns,
    target_patterns,
    add_twn,
    get_selection_dict,
):
    input_file = CONSTANTS.INPUT_DATA_DIR / project_name / "snapshot_all_regions.csv"
    pyam_mapping_file = CONSTANTS.INPUT_DATA_DIR / project_name / "default_mapping.csv"
    iam_results_file = InputFile(
        CONSTANTS.INPUT_DATA_DIR / project_name / "snapshot_all_regions.csv"
    )
    pyam_mapping_file = InputFile(
        CONSTANTS.INPUT_DATA_DIR / project_name / "default_mapping.csv"
    )
    region_patterns = convert_to_list(region_patterns)
    model_patterns = convert_to_list(model_patterns)
    target_patterns = convert_to_list(target_patterns)

    # ## STEP 1 READ Regional iam results and AJUSTING REGION NAME
    df_iam_all_models = fun_read_df_iam_all(
        file=InputFile(input_file), add_model_name_to_region=add_model_in_region_name
    )  ## READING IN IAMS RESULTS

    RESULTS_DATA_DIR = CONSTANTS.CURR_RES_DIR(
        str(Path(os.path.abspath("")) / "Step_5_Scenario_explorer_5.py")
    )
    PREV_STEP_RES_DIR = CONSTANTS.PREV_RES_DIR(
        str(Path(os.path.abspath("")) / "Step_5_Scenario_explorer_5.py")
    )

    df_ssp = fun_add_twn_ssp_projections(add_twn) if add_twn else None
    PREV_STEP_RES_DIR = CONSTANTS.PREV_RES_DIR(
        str(Path(os.path.abspath("")) / "Step_5_Scenario_explorer_5.py")
    )

    try:
        selection_dict = get_selection_dict(
            iam_results_file,
            model=model_patterns,
            region=region_patterns,
            target=target_patterns,
            variable=["*"],
        )
    except ValueError as e:
        raise ValueError(f"{e}\nConsider using the asterisk '*' for the pattern.")

    ## slice df based on selection_dict
    df_iam_all_models = fun_sel_iam(selection_dict, df_iam_all_models)
    df_iam_all_models.loc[:, "REGION"] = df_iam_all_models.loc[:, "REGION"] + "r"
    return (
        region_patterns,
        model_patterns,
        target_patterns,
        input_file,
        pyam_mapping_file,
        df_iam_all_models,
        RESULTS_DATA_DIR,
        PREV_STEP_RES_DIR,
        df_ssp,
        selection_dict,
    )


def fun_add_twn_ssp_projections():
    ## Reading ssp data
    df_ssp = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "SspDb_country_data_2013-06-12.csv",
        sep=",",
        encoding="utf-8",
    )
    df_ssp.rename(columns={"REGION": "ISO"}, inplace=True)
    return df_ssp


def fun_create_new_var_dict_step5(fuel_list, new_var_dict, sectors_list):
    var_dict_supply = {
        "Primary Energy": fun_make_var_dict(
            fuel_list, sectors_list, demand_dict=False, main_level="Primary Energy"
        )["Primary Energy"]
    }

    levels = [
        "Secondary Energy|Electricity",
        "Secondary Energy|Liquids",
        "Secondary Energy|Gases",
    ]
    for level in levels:
        fuel_list = [x.replace("Non-Biomass Renewables|", "") for x in fuel_list]
        var_dict_supply[level] = fun_make_var_dict(
            fuel_list, sectors_list, demand_dict=False, main_level=level
        )[level]

    new_var_dict.update(var_dict_supply)
    return new_var_dict


def fun_select_criteria(df: pd.DataFrame, ra: Union[str, int]) -> pd.DataFrame:
    """Select a single criteria `ra` from your `df`
    Parameters
    ----------
    df : pd.DataFrame
        Initial dataframe with multiple criteria
    ra : Union[str, int]
        your selected criteria
    Returns
    -------
    pd.DataFrame
        Sliced dataframe  with selected criteria
    """
    if "CRITERIA" in df.index.names:
        df = df.xs(ra, level="CRITERIA")
    elif "CRITERIA" in df.columns:
        df = df[df.CRITERIA == ra].drop("CRITERIA", axis=1)
    return fun_drop_duplicates(df)


def fun_add_criteria(ra: Union[str, int], df: pd.DataFrame):
    """Add criteria wight to the index of the dataframe
    Parameters
    ----------
    ra : Union[str, int]
        Selected criteria
    df : pd.DataFrame
        dataframe
    Returns
    -------
    _type_
        dataframe with criteria as index
    """
    if "CRITERIA" not in df.columns and "CRITERIA" not in df.index.names:
        df["CRITERIA"] = ra
    if "CRITERIA" not in df.index.names:
        df = df.set_index("CRITERIA", append=True)
    return df


def fun_add_non_biomass_ren_nomenclature(df):
    primary_ren = [
        "Geothermal",
        "Hydro",
        "Solar",
        "Wind",
    ]
    iam_nomenclature_dict = {
        f"Primary Energy|{x}": f"Primary Energy|Non-Biomass Renewables|{x}"
        for x in primary_ren
    }
    df = df.rename(index=iam_nomenclature_dict)
    return df, primary_ren


def fun_explorer_file(
    model: str,
    df: pd.DataFrame,
    regions: list,
    df_countries,
    iso_col_name: str = "REGION",
):
    native_countries, countries_wo_native = fun_native_non_native_countries(
        model, df_countries, regions
    )
    iso_dict = {val: f"D.{str(val)}" for val in countries_wo_native}
    df = df.reset_index()
    df[iso_col_name] = [iso_dict.get(i, i) for i in df[iso_col_name]]
    df["MODEL"] = model
    return df.set_index(["MODEL", "SCENARIO", iso_col_name, "VARIABLE", "UNIT"])


def fun_native_non_native_countries(model, df_countries, regions):
    native_countries = fun_native_countries(model, df_countries, regions)

    ## List of non-native countries
    countries_wo_native = list(set(df_countries.ISO.unique()) - set(native_countries))

    return native_countries, countries_wo_native


def fun_downloadable_file(
    model: str,
    df: pd.DataFrame,
    regions: list,
    df_countries,
    iso_col_name: str = "REGION",
) -> Union[pd.DataFrame, dict]:
    native_countries, countries_wo_native = fun_native_non_native_countries(
        model, df_countries, regions
    )
    df = df.reset_index()
    df.rename(columns={"ISO": iso_col_name}, inplace=True)  ## Renaming ISO to region
    model_dict = {
        x: f"{model}_downscaled"
        for x in countries_wo_native
        if "_downscaled" not in model
    }
    ## Renaminig model name
    df["MODEL"] = [model_dict.get(i, model) for i in df.REGION]
    df[iso_col_name] = [x.replace("D.", "") for x in df[iso_col_name]]
    df = df.set_index(["MODEL", "SCENARIO", iso_col_name, "VARIABLE", "UNIT"])
    return df, model_dict


def fun_exclude_variables_from_df(
    df: pd.DataFrame, list_of_var: list, level: str
) -> pd.DataFrame:
    """Exclude variables from `df` that are listed in `list_of_var` in `level`.
    NOTE: function excludes variables starting with the variable names listed in `list_of_var`

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe
    list_of_var : list
        List of variables to be excluded
    level : str
        level where to check for variable

    Returns
    -------
    pd.DataFrame
        Dataframe excluding selected variables
    """
    for x in list_of_var:
        df = df[~df.index.get_level_values(level).str.startswith(x)]

    return df


def fun_make_step1_input(
    conv_range,
    list_of_regions,
    list_of_targets,
    ref_target,
    file_suffix,
    model_in_region_name,
    input_file,
    pyam_mapping_file,
    selection_dict,
    n_sectors,
    df_iam_all,
    df_iea_h,
    fun_regions_model,
    default_ssp_scenario,
    split_res_and_comm,
    scen_dict,
):  # sourcery skip: raise-specific-error
    if model_in_region_name:
        # print("RAM memory % used IN INPUT LIST:", psutil.virtual_memory()[2])
        input_list = [
            {
                "input_file": input_file,
                "model_patterns": m,
                "region_patterns": f"{r}*",
                "convergence": conv,
                "file_suffix": file_suffix,
                "n_sectors": n_sectors,
                "pyam_mapping_file": pyam_mapping_file,
                "ref_target": ref_target,
                "target_patterns": list_of_targets,
                "regions_input": [f"{r}r"],
                "df_iam_all_input": df_iam_all[
                    (df_iam_all.MODEL == m)
                    & (
                        df_iam_all.REGION.isin(
                            fun_regions_model(
                                m,
                                pyam_mapping_file,
                                list_of_regions,
                                model_in_region_name,
                            )
                        )
                    )
                ],
                "df_iea_h": None,
                "ssp_scenario": default_ssp_scenario,
                "split_res_and_comm": split_res_and_comm,
                "scen_dict": scen_dict,
            }
            for m, sel in selection_dict.items()
            for r in sel["regions"]
            for conv in conv_range
            if r.find(m) != -1
        ]  ## This line excludes R5 regions (e.g. OECD)
        # print("RAM memory % used IN INPUT LIST:", psutil.virtual_memory()[2])

    else:  ## the only difference with the above is if r.find(m)!=-1
        # print("RAM memory % used IN INPUT LIST:", psutil.virtual_memory()[2])
        input_list = [
            {
                "input_file": input_file,
                "model_patterns": m,
                "region_patterns": f"{r.rsplit('|')[1]}*",
                "convergence": conv,
                "file_suffix": file_suffix,
                "n_sectors": n_sectors,
                "pyam_mapping_file": pyam_mapping_file,
                "ref_target": ref_target,
                "target_patterns": list_of_targets,
                "regions_input": [f"{r}r"],
                "df_iam_all_input": df_iam_all[
                    (df_iam_all.MODEL == m)
                    & (df_iam_all.REGION == r.rsplit("|")[1] + "r")
                ],
                "df_iea_h": None,
                "ssp_scenario": default_ssp_scenario,
                "split_res_and_comm": split_res_and_comm,
                "scen_dict": scen_dict,
            }
            for m, sel in selection_dict.items()
            for r in sel["regions"]
            for conv in conv_range
        ]  ## This line excludes R5 regions (e.g. OECD)
        # print("RAM memory % used IN INPUT LIST:", psutil.virtual_memory()[2])

    if len(input_list[0]["df_iam_all_input"]) == 0:
        raise Exception(
            "Dataframe df_iam_all_input is empty, please check your input_list",
        )
    df = input_list[0]["df_iam_all_input"]
    check_var_list = [
        "GDP|PPP",
        "Population",
    ]  ## we check if dataframe contains these variables
    for check_var in check_var_list:
        if len(df[df.VARIABLE == check_var]) == 0:
            raise Exception(
                f"Dataframe df_iam_all_input does not contain {check_var}, please check your input_list",
            )
    if input_list[0]["df_iea_h"] is not None:
        print(
            "This is the countrylist with historical data (index of df_iea_h):",
            df_iea_h.index.unique(),
        )
        if len(input_list[0]["df_iea_h"]) == 0:
            raise Exception(
                "Dataframe df_iea_h is an empty dataframe, please check your input_list",
            )

    return input_list


def fun_print_missing_or_duplicated_var(df):
    if (len(df.iloc[df.index.duplicated()])) != 0:
        print("##### WARNING - THERE ARE STILL DUPLICATED VALUES IN THE DF")
        ## Check if we accidentally dropped all gdp/population data:
    if (
        len(
            fun_multindex(
                df,
                [
                    ("VARIABLE", "Population"),
                ],
            )
        )
        == 0
    ):
        print("##### WARNING - Population not included in the DF")
    if (
        len(
            fun_multindex(
                df,
                [
                    ("VARIABLE", "GDP|PPP"),
                ],
            )
        )
        == 0
    ):
        print("##### WARNING - GDP not included in the DF")


def fun_append_gdp(model, df_gdp, df, countries_wo_native):
    df_append=fun_multindex(df_gdp, [("MODEL", model), ("ISO", countries_wo_native)])
    df= pd.concat([df, df_append])
    # df = df.append(
    #     df_append
    # )

    df_gdp.rename(columns={"ISO": "REGION"}, inplace=True)
    return df


def fun_non_co2_data_single_gas_region_time(
    df_non_co2: pd.DataFrame, var: str, countrylist: list, t: int
) -> pd.DataFrame:
    """Slices non-co2 data for a single sector, indutry, region and add regional data (sum across countries) in a new column 'reg_prod'.
    It returns a dataframe.

    Parameters
    ----------
    df_non_co2 : pd.DataFrame
        Dataframe with non-co2 emissions from GAINS
    var : str
        Selected GHG sector e.g. 'Energy'
    countrylist : list
        List of countries within a region e.g. ['AUS', 'JPN', 'NZL']
    t : int
        Selected time (e.g. 2050)

    Returns
    -------
    pd.DataFrame
        Sliced dataframe for a single sector, indutry, region. Dataframe includes regional data (sum across countries) in a new column 'reg_prod'.
    """
    df_non_co2 = df_non_co2.xs(var, level="VARIABLE")

    df_non_co2 = df_non_co2[
        df_non_co2.index.get_level_values("REGION").isin(countrylist)
    ]
    res_by_target = {}
    targets = df_non_co2.reset_index()["SCENARIO"].unique()
    for scen in targets:
        res_by_target[scen] = pd.DataFrame(
            df_non_co2.xs(scen, level="SCENARIO", drop_level=False)[t]
        )
        groupby_idx = [x for x in res_by_target[scen].index.names if x != "REGION"]
        res_by_target[scen]["reg_prod"] = (
            res_by_target[scen].groupby(groupby_idx).sum()[t][0]
        )
    if len(res_by_target):
        return pd.concat([res_by_target[scen] for scen in targets])
    return pd.DataFrame()


def fun_print_missing_or_duplicated_var(df):
    if (len(df.iloc[df.index.duplicated()])) != 0:
        print("##### WARNING - THERE ARE STILL DUPLICATED VALUES IN THE DF")
        ## Check if we accidentally dropped all gdp/population data:
    if (
        len(
            fun_multindex(
                df,
                [
                    ("VARIABLE", "Population"),
                ],
            )
        )
        == 0
    ):
        print("##### WARNING - Population not included in the DF")
    if (
        len(
            fun_multindex(
                df,
                [
                    ("VARIABLE", "GDP|PPP"),
                ],
            )
        )
        == 0
    ):
        print("##### WARNING - GDP not included in the DF")


def fun_append_gdp(model, df_gdp, df, countries_wo_native):
    df_append=fun_multindex(df_gdp, [("MODEL", model), ("ISO", countries_wo_native)])
    df= pd.concat([df, df_append])
    # df = df.append(
    #     fun_multindex(df_gdp, [("MODEL", model), ("ISO", countries_wo_native)])
    # )

    df_gdp.rename(columns={"ISO": "REGION"}, inplace=True)
    return df


def fun_non_co2_data_single_gas_region_time(
    df_non_co2: pd.DataFrame, var: str, countrylist: list, t: int
) -> pd.DataFrame:
    """Slices non-co2 data for a single sector, indutry, region and add regional data (sum across countries) in a new column 'reg_prod'.
    It returns a dataframe.

    Parameters
    ----------
    df_non_co2 : pd.DataFrame
        Dataframe with non-co2 emissions from GAINS
    var : str
        Selected GHG sector e.g. 'Energy'
    countrylist : list
        List of countries within a region e.g. ['AUS', 'JPN', 'NZL']
    t : int
        Selected time (e.g. 2050)

    Returns
    -------
    pd.DataFrame
        Sliced dataframe for a single sector, indutry, region. Dataframe includes regional data (sum across countries) in a new column 'reg_prod'.
    """
    df_non_co2 = df_non_co2.xs(var, level="VARIABLE")

    df_non_co2 = df_non_co2[
        df_non_co2.index.get_level_values("REGION").isin(countrylist)
    ]
    res_by_target = {}
    targets = df_non_co2.reset_index()["SCENARIO"].unique()
    for scen in targets:
        res_by_target[scen] = pd.DataFrame(
            df_non_co2.xs(scen, level="SCENARIO", drop_level=False)[t]
        )
        groupby_idx = [x for x in res_by_target[scen].index.names if x != "REGION"]
        res_by_target[scen]["reg_prod"] = (
            res_by_target[scen].groupby(groupby_idx).sum()[t][0]
        )
    if len(res_by_target):
        return pd.concat([res_by_target[scen] for scen in targets])
    return pd.DataFrame()


def fun_spline_interpolation(x, x_points, y_points, k=1):
    len_x = len(x_points)
    if len_x < k + 1:
        raise ValueError(
            f"You need to provide at least {k+1} `x_points`. You have provided {len_x} `x_points`: {x_points}."
        )
    tck = interpolate.splrep(x_points, y_points, k=k)
    return interpolate.splev(x, tck)


def fun_spline_interpolation_within_range(
    x: float, x_points: list, y_points: list
) -> float:
    """Spline interpolation within the min/max range defined by `y_points`
    Parameters
    ----------
    x : float
        Your x value
    x_points : list
        x Points
    y_points : list
        y Points
    Returns
    -------
    float
        Interpolated value
    """

    myval = float(
        fun_spline_interpolation(
            np.minimum(max(x_points), np.maximum(min(x_points), x)), x_points, y_points
        )
    )
    if myval is np.nan:
        myval = 0
    return np.minimum(max(y_points), np.maximum(min(y_points), myval))


def fun_read_dat_file(file: str) -> pd.DataFrame:
    """
    Reads a semicolon-separated data file into a DataFrame.

    The first column is renamed to "share" which represents the share of total resources. 
    The last column of the DataFrame, which contains global values, is not returned.

    Args:
        file (str): Path to the data file to be read.

    Returns:
        pd.DataFrame: A DataFrame with the first column set as the index and the last column removed.
    """
    df = pd.read_table(file, sep=";")
    cols = list(df.columns)
    cols[0] = "share"  # of total resources
    df.columns = [x.rsplit("=")[-1] for x in cols]
    # We do not return the last column as it contains global values
    return df.iloc[:, :-1].set_index("share")


def fun_get_interpolated_cost(
    x: float,
    df_cost: pd.DataFrame,
    countrylist: List[str]
) -> Dict[str, Any]:
    """
    Get interpolated cost data for a given level of resource usage.

    The function calculates the cost for each country based on the provided resource value 
    (ranging from 0 to 1, where 1 means 100% of resources are used).

    Args:
        x (float): The level of resource usage, ranging from 0 to 1.
        df_cost (pd.DataFrame): A DataFrame with supply cost curves where resources are expressed as percentages between 0 and 1.
        countrylist (List[str]): A list of countries for which to retrieve the cost data.

    Returns:
        Dict[str, Any]: A dictionary mapping country ISO codes to their respective cost data. 
                         If a country is not present in the cost DataFrame, its value will be NaN.
    """
    return {
        c: fun_spline_interpolation_within_range(x, df_cost.index, df_cost[c])
        if c in df_cost.columns
        else np.nan
        for c in countrylist
    }



def fun_get_interpolated_resources(
    x: float, 
    df_cost: pd.DataFrame, 
    countrylist: List[str]
) -> Dict[str, float]:
    """Get cumulative production (ranging from 0 to 1, where 1 means 100% of total resources) 
    associated with a given cost.

    This function takes a cost value and retrieves the cumulative production for each country 
    in the specified list based on supply cost curves.

    Parameters
    ----------
    x : float
        Your cost value (unit = $/GJ or $/Kwh).
        
    df_cost : pd.DataFrame
        DataFrame with supply cost curves. Resources are represented as percentages between 0 and 1.
        
    countrylist : list[str]
        List of countries for which to obtain the cumulative production associated with the given cost.

    Returns
    -------
    dict[str, float]
        A dictionary where keys are country ISO codes and values are the cumulative production 
        associated with the given cost. Returns NaN for countries without data.
    """
    my_dict = {}
    for c in countrylist:
        if c in df_cost.columns:
            # Switch index/columns
            df = (
                df_cost[c]
                .dropna(how="all")
                .drop_duplicates()
                .reset_index()
                .set_index(c)
            )
            if len(df) > 0:
                try:
                    my_dict[c] = fun_spline_interpolation_within_range(
                        x, df.index, df["share"]
                    )
                except Exception:
                    my_dict[c] = np.nan
            else:
                my_dict[c] = np.nan
        else:
            my_dict[c] = np.nan
    return my_dict


def fun_calculate_cum_prod(df_iam_sel: pd.DataFrame) -> pd.DataFrame:
    """Calculate cumulative production based on a DataFrame of annual production values.

    This function takes a DataFrame of annual production data, fills in any missing years, 
    and calculates the cumulative production over time.

    Parameters
    ----------
    df_iam_sel : pd.DataFrame
        A DataFrame with annual production data indexed by year.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing the cumulative production values, indexed by year, with 
        years not present in the input DataFrame interpolated and filled.
    """
    min_year = df_iam_sel.index.min()
    max_year = df_iam_sel.index.max()
    
    if isinstance(df_iam_sel, pd.DataFrame):
        df_annual = pd.concat(
            [
                df_iam_sel,
                pd.DataFrame(
                    index=[
                        x
                        for x in range(min_year, max_year + 1)
                        if x not in df_iam_sel.index
                    ],
                    columns=df_iam_sel.columns,
                ),
            ],
            axis=1,
        )
    else:
        df_annual = pd.concat(
            [
                df_iam_sel,
                pd.DataFrame(
                    index=[
                        x
                        for x in range(min_year, max_year + 1)
                        if x not in df_iam_sel.index
                    ],
                    columns=[df_iam_sel.name],
                ),
            ],
            axis=1,
        )
    
    df_annual = df_annual.interpolate(axis=0)
    return df_annual.cumsum().loc[df_iam_sel.index].dropna(how="all", axis=1)


# Blueprint for cost curves
max_prod_res_unit = {
    "BIO": 1e-9,  #'gj/yr'
    "WIND": 3.6e-12,  # 'kwh/yr',
    "SOL": 3.6e-12,  # 'kwh/yr',
    "HYDRO": 3.6e-12,  # 'kwh/yr',
}

def fun_cost_curve_downs_using_regional_mc(
    countrylist: List[str],
    df_iam_sel: pd.DataFrame
) -> pd.DataFrame:
    """
    Calculates the downscaled cost curves for various renewable energy sources 
    using regional marginal costs.

    This function computes the regional marginal costs (MC) for different renewable 
    energy sources based on the selected IAM data and merges the costs with 
    production capabilities of countries to derive annual production estimates.

    Args:
        countrylist (List[str]): A list of country ISO codes for which 
                                  the production and cost calculations are made.
        df_iam_sel (pd.DataFrame): Dataframe containing selected IAM data indexed 
                                    by time for different energy sources.

    Returns:
        pd.DataFrame: A DataFrame containing downscaled production estimates for 
                       each energy source at the country level, indexed by 
                       time and country ISO codes.
    """
    cost_curve_dict = {
        "SOL": "_PV",
        "BIO": "_1stGen_Bio",
        "WIND": "_onshore_Wind",
        "HYDRO": "HYD_lpjml_gfdl-esm2m_ewembi_rcp26_rcp26soc_co2_qtot_global_daily_2031_2070_merge_monmean_Hydro",
    }
    res_dict: Dict[str, pd.Series] = {}
    
    for i, j in cost_curve_dict.items():
        # Read cost and maximum production data from files
        df_cost = fun_read_dat_file(f"{CONSTANTS.INPUT_DATA_DIR}/CostCurve{j}.dat")
        df_res = fun_read_dat_file(f"{CONSTANTS.INPUT_DATA_DIR}/MaxProd{j}.dat")

        # Identify relevant columns for the countries
        mycols = [c for c in countrylist if c in df_res.columns]

        # Calculate regional cost data (weighted average across countries based on RES availability)
        res_list = []
        for t in df_iam_sel.index:
            # Calculate the share of resources in the region
            reg_perc_share = (
                df_iam_sel.loc[t, i]
                / (df_res[mycols] * max_prod_res_unit[i]).sum(axis=1)[0]
            )
            # Get country-level marginal costs
            country_mc = fun_get_interpolated_cost(
                reg_perc_share, df_cost, countrylist
            )

            # Calculate regional marginal cost as a weighted average
            reg_mc = sum(
                [
                    float(df_res[c] / ((df_res[mycols].sum(axis=1))[0]) * country_mc[c])
                    for c in mycols
                ]
            )
            # Get production at the country level based on regional marginal costs
            country_prod = fun_get_interpolated_resources(reg_mc, df_cost, countrylist)

            res_list.append(country_prod)

        # Create a DataFrame for production results
        df_prod = pd.DataFrame(res_list, index=df_iam_sel.index)
        
        # Adjust production using the ratio of IAM selected data
        ratio = df_iam_sel[i] / df_prod.sum(axis=1)
        df_prod_adj = pd.concat([df_prod[c] * ratio for c in mycols], axis=1)
        df_prod_adj.columns = mycols
        
        # Store the results in the dictionary
        res_dict[i] = df_prod_adj.stack()

    return pd.DataFrame(res_dict)


# Resources data at the country level
def fun_get_country_prod_from_reg_prod(
    x: float, df_cost: pd.DataFrame, countrylist: list, x_col="reg_prod", y_col="prod"
):
    df_cost = df_cost.sort_values("reg_prod")
    reg_val = x
    # Get regional value within min/max range of the df_cost:
    # reg_val=x.clip(df_cost[x_col].min(),df_cost[x_col].max())
    my_dict = {}
    for c in countrylist:
        # switch index/columns
        df = df_cost[df_cost.ISO == c].dropna(how="all").drop_duplicates().reset_index()
        if len(df) > 0:
            if len(df[y_col].unique()) > 1:
                try:
                    my_dict[c] = fun_spline_interpolation_within_range(
                        reg_val, df[x_col], df[y_col]
                    )
                except:
                    my_dict[c] = np.nan
            else:
                my_dict[c] = df[y_col].unique()[0]

        else:
            my_dict[c] = np.nan
    return my_dict


def fun_convert_ghg_df_in_iamc(model, target, gas, unit, country_prod_harmo):
    country_prod_harmo.index.names = ["REGION"]
    country_prod_harmo["MODEL"] = model
    country_prod_harmo["TARGET"] = target
    country_prod_harmo["VARIABLE"] = gas
    # if not reg_val:
    #     country_prod_harmo["UNIT"] ='not available from iams scenario'
    # else:
    country_prod_harmo["UNIT"] = unit
    iamc_index = ["MODEL", "TARGET", "REGION", "VARIABLE", "UNIT"]
    return country_prod_harmo.reset_index().set_index(iamc_index)


def fun_diffusion_constraints(df: pd.DataFrame, dt: int = 5, scalar: int = 2):
    """Impose max/min diffusion constraints based on previous period data multiplied by a scalar.
    E.g. if scalar=2, your data in time t will be at maximum twice as much (2x) and at least half (1/2) as much
    of your previous time period (t-dt).

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe
    dt : int, optional
        Time steps difference, by default 5
    scalar : int, optional
        Scalar (divide / multiply your previous data by a scalar ), by default 2

    Returns
    -------
    _type_
        Your dataframe with maximum diffusion constraints
    """
    for t in df.index:
        if t > df.index[0]:
            prev_val = df.loc[t - dt]
            df.loc[t] = df.loc[t].clip(prev_val / scalar, prev_val * scalar)
    return df


def fun_convert_iam_data_to_mt(reg_val: pd.Series):
    """Convert GHG emissions (e.g. from kt to Mt, based on the index.name of the pd.Series)

    Parameters
    ----------
    reg_val : pd.Series
        Regional emissions data to be converted

    Returns
    -------
    _type_
        Converted data to Mt

    Raises
    ------
    ValueError
        Raise error if we cannot identify the unit of IAM data
    """
    if "kt " in reg_val.index[0]:
        reg_val_conv = reg_val / 1e3
    elif "Mt " in reg_val.index[0]:
        reg_val_conv = reg_val
    else:
        raise ValueError("Please check your `reg_val` unit and convert it to Mt")

    return reg_val_conv


def fun_read_gwp() -> pd.DataFrame:
    """Reads GWP (Global Warmining Potential) data by gases and IPCC reports

    Returns
    -------
    pd.DataFrame
        GWP data
    """
    return pd.read_csv(CONSTANTS.INPUT_DATA_DIR / "gwp.csv")


def fun_harmonize(
    df: pd.DataFrame,
    region: str,
    gwp_dict: dict,
    ghg_emi_unit: str,
    df_iam: pd.DataFrame,
    var: str = "Emissions|Total Non-CO2",
    add_r_to_region_name: bool = True,
) -> pd.DataFrame:
    """Harmonize variables to match regional IAMs results

    Parameters
    ----------
    df : pd.DataFrame
        Downscaled results
    region : str
        IAM region
    gwp_dict : dict
        GWP dictionary
    ghg_emi_unit : str
        Unit of GHG emissions
    df_iam : pd.DataFrame
        Regional IAMs data
    var : str, optional
        Variable, by default "Emissions|Total Non-CO2"
    add_r_to_region_name : bool, optional
        Adds 'r' string at the end of the region name, by default True
    Returns
    -------
    _type_
        _description_
    """
    iam_vars = df_iam.reset_index().VARIABLE.unique()
    if (
        var == "Emissions|Total Non-CO2"
        and "Emissions|Kyoto Gases" in iam_vars
        and "Emissions|CO2" in iam_vars
    ):
        df_iam = fun_create_var_as_sum(
            df_iam,
            var,
            {"Emissions|Kyoto Gases": 1, "Emissions|CO2": -1},
            unit=ghg_emi_unit,
        )
    else:
        df_iam = fun_create_var_as_sum(
            df_iam,
            var,
            gwp_dict,
            unit=ghg_emi_unit,
        )
    if add_r_to_region_name:
        df["REGION"] = f"{region}r"
    else:
        df["REGION"] = region
    df = fun_reg_harmo_single_variable_all_regions_targets_negative_values(
        var,
        df_iam.rename_axis(index={"TARGET": "SCENARIO"}),
        df.rename_axis(index={"TARGET": "SCENARIO", "REGION": "ISO"}).set_index(
            "REGION", append=True
        ),
    )

    return df


def fun_get_gwp_and_ghg_unit(
    gwp_data: pd.DataFrame,
    df_iam: pd.DataFrame,
    res_dict_all_gases: dict,
    non_co2_gases: list,
    ar: Union[None, str] = None,
) -> Union[dict, str]:
    """_summary_

    Parameters
    ----------
    gwp_data : pd.DataFrame
        Dataframe with GWP data
    df_iam : pd.DataFrame
        Regional IAM results
    res_dict_all_gases : dict
        dictionary with downscaled non-gases
    non_co2_gases : list
        List of non co2 gases
    ar: Union[None, str], optional
        Get GWP for a selected Assessment Report e.g. 'AR5'. If None we use most updated AR data available (e.g AR6), defaults to None
    Returns
    -------
    Union[dict, str]
        Dictionary with GWP and unit of Emissions|Kyoto Gases variable

    Raises
    ------
    ValueError
        Raises error if multiple units are found for Emissions|Kyoto Gases  in the regional IAM results
    """
    # Get all GWPs for all IPCC ARs
    ipcc = {}
    for i in ["AR4", "AR5", "AR6"]:
        ipcc[i] = {
            f"Emissions|{g}": fun_conversion_to_mtco2_eq(
                gwp_data, res_dict_all_gases[g], i
            )
            for g in non_co2_gases
        }

    # Use most updated GWP data (e.g. AR6 if available)
    gwp_dict = {}
    for i in ["AR4", "AR5", "AR6"]:
        for g in non_co2_gases:
            key = f"Emissions|{g}"
            if ipcc[i][key] > 0:
                gwp_dict[key] = ipcc[i][key]

    # Get GWP for a selected AR (if data are available, if not use most updated data)
    if ar is not None:
        for g in non_co2_gases:
            key = f"Emissions|{g}"
            if ipcc[ar][key] > 0:
                gwp_dict[key] = ipcc[ar][key]

    ghg_emi_unit = (
        df_iam.xs("Emissions|Kyoto Gases", level="VARIABLE").reset_index().UNIT.unique()
    )
    if len(ghg_emi_unit) > 1:
        raise ValueError("Multiple units found in the dataframe for GHG emissions")
    ghg_emi_unit = ghg_emi_unit[0]
    return gwp_dict, ghg_emi_unit


def fun_non_co2_downscaling(
    df_non_co2: pd.DataFrame,
    model: str,
    df_iam: pd.DataFrame,
    region: str,
    countrylist: list,
    target: str,
) -> dict:
    """_summary_

    Parameters
    ----------
    df_non_co2 : pd.DataFrame
        Non-CO2 data from GAINS
    model : str
        IAM model
    df_iam : pd.DataFrame
        Regional IAM results
    region : str
        Region of IAMs
    countrylist : list
        list of countries within a region
    target : str
        scenario

    Returns
    -------
    dict
        Dictionary with downscaled non-co2 emissions by gases, for a single model, region, target
    """
    res_dict_all_gases = {}
    for g in ["CH4", "N2O", "HFC", "SF6"]:
        if len(
            fun_non_co2_data_single_gas_region_time(df_non_co2, g, countrylist, 2010)
        ):
            gas = f"Emissions|{g}"
            reg_val_all_time = None
            if (
                gas
                in df_iam.xs((model, target, f"{region}r"))
                .reset_index()
                .VARIABLE.unique()
            ):
                reg_val_all_time = df_iam.xs((model, target, f"{region}r", gas))
            else:
                print(f"{gas} not available for {region} - we assume is zero")

            res_dict_single_t = {}
            for t in df_iam.columns:
                # Get country level data from GAINS for all targets (h_ndc, o_1p5C) for a single year, sector, industry, region (and add regional data)
                gains_country = fun_non_co2_data_single_gas_region_time(
                    df_non_co2, g, countrylist, min(t, 2050)
                )

                if reg_val_all_time is not None:
                    reg_val = reg_val_all_time[t]
                    # convert GHG emissions (e.g. from kt to Mt, based on the index.name of the pd.Series)
                    reg_val_conv = fun_convert_iam_data_to_mt(reg_val)
                else:
                    unit_dict = {
                        "HFC": {"kt HFC134a-equiv/yr": 0},
                        "SF6": {"kt SF6/yr": 0},
                        "CH4": {"Mt CH4/yr": 0},
                        "N2O": {"kt N2O/yr": 0},
                    }
                    reg_val_conv = pd.Series(unit_dict[g])
                    reg_val_conv.index.name = "UNIT"

                gains_country = gains_country.reset_index()
                gains_country = gains_country.rename({"REGION": "ISO"}, axis=1)
                # Get Resources data at the country level by interpolating GAINS data based on IAM results (x= regional iam results)
                res_dict_single_t[t] = fun_get_country_prod_from_reg_prod(
                    reg_val_conv,
                    gains_country,
                    countrylist,
                    x_col="reg_prod",
                    y_col=min(t, 2050),
                )

            country_prod = pd.DataFrame(
                [res_dict_single_t[t] for t in df_iam.columns],
                index=df_iam.columns,
            )

            # Impose diffusion constraints:
            country_prod = fun_diffusion_constraints(country_prod, scalar=10)

            # Harmonize data
            # if isinstance(reg_val_conv, pd.Series):
            if gas in df_iam.reset_index().VARIABLE.unique():
                ratio = df_iam.xs((model, target, f"{region}r", gas)).sum(
                    axis=0
                ) / country_prod.sum(axis=1)
            else:
                ratio = np.nan  # this means no IAMs data (regional value=0)
            country_prod_harmo = country_prod.T * ratio

            # Convert to IAMC format
            res_dict_all_gases[g] = fun_convert_ghg_df_in_iamc(
                model, target, gas, reg_val_conv.index[0], country_prod_harmo
            )

    return res_dict_all_gases


def fun_conversion_to_mtco2_eq(
    gwp_data: pd.DataFrame, df: pd.DataFrame, ipcc: str = "AR4"
):
    variables_count = len(df.reset_index().VARIABLE.unique())

    if variables_count > 1:
        raise ValueError(
            "Dataframe should contain only one variable."
            f"We found {variables_count} Variables"
        )

    g = df.reset_index().VARIABLE.unique()[0]
    g = g.replace("Emissions|", "")
    unit = df.reset_index().UNIT.unique()[0]
    magnitude = unit[:2]
    gwp = gwp_data[(gwp_data.gwp == ipcc)]
    if g in gwp_data.chemical_formula.unique():
        gwp = gwp[gwp.chemical_formula == g].value.iloc[0]
        gwp = float(gwp.replace(",", ""))
    else:
        u = unit.replace("kt ", "").replace("-equiv/yr", "")
        u = u.replace("HFC", "HFC-")
        if u in u in gwp_data.common_name.unique():
            gwp = gwp[gwp.common_name == u]
            if len(gwp):
                gwp = float(gwp.value.iloc[0].replace(",", ""))
            else:
                gwp = np.nan
        else:
            raise ValueError("Unable to find GWP in this dataframe")
    magnitude_dict = {"mt": 1, "kt": 1e-3, "t": 1}
    return magnitude_dict[magnitude.lower()] * gwp


def fun_flatten_list(l, _unique=False):
    mylist = [item for sublist in l for item in sublist]
    return unique([item for sublist in l for item in sublist]) if _unique else mylist

# Calculate frequency (occurrency) for each tuple (method 1)
# frequency={all_seeds.count(x):x for x in all_seeds}
# # Selected seeds with max occurrency
# sel_seeds= frequency[max(frequency)]

# Calculate frequency (occurrency) for each tuple (method 2 - better)
# frequency={x:all_seeds.count(x) for x in all_seeds}
# # Selected seeds with max occurrency
# sel_seeds = {i for i in frequency if frequency[i]==max(frequency.values())}

# Sort seeds tuple by max occurrence (method 3 - more flexible)
def fun_select_max_occurrency_in_list_of_tuples(_top: int, list_of_tuple: list) -> list:
    """Select items with maximum occurrency in a list of tuple.
    Example: _top=1, mytuple=[(1,2), (1,2), (3,1)]  -> [(1, 2)] (element with max occurence)

    Parameters
    ----------
    _top : _type_
        How many element with max occurrence you want to return

    list_of_tuple : list
        Your list of tuple

    Returns
    -------
    list
        _top most frequent elements in your list_of_tuple
    """
    frequency = {x: list_of_tuple.count(x) for x in list_of_tuple}
    sel_seeds2 = []
    for x in heapq.nlargest(_top, frequency.values()):
        sel_seeds2 = sel_seeds2 + [i for i in frequency if frequency[i] == x]
    return sel_seeds2[:_top]

def fun_select_max_range_across_all_countries(
    df_desired_dict: pd.DataFrame,
    countrylist: list,
    var: str,
    full_range,
    _max_time=2050,
    _type: str = "dynamic_recursive",
    absolute_value: bool = True,  # True for top_20 analysis
):

    all_seeds = []

    for c in countrylist:
        res_dict = {}
        res_dict["max"] = {}
        res_dict["min"] = {}
        for x in full_range:
            if absolute_value:
                diff = (
                    df_desired_dict[x][_type][var]
                    # NOTE 'standard assumption' are always 'dynamic_recursive'
                    - df_desired_dict["standard"]["dynamic_recursive"][var]
                )
            else:
                # Percentage values
                diff = (
                    df_desired_dict[x][_type][var]
                    # NOTE 'standard assumption' are always 'dynamic_recursive'
                    / df_desired_dict["standard"]["dynamic_recursive"][var]
                    - 1
                )

            for mi in ["min", "max"]:
                _val = (
                    diff.clip(0).loc[:_max_time]
                    if mi == "max"
                    else diff.clip(-np.inf, 0).loc[:_max_time]
                )

                # Below Bleuprint for ENLONG (in case we want to choose criteria with max deviation compared to ENLONG)
                # _val = (
                #     diff.clip(0).loc[:_max_time]
                #     if mi == "max"
                #     else diff.clip(-np.inf, 0).loc[_max_time:]
                # )

                _val = np.abs(_val.xs(c, level="ISO")).sum()  # if len(_val) > 0 else 0
                res_dict[mi].update({x: _val})

        min_seed = fun_get_dict_key_associated_with_max_value(res_dict["min"])
        max_seed = fun_get_dict_key_associated_with_max_value(res_dict["max"])
        seed = [(min_seed, max_seed)]

        all_seeds = all_seeds + seed

    # NOTE: below we sort the tuple because for our purpose (5,1) is equivalent to (1,5)
    # all_seeds=[(1,5), (5,1)]
    # -> all_seeds=[(1,5), (1,5)]
    # return [tuple(sorted(x)) for x in all_seeds]
    return [tuple(sorted(x)) for x in all_seeds if "standard" not in x]



def fun_get_dict_key_associated_with_max_value(mydict):
    max_val = max(mydict.values())
    return list({i: j for i, j in mydict.items() if mydict[i] == max_val}.keys())[0]



def fun_get_default_mapping_from_various_projects(current_project: str, folders: list):
    """Create default_mapping.csv file from various projects (e.g. SHAPE, NAVIGATE) and save it to downscaler/input_data

    Parameters
    ----------
    folders : list
        List of project folders (e.g. ['NAVIGATE', 'SHAPE'])

    Returns
    -------
    _type_
        Dataframe with regional mapping
    """
    # NOTE: The order of the folders matters!!!
    # Start with the folders that contain the most updated data (default_mapping).
    # Reason: We use default_mapping from the less recent (less updated) folders only if the model mapping information is not present in the most recent folders

    mainf = "input_data"
    df_all = pd.DataFrame()
    for f in folders:
        print(f)
        mypath = f"{mainf}/{f}/default_mapping.csv"
        df = pd.read_csv(mypath).set_index("ISO")
        for x in df.columns:
            if x not in df_all.columns:
                df_all = pd.concat([df_all, df[x]], axis=1, sort=True)
    return df_all


def fun_find_all_xlsx_files_and_convert_to_csv(path: Union[str, Path]):
    """Get all *.xlsx files in a given folder `path` and save each sheet as `{file}-{sheetname}.csv`.

    Parameters
    ----------
    path : Union[str, Path]
        Folder path
    """
    # Step 1 convert all xsls excel files to csv
    # find all *.xlsx files into current directory and iterate over it
    # and save to csv files
    for file in Path(path).glob("*.xlsx"):
        if not os.path.exists(f"{Path(str(file).replace('.xlsx', '-data.csv'))}"):
            wb = load_workbook(file)  # read the Excel file
            print(file, wb.active.title)
            # Export all sheets to CSV:
            for sheetname in wb.sheetnames:
                df = pd.DataFrame(wb[sheetname].values)
                df = fun_skip_first_row(df)
                df.to_csv(f"{path}/{file.stem}-{sheetname}.csv", encoding="utf-8-sig")

                # # Write to utf-8 encoded file with BOM signature
                # with open(
                #     f"{path}/{file.stem}-{sheetname}.csv", "w", encoding="utf-8-sig"
                # ) as csvfile:
                #     # Write to CSV
                #     spamwriter = csv.writer(csvfile)
                #     # Iterate over rows in sheet
                #     for row in wb[sheetname].rows:
                #         # Write a row
                #         spamwriter.writerow([cell.value for cell in row])
        else:
            print(
                f"{Path(str(file).replace('.xlsx', '-data.csv'))} - skip we already have the csv file"
            )


def fun_concatenate_all_df_in_folder(path):
    """Concatendate all datraframes (csv file) in a given folder

    Parameters
    ----------
    path : _type_
        path to folder with csv files

    Returns
    -------
    _type_
        Concatenated dataframes (all csv files)
    """
    df_all = pd.DataFrame()
    for root, dirs, files in os.walk(path):
        for file in files:
            if file.endswith(".csv"):
                if file=="versions.csv":
                    # Move file to parent directory (input_data/project) folder
                    dest = f"{path}/.."
                    print(f'Moving {file} to {dest}/{file} folder')
                    shutil.move(f"{path}/{file}", f"{dest}/{file}")
                else:
                    file_path = os.path.join(root, file)
                    print(os.path.join(root, file))
                    df = pd.read_csv(file_path)
                    df_all = pd.concat([df_all, df], axis=0, sort=True)
                    ## possible way to go - blueprint (replacing the above - to be tested):
                    ## NOTES: requires avoiding saving to csv all the time (otherwise will read / load the data twice)
                    ## requires modifying 1) fun_split_snapshot_by_model, and 2) remove fun_find_all_xlsx_files_and_convert_to_csv
                    # file_path = os.path.join(root, file)
                    # print(os.path.join(root, file))
                    # if file.endswith(".csv"):
                    #     df = pd.read_csv(file_path)
                    # elif file.endswith(".xls"):
                    #     df = pd.read_excel(file_path)
                    # elif file.endswith(".xlsx"):
                    #     df = pd.read_excel(file_path, engine="openpyxl")
                    # else:
                    #     df=pd.DataFrame()
                    # df_all = fun_drop_duplicates(pd.concat([df_all, df], axis=0, sort=True))
    return df_all


def fun_create_add_region(df, df_registration, mycol, model, replace_dot=True):
    """This function creates aggregated regions e.g. WEU+EEU, if such region is present in the df with IAM results.
    It returns the updated df_registration
    """
    if model not in df.index.get_level_values("MODEL").unique():
        return df_registration

    df_regions = df.xs(model, level="MODEL").index.get_level_values("REGION").unique()
    agg_regions = [x.rsplit("+") for x in df_regions][0]

    if len(agg_regions):
        if replace_dot:
            mod_region = [x.replace(".", "") for x in agg_regions]
        else:
            mod_region = agg_region
        agg_region = {x: df_regions[0] for x in mod_region}
        df_registration[mycol] = [agg_region.get(x, x) for x in df_registration[mycol]]
        return df_registration
    else:
        return df_registration


def fun_create_default_mapping(
    project: str,
    model_reg_folder: Union[None, str] = None,
    previous_projects_folders: Union[None, list] = None,
    df=None,
    sheet: str = "regional definition",
    col_sheet: str = "Native Region Name",  # "Native Region Name_RAW",
    add_model_name_to_region: bool = False,  # e.g. `MESSAGE|WEU`
    model=None,
    replace_slash_with_undescore=True,
    clean_file=True,
    automatically_detect_region_names=True,
    region_name_short=False,
    country_marker_list: Union[None, list] = None,
    save_to_csv: bool = True,
):
    models_available = []
    if df is not None:
        models_available = [
            x.replace(".REGION", "")
            for x in df.columns
            if type(x) is str and "REGION" in x
        ]
    df_mapping = None
    if os.path.exists(f"{project}/default_mapping.csv"):
        df_mapping = pd.read_csv(f"{project}/default_mapping.csv")
        df_mapping = df_mapping.rename({fun_find_iso_column(df_mapping): "ISO"}, axis=1)
        df_mapping = (
            df_mapping.rename({"index": "ISO"}, axis=1)
            if "ISO" not in df_mapping.columns
            else df_mapping
        ).set_index("ISO")
        if model in models_available:
            print(
                f"We use existing default_mapping available in:  {project}/default_mapping.csv. We do not create a new file.  \n"
                "If you wish to create a new file from scratch please delete the `default_mapping.csv` file "
            )
            return df_mapping

    if model_reg_folder is not None:
        print(
            "We create `default_mapping.csv` from model registration files \n"
            f"(available in: {model_reg_folder})."
        )
        df_mapping = fun_create_mapping_from_model_registration(
            df,
            model_reg_folder,
            sheet,
            col_sheet,
            add_model_name_to_region,
            df_mapping,
            replace_slash_with_undescore=replace_slash_with_undescore,
            clean_file=clean_file,
            automatically_detect_region_names=automatically_detect_region_names,
            region_name_short=region_name_short,
            country_marker_list=country_marker_list,
        )
        # Save to CSV
        if save_to_csv:
            df_mapping.to_csv(Path(project) / "default_mapping.csv")
        return df_mapping

    # Get default mapping from different projects
    if previous_projects_folders is not None:
        df_mapping = fun_get_default_mapping_from_various_projects(
            project, previous_projects_folders
        )
        # WE save it in input_data directory (to be moved in the relevant folder)
        df_mapping.reset_index().to_csv(Path(project) / "default_mapping.csv")
        return df_mapping

    if not os.path.exists(f"{project}/default_mapping.csv"):
        raise ValueError(
            f"Cannot find a default_mapping.csv in {project}. There are two options: \n"
            f"1 - If you have a folder with the model registration files, please pass the path `model_reg_folder` (Currently `model_reg_folder` is None) \n"
            f"2 - If you have run the downscaling code before, you can get the mapping file from previous project folders. In this case, please pass a list with `previous_projects_folders` (Currently None)"
        )
    return pd.read_csv(f"{project}/default_mapping.csv")


def fun_create_mapping_from_model_registration(
    df,
    model_reg_folder,
    sheet,
    col_sheet,
    add_model_name_to_region,
    df_mapping_existing: pd.DataFrame,
    replace_slash_with_undescore=True,
    clean_file=True,
    automatically_detect_region_names=True,
    region_name_short=False,
    country_marker_list: Union[None, list] = None,
):
    df_registration_all = pd.DataFrame()
    existing_models = []
    models = []
    for file in model_reg_folder.glob("*.xls*"):  # .iterdir():
        try:
            if os.path.splitext(file)[1] == ".xlsx":
                df_model = pd.read_excel(
                    file, engine="openpyxl", sheet_name="model regions"
                )
            else:
                df_model = pd.read_excel(file, sheet_name="model regions")
            df_model.columns = [x.upper() for x in df_model.columns]
            model = df_model.MODEL.unique()[0]

            if df_mapping_existing is not None and len(df_mapping_existing):
                existing_models = [
                    x.replace(".REGION", "") if type(x) is str else x
                    for x in df_mapping_existing.columns
                ]
            if df_registration_all is not None and len(df_registration_all):
                existing_models = existing_models + [
                    x.replace(".REGION", "") if type(x) is str else x
                    for x in df_registration_all.columns
                ]
            if replace_slash_with_undescore:
                model = model.replace("/", "_")

            if model not in existing_models:
                if os.path.splitext(file)[1] == ".xlsx":
                    df_registration = pd.read_excel(
                        file, engine="openpyxl", sheet_name=sheet
                    )
                else:
                    df_registration = pd.read_excel(file, sheet_name=sheet)
                mycol = f"{model}.REGION"

                # automatically get region name
                if automatically_detect_region_names:
                    mydict = {
                        x: max(
                            [0] + [len(i) for i in df_registration[x] if type(i) is str]
                        )
                        for x in df_registration.columns
                        if x.strip() not in ["ISOCode", "Country"]
                        if "Unnamed" not in x
                    }
                    if region_name_short == True:
                        # Get `short` native region names e.g. 'LAM'
                        col_sheet = [
                            k for k, v in mydict.items() if v == min(mydict.values())
                        ][0]
                    else:
                        # get `long` region name e.g. 'Latin America'
                        col_sheet = [
                            k for k, v in mydict.items() if v == max(mydict.values())
                        ][0]
                df_registration.rename(
                    columns={
                        "ISO Code": "ISO",
                        col_sheet: mycol,
                        "Country ": "Country",
                    },
                    inplace=True,
                )

                if add_model_name_to_region:
                    df_registration.loc[:, mycol] = [
                        f"{model}|{x}" for x in df_registration[mycol]
                    ]

                if df is not None:
                    df_registration = fun_create_add_region(
                        df, df_registration, mycol, model, replace_dot=True
                    )
                df_registration.loc[:, "Country"] = [
                    x.lower().capitalize() if type(x) is str else x
                    for x in df_registration.Country
                ]
                if "ISO" in df_registration.columns:
                    df_registration.set_index(
                        [
                            "ISO",
                        ],
                        inplace=True,
                    )  #'Country'
                # df_registration[f"{model}.NAME"] = df_registration[f"{model}.REGION"]

                df_registration = df_registration[[f"{model}.REGION", "Country"]]
                if (
                    df_registration[f"{model}.REGION"].isna().sum(axis=0)
                    / len(df_registration)
                ) == 1:
                    # This means all data are np.na
                    raise ValueError(
                        f"{model} model does not seem to contain any ISO mapping. Please check your data, sheet {sheet} "
                    )

                df_registration_all = pd.concat(
                    [df_registration_all, df_registration.dropna(how="all")],
                    axis=1,
                    sort=True,
                )
                models = [model] + models
        except Exception as e:
            print(f"file {file} did not work: {e}")
            # print(traceback.format_exc())

    if df_registration_all is not None:
        for x in ["R5_region", "IPCC"]:
            if x not in df_registration_all.columns:
                df_registration_all[x] = np.nan

    # Add markers
    if country_marker_list is not None:
        region_marker_dict = fun_create_dict_of_regions_associated_with_countrylist(
            models, df_registration_all, country_marker_list
        )
        if region_marker_dict is not None:
            df, df_registration_all = fun_add_region_marker(
                None, df_registration_all, region_marker_dict, "_marker"
            )

    if clean_file:
        cols = [x for x in df_registration_all.columns if ".REGION" in x]
        df_registration_all = df_registration_all[cols]
        df_registration_all.columns = [
            x.replace(".REGION", "") for x in df_registration_all.columns
        ]

    # Just ISO codes (len=3)
    isoidx = [x for x in df_registration_all.index if type(x) is str if len(x) == 3]
    df_registration_all = df_registration_all.loc[isoidx]

    if df_mapping_existing is not None and len(df_mapping_existing):
        cols = [
            x
            for x in df_registration_all.columns
            if x not in df_mapping_existing.columns
        ]
        df_registration_all = pd.concat(
            [df_mapping_existing, df_registration_all[cols]], axis=1, sort=True
        )
    df_registration_all.index.names = ["ISO"]
    return df_registration_all


def fun_read_or_create_snapshot(
    folder,
    multiple_df_folder,
    name="snapshot_all_regions",
):
    suff_list = ["", "_RAW"]
    for s in suff_list:
        if os.path.exists(folder / f"{name}{s}.csv"):
            print(f"We use existing {name}  available in:  {folder}.")
            if multiple_df_folder is not None:
                print(
                    f"We do not create a new file from input_data/{folder}/{multiple_df_folder}."
                )
                print(
                    f"If you wish to create a new file from scratch please delete the `{name}{s}.csv` file "
                )
            df_all = pd.read_csv(Path(f"{folder}/{name}{s}.csv"))
            existing_cols = df_all.columns
            cols = [x.upper() for x in existing_cols]

            if (cols != existing_cols).any():
                df_all.columns = cols
                df_all = df_all.set_index(
                    ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]
                )

            if len(df_all) > 1:
                return df_all.set_index(
                    ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]
                )
    if multiple_df_folder is not None:
        # create a snaphshot_all_regions_RAW.csv from multiple dataframes files csv or excel (pandas 1.0.5)
        # Step 1 convert excel file into csv files
        fun_find_all_xlsx_files_and_convert_to_csv(f"{folder}/{multiple_df_folder}")
        # Step 2 combine all csv files available in folder, into a single file:
        df_all = fun_concatenate_all_df_in_folder(f"{folder}/{multiple_df_folder}")

        # Step 3 Save folder to CSV file
        if len(df_all) > 1:
            df_all = fun_clean_up_snapshot(df_all)
            return df_all
        raise ValueError(f"Unable to get files from {folder}/{multiple_df_folder}")
    raise ValueError(
        f"Unable to find `snapshot_all_regions.csv` nor `snapshot_all_regions_RAW.csv` in {folder}/. \n "
        "If you wish to create a snapshot by combining multiple datasets, please provide a `multiple_df_folder` (currently equal to None)"
    )


def fun_add_non_biomass_ren_nomenclature(df, inverse=False):
    primary_ren = [
        "Geothermal",
        "Hydro",
        "Solar",
        "Wind",
    ]
    iam_nomenclature_dict = {
        f"Primary Energy|{x}": f"Primary Energy|Non-Biomass Renewables|{x}"
        for x in primary_ren
    }
    if inverse:
        iam_nomenclature_dict = {v: k for k, v in iam_nomenclature_dict.items()}
    df = df.rename(index=iam_nomenclature_dict)
    return df, primary_ren


def fun_add_regions_scenarios_names(
    country_marker_list: list,
    scenario_marker_dict: dict,
    rename_df_mapping_dict: dict,
    project: str,
    df_mapping: pd.DataFrame,
    model: str,
    coerce_errors: bool,
) -> pd.DataFrame:
    """Rename regions and scenario names.

    Parameters
    ----------
    country_marker_list : list
        List of countries that you would like to downscale
    scenario_marker_dict : dict
        List of scenarios that you want to mark as "_marker" e.g. 'SSP1' -> 'SSP1_marker'
    rename_df_mapping_dict : dict
        dictionary with region names
    project : str
        Project folder e.g. 'NGFS_2022'
    df_mapping : pd.DataFrame
        IAMs regional country mapping
    model : str
        model e.g. MESSAGE
    coerce_errors: bool
        Coerce errors and action input (e.g. ask if user whises to continue)

    Returns
    -------
    pd.DataFrame
        Updated dataframes with renames regions and scenarios
    """
    df = fun_get_df_snapshot(project, model, split_snapshot_by_model=False)

    # Check alternative model mapping
    df_mapping = fun_find_alternative_model_mapping(df_mapping, df)

    # Ranaming sub-saharan africa in df_mapping (fix upper/lower case)
    if model in rename_df_mapping_dict:
        df_mapping = df_mapping.replace(rename_df_mapping_dict[model])
    else:
        df_mapping = fun_fix_upper_lower_case_in_regions(
            df, df_mapping, coerce_errors=coerce_errors
        )

    # Remove np.nan from df.index.get_level_values('VARIABLE')

    # Add scenario marker:
    if scenario_marker_dict:
        df = fun_add_marker(
            df, "_marker", scenario_marker_dict, key="MODEL", val="SCENARIO"
        )
        print("Adding scenario markers")
    print(
        "adding model to regions names (e.g. MESSAGE|Western Europe) like NGFS project..."
    )
    df=fun_drop_duplicates(df)
    df = fun_add_ngfs_regions_names(
        df,
        df_mapping,
    )

    # Add regions marker (e.g. EU only)
    if country_marker_list is not None and len(country_marker_list):
        print("Adding region markers")
        df, df_mapping = fun_check_input_and_add_region_markers(
            country_marker_list,
            None,
            df,
            df_mapping,
        )
    return df_mapping, df


def fun_get_df_snapshot(project, model, split_snapshot_by_model=False):
    df = fun_snapshot(
        CONSTANTS.INPUT_DATA_DIR / project,
        model,
    )

    # Split snapshot
    if split_snapshot_by_model:
        fun_split_snapshot_by_model(project, None, df)
    idxnames = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]

    # Fix bad nomenclature below
    df = fun_fix_bad_nomenclature(df, idxnames)

    df = fun_remove_np_in_df_index(df)
    return df


def fun_find_alternative_model_mapping(
    df_mapping: pd.DataFrame, df: list
) -> pd.DataFrame:
    """Suggests an alternative model mapping, if a model is not present in the default mapping.

    Parameters
    ----------
    df_mapping : pd.DataFrame
        Dataframe with regional/country mapping
    df : list
        Dataframe with IAMs snapshot file

    Returns
    -------
    pd.DataFrame
        Default mapping

    Raises
    ------
    ValueError
        If IAMs snapshot file contains multiple models (or zero models).
    ValueError
        If a model is not present in the default_mapping, and the user decides to not use an alternative model mapping.
    """
    models = df.reset_index().MODEL.unique()
    if len(models) == 1:
        model = models[0]

        model_options = fun_fuzzy_match(list(df_mapping.columns), model)
        if not len(model_options):
            model_options = fun_sort_list_order_based_on_element_name(
                list(df_mapping.columns), model
            )
            model_options = unique(model_options)
        if f"{model}.REGION" not in model_options:
            action = input(
                f"{model} not available in default mapping."
                f"Would you like to use {model_options[0]} instead (y/n)? \n Or please type your model, e.g. {model_options[1:4]}"
            )
            if action.lower() in ["yes", "y"]:
                df_mapping[f"{model}.REGION"] = df_mapping[f"{model_options[0]}"]
            elif action in ["no", "n"]:
                raise ValueError(f"{model} is not available in the default mapping")
            else:
                df_mapping[f"{model}.REGION"] = df_mapping[action]

    else:
        print(f"df snapshot contains multiple (or zero) models: {models}")

    return df_mapping


def fun_snapshot(
    project,
    multiple_df_folder,
    name="snapshot_all_regions",
):
    df = fun_read_or_create_snapshot(
        project,
        multiple_df_folder,
        name=name,
    )

    return fun_add_non_biomass_ren_nomenclature(df)[0]


def fun_clean_up_snapshot(df):
    df.columns = [i.upper() for i in df.columns if type(i) is str]
    df.columns = [int(i) if i[:1] == "1" or i[:1] == "2" else i for i in df.columns]
    ## REPLACING AIM/CGE with AIM_CGE
    for x in ["/"]:
        df.loc[:, "MODEL"] = [i.replace(x, "_") for i in df["MODEL"]]
        df.loc[:, "REGION"] = [
            str(i).replace(x, "_") for i in df["REGION"]
        ]  ## SAME AS A
    df = df.set_index(["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    cols = [x for x in df.columns if str(x)[0] in ["1", "2"]]
    df = df[cols]
    return df


def fun_add_marker(
    df, suffix, ssp_marker_dict, key: str = "MODEL", val: str = "SCENARIO"
):
    """Add a marker `suffix` in your `df`, based on a  `ssp_marker_dic` (defines the marker for example for a given model/scenario combination).
    Example:
    If you run: fun_add_marker(df, "_marker", {"IMAGE 3.3":'SDP_EI-1p5C'}, key="MODEL", val="SCENARIO").loc["IMAGE 3.3"].reset_index().SCENARIO.unique()
    You might get this -> ['SDP_EI-1p5C_marker'] as a list of scenarios for IMAGE

    Parameters
    ----------
    df : _type_
        You dataframe (usually with regional IAM results)
    suffix : _type_
        A string to mark your scenario or region (or whethever you have defined ssp_marker_dict)
    ssp_marker_dict : _type_
        You marker dictionary e.g. {"IMAGE 3.3":'SDP_EI-1p5C'} will mark all 'SDP_EI-1p5C' scenarios of IMAGE as 'SDP_EI-1p5C_marker'
    key : str, optional
        The column of the df corresponing to the key in the `ssp_marker_dict`, by default "MODEL"
    val : str, optional
        The column of the df corresponing to the value in the `ssp_marker_dict`. In other workds this is the column of your df that you want to modify (add  `suffix`), by default "SCENARIO"

    Returns
    -------
    _type_
        Dataframe with region and scenario markers

    Raises
    ------
    ValueError
        If `ssp_marker_dict` is not a dictionary
    """
    if not isinstance(ssp_marker_dict, dict):
        raise ValueError(
            f"`ssp_marker_dict` must be a dictionary. You passed a {type(ssp_marker_dict)}"
        )
    for model in ssp_marker_dict:
        if model in df.index.get_level_values(key).unique():
            print(model)
            if not isinstance(ssp_marker_dict[model], list):
                ssp_marker_dict[model] = [ssp_marker_dict[model]]
            for k in ssp_marker_dict[model]:
                x = f"{model}|{k}" if val == "REGION" else k
                if x in df.index.get_level_values(val).unique():
                    print(ssp_marker_dict[model])
                    marker = df.xs(
                    (model, x), level=(key, val), drop_level=False
                    ).copy()
                    scen_dict = {f"{x}": f"{x}{suffix}"}
                    idx = marker.index
                    marker.rename(index=scen_dict)
                    df = pd.concat([df.drop(idx, axis=0), marker.rename(index=scen_dict)])
                    # df = df.drop(idx, axis=0).append(marker.rename(index=scen_dict))
    return df


def fun_rename_message_regions_for_ngfs(df):
    if len(df.index.get_level_values("MODEL").unique()) == 1:
        model = df.index.get_level_values("MODEL").unique()[0]
        print(model)

        if "MESSAGE" in model:
            message_region_dict = {
                x: f"{model}|{x.rsplit()[0]}"
                for x in df.reset_index()["REGION"].unique()
                if x.find("|") != (-1)
            }
            if model != list(message_region_dict.keys())[0].split("|")[0]:
                print("we rename the MESSAGE regions")
                # message_region_dict={x:f'{model}|{x.rsplit()[1]}' for x in df.reset_index()['REGION'].unique() if x.find("|")!=(-1)}
                message_region_dict = {
                    x: f"{model}|{x.rsplit('|')[1]}"
                    for x in df.reset_index()["REGION"].unique()
                    if x.find("|") != (-1)
                }
                df2 = df.reset_index().copy(deep=True)
                df2.loc[:, "REGION"] = [
                    message_region_dict.get(x, x) for x in df2.REGION
                ]
                df2.set_index(
                    ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"], inplace=True
                )
                df = df2
    return df


def fun_check_missing_regions(df, df_mapping, exclude_regions_with_one_country=True):
    models = list(df.index.get_level_values("MODEL").unique())

    nigem = "NiGEM NGFS v1.22"
    if nigem in models:
        models.remove(nigem)

    for model in models:
        regions = list(df.loc[model].index.get_level_values("REGION").unique())
        regions = [x for x in regions if x is not np.nan]
        reg_mapping = df_mapping[f"{model}.REGION"].unique()

        regions_match = [
            x.replace(f"{model}|", "")
            for x in regions
            if x.replace(f"{model}|", "") in reg_mapping
        ]
        regions_present = df_mapping[df_mapping[f"{model}.REGION"].isin(regions_match)]
        regions_not_present = set(df_mapping.index) - set(regions_present.index)
        missing_regions = list(
            df_mapping.loc[list(regions_not_present)][f"{model}.REGION"].unique()
        )
        if np.nan in missing_regions:
            missing_regions.remove(np.nan)

        if missing_regions:
            if exclude_regions_with_one_country:
                cols = [x for x in df_mapping if model in x]
                for col in cols:
                    for region in missing_regions:
                        if len(df_mapping[df_mapping[col] == region]) <= 1:
                            missing_regions.remove(region)
        if missing_regions:
            print("#######")
            print(model, end=2 * "\n")
            print("Missing regions:")
            print(missing_regions, end=2 * "\n")
            print("Regions currently available in snapshot")
            print(list(regions), end=2 * "\n")
    return missing_regions, regions


def fun_check_ngfs_region_names(df, df_countries):
    # We need an ISO column in the df_countries.
    # Therefore we check if an ISO column exists:
    # A) in the index names -> if so we reset index
    # B) in the index colums -> all good, we do nothing
    # C) we check if the index contains an ISO code but it is not named as such by. This is the case if
    # C1) maximum one index names -> len(df_countries.index.names) == 1
    # C2) max/min len==3 (because we use ISO3 code)
    # C3) "AUT" should be in that column. -> "AUT" in df_countries.index
    # If all conditions are met so we call that column 'ISO'
    # D) we check if a column contains ISO codes, but this column is not named as such.
    # NOTE we keep option D as the last one as there is a risk to choose a wrong column:
    # e.g. if a model uses short names for regions and those regions coincides with ISO codes e.g. `USA`

    if "ISO" in df_countries.index.names:  # A)
        df_countries = df_countries.reset_index()
    elif "ISO" in df_countries.columns:  # B)
        pass
    elif (  # C)
        min(len(str(x)) for x in df_countries.index if type(x) is str) == 3
        and max(len(str(x)) for x in df_countries.index if type(x) is str) == 3
        and "AUT" in df_countries.index
        and len(df_countries.index.names) == 1
    ):
        df_countries.index.names = ["ISO"]
    else:  # D)
        df_countries = df_countries.reset_index()
        df_countries = df_countries.rename(
            {
                fun_find_iso_column(df_countries, iso_list=("AUT", "BEL")): "ISO",
            },
            axis=1,
        )

    regions_to_be_renamed = {}
    for model in df.index.get_level_values("MODEL").unique():
        regions = list(df.loc[model].index.get_level_values("REGION").unique())
        reg_list_renamed = {}
        for region in regions:
            if region in df_countries[f"{model}.REGION"].unique():
                if "ISO" in df_countries.columns:
                    countrylist = df_countries[
                        df_countries[f"{model}.REGION"] == region
                    ].ISO
                else:
                    countrylist = df_countries[
                        df_countries[f"{model}.REGION"] == region
                    ].index
                if (
                    len(countrylist) > 0
                    and model not in region
                    and f"{model}|{region}" not in regions
                ):
                    reg_list_renamed[region] = f"{model}|{region}"
        regions_to_be_renamed[model] = reg_list_renamed
    if regions_to_be_renamed:
        print("#####")
        print(
            "Regions must contain model name for NGFS, otherwise (step1 and step2) will not run"
        )
    return regions_to_be_renamed


def fun_add_region_marker(
    df: pd.DataFrame,
    df_mapping: pd.DataFrame,
    region_marker_dict: dict,
    marker_fuffix: str,
) -> pd.DataFrame:
    """Adds a `marker_suffix` to regions in the `df` and `df_mapping` based on a `region_marker_dict`

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe with regional IAMs results
    df_mapping : pd.DataFrame
        IAMs regional-country mapping
    region_marker_dict : dict
        Regions for which we want to add markers
    marker_fuffix : str
        String to be added as suffix to selected regions e.g. 'Western Europe' -> 'Western_europe_marker'

    Returns
    -------
    pd.DataFrame
        Updated `df` and `df_mapping`
    """
    if df is not None:
        df = fun_add_marker(
            df, marker_fuffix, region_marker_dict, key="MODEL", val="REGION"
        )

    for model, regions in region_marker_dict.items():
        cols = [x for x in df_mapping.columns if model in x]
        reg_dict = {region: f"{region}{marker_fuffix}" for region in regions}
        for col in cols:
            df_mapping.loc[:, col] = [
                reg_dict.get(x, x) for x in df_mapping.loc[:, col]
            ]
    df_mapping = fun_clean_up_default_mapping(df_mapping)
    return df, df_mapping


def fun_check_input_and_add_region_markers(
    country_marker_list: list,
    region_marker_dict: dict,
    df: pd.DataFrame,
    df_mapping: pd.DataFrame,
) -> pd.DataFrame:
    """Add region markers to `df` given a `country_marker_list` and a `df_mapping`

    Parameters
    ----------
    country_marker_list : list
        List of countries that you want to downscale. e.g. ['AUT','BEL']. Regions associated to these countries will be marked as '_marker'
    region_marker_dict : dict
        List of regions that you want to downscale e.g. ['Western Europe'], that will be marked as '_marker'
    df : pd.DataFrame
        Dataframe with IAMs
    df_mapping : pd.DataFrame
        IAMs regional/country mapping

    Returns
    -------
    pd.DataFrame
        _description_

    Raises
    ------
    ValueError
        If the user provides both  `country_marker_list` and a `region_marker_dict`
    """
    if country_marker_list is not None:
        if region_marker_dict is not None:
            raise ValueError(
                "Please either provide a `country_marker_list` or a region_marker_dict, not both.\n"
                "You have provided:\n"
                f"`country_marker_list`= {country_marker_list} \n"
                f"`region_marker_dict`= {region_marker_dict} \n"
            )

        region_marker_dict = None
        models = df.index.get_level_values("MODEL").unique()
        region_marker_dict = fun_create_dict_of_regions_associated_with_countrylist(
            models, df_mapping, country_marker_list
        )
        print("adding Regional markers...")

    if region_marker_dict is not None:
        df, df_mapping = fun_add_region_marker(
            df, df_mapping, region_marker_dict, "_marker"
        )
    return df, df_mapping


def fun_clean_up_default_mapping(df):
    # note clean up in case we have a region called 'Rest of Europe_marker_marker_marker_marker_marker_marker'
    # which can happen if you run step0 multipe times
    s = "_marker"
    for col in df.columns:
        df.loc[:, col] = [
            x.split(s)[0] + s if s in str(x) else x for x in df.loc[:, col]
        ]
    return df


def fun_add_ngfs_regions_names(df, df_mapping):
    ngfs_region_names = fun_check_ngfs_region_names(df, df_mapping)
    for k, v in ngfs_region_names.items():
        if len(v):
            df = df.rename(ngfs_region_names[k])
    print(
        "We rename regions and save `snapshot_all_regions_RAW` and `default_mapping` to csv"
    )
    idx_names = df.index.names
    df = df.reset_index()
    df["MODEL"] = [x.replace("/", "_") for x in df["MODEL"]]
    df = df.set_index(idx_names)
    return df


def fun_create_dict_of_regions_associated_with_countrylist(
    models, df_mapping, iso_list
):
    region_marker_dict = {}
    for model in models:
        marker_regions = df_mapping[df_mapping.index.isin(iso_list)][
            f"{model}.REGION"
        ].unique()
        # # mod 17.49 23 Dec
        # reg_list= [f'{model}|{x}' for x in marker_regions.tolist()]
        reg_list = [
            x.split("_marker")[0] if type(x) is str and "marker" in x else x
            for x in marker_regions.tolist()
        ]
        region_marker_dict[model] = reg_list
    return {k: v for k, v in region_marker_dict.items() if v != [np.nan]}


def fun_check_variables(df, var_needed, check_by="REGION"):
    df_interpolated = df
    models = list(df_interpolated.index.get_level_values("MODEL").unique())
    res = {}
    for model in models:
        model = model.replace("/", "_")
        regions = list(df.loc[model].index.get_level_values(check_by).unique())
        regions = [x for x in regions if x != str(np.nan)]
        missing_var_by_region = {}
        for region in regions:
            try:
                var_available = list(
                    df_interpolated[
                        df_interpolated.index.get_level_values(check_by) == region
                    ]
                    .loc[model]
                    .index.get_level_values("VARIABLE")
                    .unique()
                )
            except Exception as e:
                print(f"error in calculating `var_available`: {e}")

            missing = [x for x in var_needed if x not in var_available]
            if missing:
                missing_var_by_region[region] = missing
        if len(missing_var_by_region):
            # res[model] = fun_invert_dictionary(missing_var_by_region)
            res = fun_invert_dictionary(missing_var_by_region)
    if not len(res):
        print("all good with the variables")
    return res


def fun_invert_dictionary(mydict):
    new_dic = {}
    for k, v in mydict.items():
        for x in v:
            new_dic.setdefault(x, []).append(k)
    return new_dic


def fun_insert_missing_columns(df, cols = range(2005, 2105, 5)):
    cols = [int(i) for i in cols]
    # Get only time series
    selcols = [x for x in df.columns if str(x)[0] in ["1", "2"]]
    df = df[selcols]
    # Convert columns to integer and insert missing cols
    df.columns = [int(x) for x in df.columns]
    missing_cols = [x for x in cols if x not in df.columns]
    sel_col = list(set(df.columns).intersection(set(cols)))
    df = df[sel_col]
    for x in missing_cols:
        df.insert(1, x, np.nan)
    df = df.reindex(
        sorted(df.columns), axis=1
    )  ## sort columns could have been as simple as df=df[cols]

    return df, cols, missing_cols


def fun_insert_missing_cols_interpolate(df):
    df, cols, missing_cols = fun_insert_missing_columns(df)

    if len(missing_cols):
        df_interpolated = df[cols].interpolate(axis=1)
    else:
        df_interpolated = df[cols]
    for t in cols:
        if t > 2005 and t<2100 and len(df[t].value_counts()) == 0:
            selcols = [t - 5, t, t + 5]
            df_interpolated.loc[:, t] = df[selcols].interpolate(axis=1).loc[:, t]
    df_interpolated.columns = [str(i) for i in df_interpolated.columns]
    return df_interpolated


def fun_ccs_sequestration_in_absolute_value(
    df_interpolated,
    seq_var,
):
    sequestration = df_interpolated[
        df_interpolated.index.get_level_values("VARIABLE").isin(seq_var)
    ]
    seq_idx = sequestration.index

    df_interpolated.loc[seq_idx, :] = np.abs(sequestration.loc[seq_idx, :])
    df_interpolated.xs("Carbon Sequestration|CCS|Biomass", level="VARIABLE").tail()
    sequestration.index.get_level_values("VARIABLE").unique()

    return df_interpolated


def fun_read_df_iam_iamc(input_file: str) -> pd.DataFrame:
    """[This function reads IAMs results from a given file path `input_file`]
    Args:
        input_file ([str]): [Path of file with IAMs results]
    Returns:
        [pd.DataFrame]: [Dataframe with IAM results in IAMc format]
    """  ## Reading iam results
    df_iam = pd.read_csv(input_file)
    df_iam.columns = [x.upper() for x in df_iam.columns]
    df_iam = df_iam[
        (~(df_iam.MODEL == "Reference"))
    ]  ### Get only Energy and emissions variables (2021_03_02)
    df_iam.loc[:, "REGION"] = [
        x + "r" if type(x) is str else x for x in df_iam.REGION
    ]  ## adjusting region name to avoid overlap with ISO codes
    df_iam.rename(columns={"SCENARIO": "TARGET"}, inplace=True)
    df_iam.set_index(["MODEL", "TARGET", "REGION", "VARIABLE", "UNIT"], inplace=True)
    cols = [x for x in df_iam.columns if str(x[0]) in {"1", "2"}]
    df_iam = df_iam[cols]
    df_iam.columns = [int(x) for x in df_iam.columns]
    return df_iam


def fun_get_target_regions_step5(pyam_mapping_file, selection_dict, model):
    targets = [i for i in selection_dict[model]["targets"]]

    df_countries = fun_read_df_countries(
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv",
    )
    df_countries, regions = load_model_mapping(
        model, df_countries, pyam_mapping_file.file
    )

    regions = [i for i in selection_dict[model]["regions"]]
    return targets, df_countries, regions


def fun_native_iam_countries(
    model,
    df_countries,
    native_countries,
    df_iam,
    df=None,
    add_model_name=True,
    targets=None,
):
    if targets is None:
        targets = list(df_iam.index.get_level_values("TARGET").unique())
    cols = range(2010, 2105, 5)
    cols = [str(i) for i in cols]

    if add_model_name:
        native_regions = [
            f"{model}|{df_countries[df_countries.ISO==x].REGION[0]}"
            for x in native_countries
        ]
    else:
        native_regions = [
            f"{df_countries[df_countries.ISO==x].REGION[0]}" for x in native_countries
        ]
    reg_native_dict = pd.DataFrame(
        native_countries, index=native_regions, columns=["ISO"]
    ).to_dict()["ISO"]

    df_missing_iam_native_regions = df_iam[
        (df_iam.index.get_level_values("REGION").isin(native_regions))
        & (df_iam.index.get_level_values("MODEL") == model)
        & (df_iam.index.get_level_values("TARGET").isin(targets))
    ]
    if df is not None:
        df_missing_iam_native_regions = df_missing_iam_native_regions.rename_axis(
            df.index.names
        )[cols]

    return reg_native_dict, df_missing_iam_native_regions


def fun_native_countries(model, df_countries, regions, regions_suf="r"):
    native_countries = [
        df_countries[
            df_countries.REGION == r.replace(f"{model}|", "") + regions_suf
        ].ISO[0]
        for r in regions
        if len(
            df_countries[
                df_countries.REGION == r.replace(f"{model}|", "") + regions_suf
            ].ISO
        )
        == 1
    ]
    return native_countries


def fun_fix_bad_nomenclature(df, idxnames):
    if list(df.index.names) == [None]:
        df = df.set_index(idxnames)
    if len(df.index.get_level_values("MODEL").unique()) == 1:
        idxn = df.index.names
        model_name = df.index.get_level_values("MODEL")[0]
        df = df.reset_index()
        df.loc[:, "REGION"] = [
            model_name + "|" + x.split("|")[1] if type(x) is str and "|" in x else x
            for x in df.REGION
        ]
        df = df.set_index(idxn)
    return df


def fun_step5_bottom_up_harmo(
    add_model_in_region_name: bool,
    new_var_dict: dict,
    model: str,
    df_countries: pd.DataFrame,
    regions: list,
    df: pd.DataFrame,
    df_iam: pd.DataFrame,
    cols: list,
) -> Union[pd.DataFrame, pd.DataFrame, list]:
    """Create new varibles in the `df` as the sum of sub-sectors (based on `new_var_dict` definition)
    and harmonize data to match regional `df_iam` data for a `model` and a list of `regions`.

    Parameters
    ----------
    add_model_in_region_name : bool
        Whether to add the model (e.g. MESSAGE) to the region (e.g. CPA) name. -> `MESSAGE|CPA`
    new_var_dict : dict
        Definition of the new variables (keys) created as sum of sub-sectors (values)
    model : str
        Model
    df_countries : pd.DataFrame
        Regional mapping
    regions : list
        List of regions
    df : pd.DataFrame
        _description_
    df_iam : pd.DataFrame
        _description_
    cols : list
        Time periods of IAMs results

    Returns
    -------
    Union[pd.DataFrame, pd.DataFrame, list]
        Regional IAM df, updated downscaled dataframe (with bottom up harmonization)
    """
    setindex(df, False)
    df.rename(
        columns={
            "VARIABLE": "SECTOR",
            "REGION": "ISO",
            "SCENARIO": "TARGET",
        },
        inplace=True,
    )
    setindex(df, ["ISO", "TARGET", "SECTOR"])
    df_harmo_all = pd.DataFrame()
    df_harmo = pd.DataFrame()

    for region in regions:
        myreg = region + "r"
        df_iam, countrylist = fun_get_df_iam_countrylist(
            add_model_in_region_name, model, df_countries, regions, df_iam, region
        )

        if len(countrylist) > 0:
            df_harmo = df[df.index.get_level_values("ISO").isin(countrylist)].copy()
            for j, i in new_var_dict.items():
                if j in df_iam.reset_index().VARIABLE.unique():
                    df_harmo = fun_bottom_up_harmonization(
                        model,
                        myreg,
                        j,
                        i,
                        df_harmo,
                        df_iam,
                        add_model=f"{model}_downscaled",
                        add_unit="EJ/yr",
                    )

                df_harmo = df_harmo[cols]
                print("Bottom up harmo ", region)
                # df_harmo_all = df_harmo_all.append(df_harmo)
                df_harmo_all = pd.concat([df_harmo_all, df_harmo])
                # NOTE: remove duplicated values
                df_harmo_all = df_harmo_all[~df_harmo_all.index.duplicated()]
    return df_iam, df_harmo_all


def fun_get_df_iam_countrylist(
    add_model_in_region_name, model, df_countries, regions, df_iam, region
):
    if add_model_in_region_name:
        reg_dict = {f"{x.split('|')[1]}r": f"{x}r" for x in regions}
        df_iam = df_iam.xs(model, level="MODEL", drop_level=False).rename(reg_dict)

    countrylist = df_countries[
        df_countries.REGION == region.replace(f"{model}|", "") + "r"
    ].ISO.unique()

    return df_iam, countrylist


def fun_rename_sector_and_region(df_harmo_all):
    setindex(df_harmo_all, False)
    df_harmo_all.rename(
        columns={
            "SECTOR": "VARIABLE",
            "ISO": "REGION",
            "TARGET": "SCENARIO",
        },
        inplace=True,
    )
    setindex(df_harmo_all, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    return df_harmo_all


def fun_no_trade_after_2060(df):
    index0 = (
        fun_multindex(df, [("VARIABLE", "Secondary Energy|Electricity|Trade")])
    ).index
    index0
    df.loc[index0, "2065":] = np.nan
    return df


def fun_select_countries_that_report_variable(df, var):
    iso_final_energy = (
        fun_multindex(
            df,
            [
                ("VARIABLE", var),
            ],
        )
        .index.get_level_values("REGION")
        .unique()
        .tolist()
    )

    df = fun_multindex(df, [("REGION", iso_final_energy)])
    return df


def fun_print_variable_summary(model, df, selection_dict):
    test_variables, df_test_all = fun_create_df_test_all(selection_dict)
    df_test = pd.DataFrame(index=[model], columns=test_variables)
    for test in test_variables:
        df_test.loc[:, test] = pd.DataFrame(
            index=[model],
            columns=[test],
            data=len(
                fun_multindex(
                    df, [("VARIABLE", test)]  # ('MODEL',model+'_downscaled'),
                )
                .index.get_level_values("REGION")
                .unique()
            ),
        )
        print(
            model,
            test,
            len(
                fun_multindex(
                    df, [("VARIABLE", test)]  # ('MODEL',model+'_downscaled'),
                )
                .index.get_level_values("REGION")
                .unique()
            ),
        )
    df_test_all = pd.concat([df_test_all,df_test])
    # df_test_all = df_test_all.append(df_test)
    df_test_all = df_test_all.dropna(how="all")
    print(df_test_all)
    return


def fun_shape_df_step5(model, df, model_dict, myindex_names):
    df = (df.droplevel("REGION").reset_index()).rename(columns={"ISO": "REGION"})
    df["MODEL"] = [model_dict.get(i, model) for i in df.REGION]
    df.set_index(myindex_names, inplace=True)
    return df


def fun_remove_non_biomass_ren_nomenclature(df, primary_ren=['Geothermal', 'Hydro','Solar', 'Wind']):
    nigem_nomenclature_dict = {
        f"Primary Energy|Non-Biomass Renewables|{x}": f"Primary Energy|{x}"
        for x in primary_ren
    }
    df = df.rename(index=nigem_nomenclature_dict)
    return df


def fun_add_twn_results(_scen_dict, df_ssp, model, targets, df, cols, target):
    cols = range(2010, 2105, 5)
    cols = [str(i) for i in cols]

    for target in targets:
        ssp_scen = fun_get_ssp(target, default_ssp="SSP2", _scen_dict=_scen_dict)
        df_ssp_twn = fun_df_ssp_twn(df_ssp, model, ssp_scen)
        if (
            len(
                fun_multindex(
                    df,
                    [
                        ("REGION", "TWN"),
                        ("SCENARIO", target),
                        ("VARIABLE", "Population"),
                    ],
                )
            )
            == 0
        ):
            setindex(df_ssp_twn, False)
            df_ssp_twn["SCENARIO"] = target
            setindex(df_ssp_twn, ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
            df = pd.concat([df,df_ssp_twn[cols]])
            # df = df.append(df_ssp_twn[cols])
    return df


def fun_df_ssp_twn(df_ssp, model, ssp_scen):
    df_ssp_twn = pd.DataFrame()
    df_ssp_twn = df_ssp[
        (df_ssp.ISO == "TWN")
        & (df_ssp.VARIABLE == "Population")
        # & (df_ssp.SCENARIO.str.contains("SSP2"))
        & (df_ssp.SCENARIO.str.contains(ssp_scen))
    ]
    df_ssp_twn = df_ssp_twn.rename(columns={"ISO": "REGION"})
    if model == "GCAM5.3_NGFS":
        df_ssp_twn["MODEL"] = model
    elif "_downscaled" not in model:
        df_ssp_twn["MODEL"] = model + "_downscaled"
    return df_ssp_twn


def fun_downscale_var_using_proxi(
    df: pd.DataFrame,
    df_iam_all_models: pd.DataFrame,
    model: str,
    targets: list,
    df_countries: pd.DataFrame,
    regions: list,
    tup_dict: dict,
):
    """Downscsale variables in  `tup_dict.keys()` based on proxi variables `tup_dict.values()`

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe with downscaled results
    df_iam_all_models : pd.DataFrame
        IAMs results
    model : str
        IAM e.g. MESSAGE
    targets : list
        Scenario e.g. h_cpol
    df_countries : pd.DataFrame
        Regional-Country mapping
    regions : list
        list of regions
    tup_dict : dict
        Dictionry defining variables to be downscaled as proxi

    Returns
    -------
    _type_
        Updated downscaled dataframe
    """
    for var, var_proxi in tup_dict.items():
        if var not in df_iam_all_models.VARIABLE.unique():
            print(
                f"Cannot find {var} in `df_iam_all_models.VARIABLE`. We skip this variable"
            )
            return df
        if var_proxi in df_iam_all_models.VARIABLE.unique():
            try:
                unit_used = df_iam_all_models[
                    df_iam_all_models.VARIABLE == var
                ].UNIT.unique()[0]
                for region in regions:
                    myreg = region + "r"
                    countrylist = df_countries[
                        df_countries.REGION == region.replace(f"{model}|", "") + "r"
                    ].ISO.tolist()
                    ## Step 0) Country level share of Final Energy Industry within region (% within region). We call this variable => var
                    ## We create new variables Only for downscaled results (not if not native regions)
                    if len(countrylist) >= 2:
                        df = fun_create_variable_ratio(
                            df,
                            _new_var_name=var,
                            _models_list=[f"{model}_downscaled"],
                            _num_var_list=var_proxi,
                            _den_var_list=var_proxi,
                            _iso_list=countrylist,
                            _den_sum_all_countries=True,
                            _unit=unit_used,
                        )

                        ## Step 1) Regional value
                        for target in targets:
                            reg_val = fun_read_reg_value(
                                model, myreg, target, var, df_iam_all_models
                            )["VALUE"]
                            reg_val.index = [str(i) for i in reg_val.index]

                            index0 = (
                                fun_multindex(df, [("REGION", countrylist)])
                                .xs(target, level="SCENARIO", drop_level=False)
                                .xs(var, level="VARIABLE", drop_level=False)
                                .index
                            )

                            ## Step 2) multiply reg_val by country shares
                            df.loc[index0, :] = df.loc[index0, :] * reg_val
            except Exception:
                pass
        else:
            print(
                f"We could not find the variable {var_proxi} in `df_iam_all_models.VARIABLE`. We skip this variable"
            )
    return df


def fun_fix_upper_lower_case_in_regions(
    df: pd.DataFrame, df_mapping: pd.DataFrame, coerce_errors: bool
) -> pd.DataFrame:
    """Fix regions name (upper/lower) case in `df_mapping` (to be consistent with region name in `df`)

    Parameters
    ----------
    df : pd.DataFrame
        Snapshot with IAMs results
    df_mapping : pd.DataFrame
        Regional-country mapping file
    coerce_errors: bool
        Coerce errors and action input (e.g. ask if user whises to continue)
    Returns
    -------
    pd.DataFrame
        Updated Regional-country mapping file (with updated region name)

    Raises
    ------
    ValueError
        If there is a mismatch between regions in the `df_mapping` file and the regions in IAM snapshot `df` file
    """
    model_name = df.reset_index().MODEL.unique()[0]
    df_regions_init = [
        x.replace(f"{model_name}|", "") for x in df.reset_index().REGION.unique()
    ]
    mapping_regions = list(df_mapping[f"{model_name}.REGION"].unique())
    mapping_regions = [x for x in mapping_regions if type(x) is str]
    mapping_regions = [x.replace("_marker", "") for x in mapping_regions]

    df_regions = [x for x in df_regions_init if x in mapping_regions]

    df_regions.sort()
    mapping_regions.sort()

    missing_regions = [x for x in mapping_regions if x not in df_regions]
    if len(missing_regions) and not coerce_errors:
        action = input(
            f"{missing_regions} present in df_mapping but missing in IAM snapshot :{df_regions_init}.\n"
            "Do yo wish to continue (y/n)?"
        )
        if action.lower() not in ["yes", "y"]:
            raise ValueError(f"Simulation aborted by the user (user input={action})")
        # NOTE exclude missing regions (do not rename missing regions)
        mapping_regions = [x for x in mapping_regions if x not in missing_regions]
    rename_dict = dict(zip(mapping_regions, df_regions))
    errors_dict = {
        i: j
        for i, j in rename_dict.items()
        if i.lower().replace("_marker", "") != j.lower().replace("_marker", "")
    }
    if len(errors_dict):
        txt_error = "Error in renaming regions with different lower/upper case letters, in snapshot vs mapping file"
        print(f"{txt_error}: {errors_dict}")
    else:
        df_mapping = df_mapping.replace(rename_dict)
    return df_mapping


def fun_create_iea_dict_from_iea_flow_dict(var_list: list, iea_flow_dict: dict) -> dict:
    """Creates an iea dictionary from `iea_flow_dict` used in step1 (defined in fixtures.py)

    Parameters
    ----------
    var_list : list
        List of variables you are interested in
    iea_flow_dict : dict
        `iea_flow_dict` used in step1 (defined in fixtures.py)

    Returns
    -------
    dict
        IEA dictionary

    Raises
    ------
    ValueError
        If variable is not found in iea_flow_dict.keys()
    """
    res = {}
    for var in var_list:
        if var in var_list:
            flow = iea_flow_dict[var][0]
            product = iea_flow_dict[var][1]
            if type(flow) is not list:
                # flow = flow[0]
                flow = [flow]
            if type(product) is not list:
                product = [product]
            res[var] = {"flow": flow, "product": product}
        else:
            raise ValueError(f"{var} not found in `iea_flow_dict.keys()`")
    return res


def fun_anchor_emi_to_hist_data_step3(
    input_file, countrylist, df_return, iea_dict, var, t_list
):
    unit = df_return.xs(var, level="VARIABLE").reset_index().UNIT.unique()
    if len(unit) > 1:
        raise ValueError(
            f"We detected multiple units for {var}:  unit={unit}. Please check your dataframe "
        )

    elif len(unit) == 1:
        print("We load historical energy data")
        df_iea = fun_read_hist_energy()
    else:
        print(
            "No unit found, we assume you are trying to harmonize Emissions data (not energy data)"
        )
        df_iea = fun_read_hist_emissions()

    df_return.columns = [str(x) for x in df_return.columns]
    idxname = df_return.index.names
    for x in t_list:
        df_return = fun_anchor_single_var_to_hist_data(
            df_return.rename_axis(index={"ISO": "REGION"}),
            var,
            # df_iea.loc[countrylist],
            df_iea[df_iea.index.isin(countrylist)],
            iea_dict,
            countrylist,
            baseyear=str(x),
        )
    df_return = df_return.rename_axis(index={"REGION": "ISO"})
    df_return = df_return.reset_index().set_index(idxname)

    df_return.columns = [int(x) for x in df_return.columns]
    return df_return


def fun_get_historical_emissions(
    var: str, df_emi: pd.DataFrame, iea_var_dict: dict
) -> pd.DataFrame:
    """Get historical emissions across all countries for a give variable `var` and a dataframe with IEA data `df_emi`, and a `iea_var_dict` (to get FLOW and PRODUCT associated to a given `var`)

    Parameters
    ----------
    var : str
        Emissions variable that you want to get
    df_emi : pd.DataFrame
        Dataframe with IEA data
    iea_var_dict : dict
        Dictionary with IEA flow and product associated to var

    Returns
    -------
    pd.DataFrame
        Dataframe with selected `var` across all countries and all time periods
    """
    iso_list = [
        x for x in df_emi.index.get_level_values("ISO").unique() if type(x) is str
    ]

    df_sliced = df_emi[
        (df_emi.index.get_level_values("FLOW").isin(iea_var_dict[var]["flow"]))
        & (df_emi.index.get_level_values("PRODUCT").isin(iea_var_dict[var]["product"]))
        & (df_emi.index.get_level_values("ISO").isin(iso_list))
    ]

    for col in df_sliced.columns:
        df_sliced.loc[:, col] = pd.to_numeric(df_sliced.loc[:, col], errors="coerce")
    return df_sliced.groupby("ISO").sum()


def fun_countries_within_max_std_across_models_scenarios(
    var: str,
    df: pd.DataFrame,
    country_data: pd.DataFrame,
    _baseyear: str = "2010",
    max_std: float = 0.075,
    worst_countries: bool = False,
) -> list:
    """_summary_

    Returns a list of countries for which we have high (or low, if `worst_countries=True`) confidence in data (low standard deviation across models/scenarios) for a given variable `var` and a given `_baseyear`
    ----------
    var : str
        Variable to be checked
    df : pd.DataFrame
        dataframe with downscaled results
    country_data : pd.DataFrame
        Historical emissions data (at the country level) from IEA
    _baseyear : str, optional
        Base year, to check data, by default "2010"
    max_std : float, optional
        Maximum standard deviation (threshold), by default 0.075
    worst_countries : bool, optional
        Wheter we want to return a list with the worst countries instead (largest standard deviation), by default False

    Returns
    -------
    list
        List of countries for which we have high confidence in the data (standard deviation across model/scenario is below the `max_std` threshold). If worst_countries=True, we return countries with highest standard deviation instead
    """
    check = np.abs(
        df.xs(var, level="VARIABLE")[_baseyear].droplevel(["MODEL", "SCENARIO", "UNIT"])
        / country_data
        - 1
    )
    check = check.dropna()
    check = check.groupby(level=0).std().sort_values()

    if worst_countries:
        return list(
            check[check > max_std].sort_values(ascending=False).iloc[:20].index.unique()
        )
    else:
        return list(check[check < max_std].index.sort_values().unique())


def fun_countries_within_max_hist_deviation(
    var: str,
    df: pd.DataFrame,
    country_data: pd.DataFrame,
    _baseyear: str = "2010",
    max_hist_dev: float = 0.17,
    worst_countries: bool = False,
):
    """List of countries in line with historical data (within a maximum threshold `max_hist_dev`), for a given variable `var` and a given `_baseyear`, for all available model/scenarios.

    Parameters
    ----------
    var : str
        Variable to be checked
    df : pd.DataFrame
        Dataframe with downscaled data
    country_data : pd.DataFrame
        Dataframe with IEA historical data at the country level
    _baseyear : str, optional
        Base year, by default "2010"
    max_hist_dev : float, optional
        Maximum percentage deviation allowed (e.g. 0.05 means 5% deviation), by default 0.17
    worst_countries : bool, optional
        Wheter we want to look at the worst countries instead (the ones with largest deviation), by default False

    Returns
    -------
    _type_
        List of countries in line with historical data (for all available models/scenarios). If `worst_countries=True` returns list of worst countries instead (countries with largest deviations compared to historical data).
    """
    check = np.abs(
        df.xs(var, level="VARIABLE")[_baseyear].droplevel(["MODEL", "SCENARIO", "UNIT"])
        / country_data
        - 1
    )
    check = check.dropna()

    # backlist are countries with large deviation (in any model/scenario)
    blacklist = check[check > max_hist_dev].index.unique()
    if worst_countries:
        return list(
            check[check.index.isin(blacklist)]
            .sort_values(ascending=False)
            .iloc[:20]
            .index.unique()
        )
    else:
        return list(check[~check.index.isin(blacklist)].sort_values().index.unique())


def fun_add_iso_column(
    df: pd.DataFrame,
    country_column: str,
    add_dict: Union[dict, None] = None,
    missing_iso=np.nan,
):
    """Adds an 'ISO' column to the `df`, given a column with country names `country_column`, based on pycountry.
    It returns the updated `df`.

    Parameters
    ----------
    df : pd.DataFrame
        initial dataframe with country names
    country_column : str
        columns with the country names
    add_dict : Union[dict,None], optional
        Additional dictionary that can be used to rename country names to dictionaries, by default None
    """

    country_iso_dict = {
        "Bolivia": "BOL",
        "Cape Verde": "CPV",
        "Central African Rep.": "CAF",
        "Cote d'Ivoire": "CIV",
        "Czech Republic": "CZE",
        "Dem. Rep. Korea": "PRK",
        "Democratic People's Republic of the Korea": "PRK",
        "Democratic Republic of the Congo": "COD",
        "Dem. Rep. Congo (DRC)": "COD",
        "Guinea Bissau": "GNB",
        "Iran": "IRN",
        "Lao People's Dem. Rep.": "LAO",
        "Micronesia": "FSM",
        "Micronesia (Federated States of)": "FSM",
        "Republic of Korea": "KOR",
        "Republic of Moldova": "MDA",
        "Saint Vincent and Grenad.": "VCT",
        "Swaziland": "SWZ",
        "Macedonia": "MKD",
        "The former Yugoslav Republic of Macedonia": "MKD",
        "United Rep. of Tanzania ": "TZA",
        "United Republic of Tanzania ": "TZA",
        "United States of America": "USA",
        "Venezuela": "VEN",
    }
    if add_dict is None:
        add_dict = {}
    countries = {country.name: country.alpha_3 for country in pycountry.countries}
    countries.update(country_iso_dict)
    countries.update(add_dict)
    # codes = [countries.get(country, 'Unknown code') for country in input_countries]
    df["ISO"] = [countries.get(x, missing_iso) for x in df[country_column]]
    missing_iso = df[df.ISO == np.nan][country_column].unique()
    if len(missing_iso):
        print("Cannot find iso code for the following countries:")
        print(missing_iso)
    return df


def fun_read_lulucf_country_data(file: str) -> pd.DataFrame:
    """Reads historical LULUCF data breaking out direct and indirect fluxes.
    Parameters
    ----------
    file : str
        Path to file (with historical LULUCF inventories)
    Returns
    -------
    _type_
        Dataframe with histotical LULUCF inventories
    """
    df_lulucf = pd.read_csv(file)
    df_lulucf = fun_add_iso_column(df_lulucf, "Region", missing_iso=999)
    # Below we select country level data only (we remove aggregates)
    df_lulucf = df_lulucf[df_lulucf.ISO != 999]
    df_lulucf.columns = [x.upper() for x in df_lulucf.columns]
    df_lulucf = df_lulucf.set_index(["MODEL", "ISO", "VARIABLE", "UNIT"])
    df_lulucf = df_lulucf.drop(["REGION", "AV. 2000-2020"], axis=1)
    df_lulucf = df_lulucf.rename({"MtCO2/y": "Mt CO2/yr"})
    return df_lulucf


def fun_get_list_of_countries_high_low_confidence(
    var: str,
    baseyear: Union[str, float],
    df: pd.DataFrame,
    df_iea_all_countries: pd.DataFrame,
    worst_countries: bool = False,
) -> list:
    """List of countries for which we have high (`worst_countries`=False) or low (`worst_countries`=True) confidence in data, for a given variable `var` and a given `baseyear`

    Parameters
    ----------
    var : str
        Variable to be checked
    baseyear : Union[str, float]
        Base year
    worst_countries : bool, by default False
        Whether we want to get a list of good countries with high (worst_countries=False) or low (worst_countries=True) confidence in data
    df : pd.DataFrame
        Dataframe with downscaled results
    df_iea_all_countries : pd.DataFrame
        Dataframe with IEA results (alredy sliced for a given variable)

    Returns
    -------
    list
        List of countries
    """

    if worst_countries:
        print("List of countries with low confidence in data")
    else:
        print("List of countries with high confidence in data")

    fun_dict = {
        "max_hist_dev": fun_countries_within_max_hist_deviation,
        "max_std_dev": fun_countries_within_max_std_across_models_scenarios,
    }

    dict_arguments = {
        "var": var,
        "df": df,
        "country_data": df_iea_all_countries[str(baseyear)],
        "_baseyear": str(baseyear),
        "worst_countries": worst_countries,
    }
    # NOTE: we store results in a dictiory:
    # - res["max_hist_dev"] contains list of countries with small (large) historical deviations
    # - res["max_std_dev"] contains list of countries with small (large) standard deviation across model/scenarios
    res = {i: fun(**dict_arguments) for i, fun in fun_dict.items()}

    # NOTE: We combine the two lists above into one (we intersect or add the two lists, depending on how many countries we get)
    if len(res["max_std_dev"]) >= 50:
        # we already have many countries -> use intersection
        final_list = list(set(res["max_hist_dev"]).intersection(res["max_std_dev"]))
    else:
        # we don't have many countries, add the two lists
        final_list = unique(res["max_hist_dev"] + res["max_std_dev"])
    return final_list


def fun_read_hist_energy():
    df = (
        pd.read_csv(
            CONSTANTS.INPUT_DATA_DIR / "Extended_IEA_en_bal_2019_ISO.csv",
            sep=",",
            encoding="latin",
            dtype={"REGION": str, "ISO": str},
        )
        .set_index(["ISO", "FLOW", "PRODUCT"])
        # .drop(["Unnamed: 0", "COUNTRY", "TIME", "REGION"], axis=1)
    )
    df = fun_drop_columns(df, case_sensitive=False, col_names=["Unnamed:", "COUNTRY", "TIME", "REGION"])
    cols = [str(x) for x in range(1950, 2100) if str(x) in df.columns]
    return df[cols]


def fun_split_snapshot_by_model(
    project: str,
    model_folders: Union[None, list],
    file_name: Union[None, str, pd.DataFrame],
) -> list:
    """Splits a dataframe that contains multiple IAMs results into single dataframes for each individual models.

    Parameters
    ----------
    project : str
        Project folder e.g. "NGFS_2022"
    model_folders : Union[None, list]
        List of models with individual IAMs results
    file_name : Union[None, str, pd.Dataframe]
        File name (or dataframe) with all IAMs results

    Returns
    -------
    list
        List of folders where to find individual df for each model

    Raises
    ------
    ValueError
        If both file_name and model_folders are None.
    ValueError
        If unable to find  file_name.csv
    """
    if file_name is None and model_folders is None:
        raise ValueError(
            "Please provide a `file_name` (currently `file_name` is None) with regional IAMs results"
            "As an alternative please provide a list of `model_folders` (currently `model_folders` is None) with the regional results for each model, to be found in the directory `input_data/{project}`"
        )
    if model_folders is None:
        if isinstance(file_name, str):
            if not os.path.exists(f"input_data/{project}/{file_name}.csv"):
                if not os.path.exists(f"input_data/{project}/{file_name}"):
                    raise ValueError(
                        f"Cannot find `{file_name}.csv`  in the directory `input_data/{project}`.\n"
                    )
                else:
                    # NOTE do not change the order of functions in fun loop
                    # (first function returns None, second one returns a df):
                    for fun in (
                        fun_find_all_xlsx_files_and_convert_to_csv,
                        fun_concatenate_all_df_in_folder,
                    ):
                        df = fun(f"input_data/{project}/{file_name}")
                    df = fun_clean_up_df_iam(
                        df, replace_scenario_w_target=False, add_r=False
                    )

            else:
                df = fun_snapshot(project, None, name=file_name)
        else:
            df = file_name

        myidex = df.index.names
        df = df.reset_index()
        df["MODEL"] = [x.replace("/", "_") for x in df.MODEL]
        df = df.set_index(myidex)
        model_folders = df.index.get_level_values("MODEL").unique()
        model_folders = [x.replace("/", "_") for x in model_folders]
        for model in model_folders:
            mydir = f"input_data/{project}/{model}"
            if not os.path.isdir(mydir):
                os.mkdir(mydir)

            if isinstance(file_name, pd.DataFrame):
                df.xs(model, drop_level=False).to_csv(
                    f"{mydir}/snapshot_all_regions_{model}-data.csv"
                )
            else:
                df.xs(model, drop_level=False).to_csv(
                    f"{mydir}/snapshot_all_regions_RAW_{model}.csv"
                )

    return model_folders


def fun_get_variables_for_selected_step(
    step1: bool, step2: bool, step3: bool, step5: bool
):
    """Get a dictionary with variables to be checked (depending on which steps we are running)

    Parameters
    ----------
    step1 : bool
        Wheter you want to run step1
    step2 : bool
        Wheter you want to run step2
    step3 : bool
        Wheter you want to run step3
    step5 : bool
        Wheter you want to run step5

    Returns
    -------
    dict
        Dictionary with variables to be checked (depending on which steps we are running)
    """
    var_check_dict = {"Price": price_var}
    if step1:
        var_check_dict["Step1"] = step1_var
    if step2:
        var_check_dict["Step2"] = step2_var
    if step3:
        var_check_dict["Step3"] = step3_var
    if step5:
        var_check_dict["Step5"] = step5b_var
    return var_check_dict


def fun_interpolate_and_abs_ccs_sequestration(
    df: pd.DataFrame, model: str
) -> pd.DataFrame:
    """Insert missing year columns  in `df` (e.g. if year 2065 is missing) and interpolates data (e.g. 2065 data will be calculated as a linear interpolation between 2060 and 2070 data)

    Parameters
    ----------
    df : pd.DataFrame
        IAMs results
    model : str
        model name e.g. MESSAGE

    Returns
    -------
    pd.DataFrame
        Updated dataframe
    """
    print("inserting columns and interpolating...")
    df = fun_insert_missing_cols_interpolate(df)

    print("Calculating ccs sequestration in absolute value...")
    try:
        df = fun_ccs_sequestration_in_absolute_value(
            df,
            seq_var,
        )
    except Exception as e:
        print(model, e)
    return df


def fun_save_csv_step0(
    save_to_csv: bool, model: str, df: pd.DataFrame, df_all: pd.DataFrame, mydir: str
) -> pd.DataFrame:
    """Save dataframe to csv

    Parameters
    ----------
    save_to_csv : bool
        Wheter you want to save your `df` to csv (one for each model). If False will return a `df_all` with all IAMs results
    model : str
        model name
    df : pd.DataFrame
        IAMs results to be saved (single model)
    df_all : pd.DataFrame
        IAMs results to be returned (all models)
    mydir : str
        Where to save your csv file (directory)

    Returns
    -------
    pd.DataFrame
        Returns dataframe that we just saved to csv
    """
    file_name=f"{mydir}/snapshot_all_regions_RAW_{model}.csv"
    
    # If a file already exists, keep all existing scenarios that are not present in the current `df`
    if os.path.exists(file_name):
        df_append=fun_read_csv({'a':file_name}, True, int)['a'].reset_index().set_index(df.index.names)
        df=fun_index_names(df, True, int)
        myscen=list(df.reset_index().SCENARIO.unique())
        df_append=fun_xs(df_append,{'SCENARIO':myscen}, exclude_vars=True)
        existing_scen=df_append.reset_index().SCENARIO.unique()
        print(f'Will add these updated scenarios {myscen}, to the existing scenarios {existing_scen} in `df_iam` ')
        df=pd.concat([df,df_append])

    # Save or return the dataframe
    if save_to_csv:
        if not os.path.exists(mydir):
            os.mkdir(mydir)
        df.to_csv(file_name)
        df_all = df
    else:
        df_all = pd.concat([df_all, df], axis=0)
    return df_all


def check_step0_input(dfname: str):
    """Checks for df file name

    Parameters
    ----------
    df : str
        Name of the dataframe

    Raises
    ------
    ValueError
        If the name of the dataframe is "snapshot_all_regions"
    """
    if dfname == "snapshot_all_regions":
        raise ValueError(
            "You cannot use a file called `snapshot_all_regions.csv` to be split across models "
            "(because the algorithm will at some point will create a csv file called `snapshot_all_regions.csv` as an output)."
            "Please rename your file using a different name."
        )


def check_regions_step0(
    df_mapping: pd.DataFrame,
    df: pd.DataFrame,
    coerce_error: bool = True,
):
    """Check missing region for (the only) model present in `df` and updated `var_check_dict`

    Parameters
    ----------
    df_mapping : pd.DataFrame
        Regional-country mapping
    df : pd.DataFrame
        Dataframe with IAMs results
    var_check_dict : dict
        Dictionary with missing regions (to be updated for each model). NOTE `df` only contains one model
    coerce_error: bool, optional
        Wheter you want to coerce missing region errors
    Returns
    -------
    dict
        Updated `var_check_dict`
    """
    missing_regions, regions_in_snapshot = fun_check_missing_regions(
        df, df_mapping, exclude_regions_with_one_country=True
    )
    regions_excl_r5_r10 = [
        x for x in regions_in_snapshot if "(R10)" not in x if "(R5)" not in x
    ]
    if len(regions_excl_r5_r10):
        regions_in_snapshot = regions_excl_r5_r10
    models = df.reset_index().MODEL.unique()
    if len(models) > 1:
        raise ValueError(f"df contains more than one model: {models}")
    if missing_regions:
        if coerce_error:
            print(f"Regions are missing {missing_regions}")
        else:
            raise ValueError(f"Regions are missing {missing_regions}")
    else:
        print(f"{models[0]}: All good with the regions")
    return {
        "missing countries": missing_regions,
        "available countries (in df_iam snapshot)": regions_in_snapshot,
    }


def fun_check_variables_by_step(
    var_check_dict: dict, model: str, df: pd.DataFrame, check_by="REGION"
) -> dict:
    """Check missing variables in IAMs results for each step

    Parameters
    ----------
    var_check_dict : dict
        Dictionary with list of variables for each step
    model : str
        Model name e.g. MESSAGE
    df : pd.DataFrame
        Dataframe with regional IAMs results
    check_by: str, optional
        Wheter you want to report missing variables by 'REGION' or 'SCENARIO', by default 'REGION'

    Returns
    -------
    dict
        Dictionary with list of missing variables by model/step
    """
    check_single_model = {}
    print(model)
    model_name = df.reset_index().MODEL.unique()[0]
    for i, j in var_check_dict.items():
        print(i)
        check_single_model[i] = fun_check_variables(df, j, check_by=check_by)
        if check_single_model[i].keys():
            model_name = list(check_single_model[i].keys())[0]
            model_name = model_name.replace("/", "_")
            # print(check_single_model[i][model_name].keys())
            print(check_single_model[i].keys())
    return check_single_model


def df_merge_with_historical_data(
    df: pd.DataFrame,
    df_emi: pd.DataFrame,
    vars: list,
    iea_var_dict: dict,
    drop_df_columns: list = ["2010", "2015"],
    unit_conversion: float = 1,
) -> pd.DataFrame:
    df_merged_all = pd.DataFrame()
    hist_data_all_countries_all = pd.DataFrame()
    myidxnames = df.index.names
    if len(vars):
        for var in vars:
            print(f"Add hist data: {var} -  {vars.index(var)}/{len(vars)}")
            hist_data_all_countries = (
                fun_get_historical_emissions(var, df_emi, iea_var_dict)
                * unit_conversion
            )
            hist_data_all_countries.index.names = ["REGION"]

            for x in drop_df_columns:
                if x in df.columns:
                    df = (
                        df.reset_index()
                        .set_index("REGION")
                        .drop(drop_df_columns, axis=1)
                    )

            # Select columns in `df` and `hist_data_all_countries`
            # NOTE: we get historical data only if a given year is not present in `df` -> hist_cols
            base_year = min(int(x) for x in df.columns if str(x)[0] in ["1", "2"])
            hist_cols = [
                x for x in hist_data_all_countries.columns if int(x) < base_year
            ]

            # merge historical data with IAMs
            df_merged = (
                pd.merge(
                    df, hist_data_all_countries[hist_cols], how="outer", on="REGION"
                )
                .reset_index()
                .set_index(df.index.names)
            )
            df_merged = df_merged.reset_index().set_index(myidxnames)
            if "CRITERIA" in df_merged.columns:
                df_merged = df_merged.drop("CRITERIA", axis=1)

            df_merged.columns = [int(x) for x in df_merged.columns]
            sort_cols = [int(x) for x in df_merged.columns]
            sort_cols.sort()
            df_merged = df_merged[sort_cols].xs(var, level="VARIABLE", drop_level=False)
            df_merged_all = pd.concat([df_merged_all, df_merged])
            hist_data_all_countries["VARIABLE"] = var
            hist_data_all_countries_all = pd.concat(
                [hist_data_all_countries_all, hist_data_all_countries]
            )
        return hist_data_all_countries, df_merged_all


def fun_plot_downscaled_results(
    palette: str,
    df_merged: pd.DataFrame,
    var: str,
    countrylist: list,
    ls: str = "-",
    xlist: Union[str, None, range] = None,
    title: Union[str, None] = None,
    subtitle: Union[str, None] = None,
    add_legend: Union[bool, None] = True,
    add_model_styles: Union[bool, None] = False,
    add_scenario_styles: Union[bool, None] = False,
    figsize=(14, 8.5),
) -> plt:
    """Plots `df_merged` (that contains dataframe with downscaled results merged with historical data) for a given variable `var`
    and a list of countries `countrylist` (each country with a different color).

    Parameters
    ----------
    model : Union[None, str]
        Model that you want to plot. If None will plot all the models (same color)
    palette : str
        cmap palette
    df_merged : pd.DataFrame
        Dataframe with downscaled results and historical data
    var : str
        Variable to plot e.g. 'Emissions|CO2|Energy'
    countrylist : list
        List pf countries to plot (each country with a different color)
    ls : str, optional
        Linestyle, by default "-"

    Returns
    -------
    plt
        Plt graph
    """

    countrylist, df_graph, pal, xlist = fun_prepare_data_for_graph(
        palette, df_merged, var, countrylist, xlist
    )

    all_colors, coldict = fun_get_one_colour_for_each_country(
        countrylist, df_graph, pal
    )

    fig, ax = fun_create_fig_and_add_titles(title, subtitle, figsize=figsize)
    myidx = ["REGION", "MODEL", "SCENARIO"]

    # plt.fill_between(df_graph.xs('GP_Glasgow', level='SCENARIO').columns,
    # df_graph.xs('GP_Glasgow', level='SCENARIO').xs('AUT', level='REGION').min(axis=0), df_graph.xs('GP_Glasgow', level='SCENARIO').xs('AUT', level='REGION').max(axis=0))
    ax = (
        df_graph.reset_index()
        .set_index(myidx)
        .droplevel(["MODEL", "SCENARIO"])[xlist]
        .T.plot(color=all_colors, ls=ls, ax=ax)
    )

    models = df_graph.reset_index().set_index("MODEL").index.unique()
    line_cycle = ["-", "--", "-.", ":"]
    ls_dict = dict(zip(models, cycle(line_cycle)))

    scenarios = df_graph.reset_index().set_index("SCENARIO").index.unique()
    marker_cycle = [str(x) for x in range(1, 1 + len(scenarios))]
    marker_dict = dict(zip(scenarios, cycle(marker_cycle)))

    # for country in countrylist:
    #     for model in models:
    #         for scen in scenarios:
    #             ax = (
    #                 df_graph.reset_index()
    #                 .set_index(myidx).xs(model, level='MODEL').xs(scen, level='SCENARIO')[xlist]
    #                 .T
    #             ).plot(
    #                 marker=marker_dict.get(scen, None),
    #                 ls=ls_dict.get(model, "-"),
    #                 ax=ax,
    #                 color=coldict[country],
    #             )

    # Stop matplotlib-repeating-labels-in-legend:
    plt.legend(
        *[*zip(*{l: h for h, l in zip(*ax.get_legend_handles_labels())}.items())][::-1]
    )
    if not add_legend:
        ax.get_legend().remove()
    ax.set(xlabel="years", ylabel=df_graph.reset_index().UNIT.unique()[0])

    return ax


def fun_get_one_colour_for_each_country(
    countrylist: list, df_graph: pd.DataFrame, pal: str, level="REGION"
) -> Union[list, dict]:
    """Assigns one colour for each country from a given colour palette

    Parameters
    ----------
    countrylist : list
        List of countries
    df_graph : pd.DataFrame
        Your dataframe
    pal : str
        Chosen color palette

    Returns
    -------
    Union[list, dict]
        List of colours and dicitonary with {country:colour}
    """
    pal = sns.color_palette(pal, len(countrylist), as_cmap=False)

    all_colors = dict(zip(countrylist, pal.as_hex()))

    all_colors = [all_colors[c] for c in all_colors]
    all_colors = fun_flatten_list(
        [
            [x for _ in range(len(df_graph.xs(countrylist[0], level=level)))]
            for x in all_colors
        ]
    )

    return all_colors, dict(zip(countrylist, pal.as_hex()))


def fun_stackplot_downscaled_results(
    model: str,
    palette: str,
    df_merged: pd.DataFrame,
    var: str,
    countrylist: list,
    scenario: str,
    xlist: Union[str, None, range] = None,
    title: Union[str, None] = None,
    subtitle: Union[str, None] = None,
    add_legend: bool = True,
    interpolate: bool = True,
    figsize: tuple = (14, 8.5),
) -> plt:
    """Plots `df_merged` (that contains dataframe with downscaled results merged with historical data) for a given variable `var`
    and a list of countries `countrylist` (each country with a different color).

    Parameters
    ----------
    model : Union[None, str]
        Model that you want to plot. If None will plot all the models (same color)
    palette : str
        cmap palette
    df_merged : pd.DataFrame
        Dataframe with downscaled results and historical data
    var : str
        Variable to plot e.g. 'Emissions|CO2|Energy'
    countrylist : list
        List pf countries to plot (each country with a different color)
    ls : str, optional
        Linestyle, by default "-"

    Returns
    -------
    plt
        Plt graph
    """

    countrylist, df_graph, pal, xlist = fun_prepare_data_for_graph(
        palette, df_merged, var, countrylist, xlist
    )

    fig, ax = fun_create_fig_and_add_titles(title, subtitle, figsize)

    df_graph = df_graph.xs(scenario, level="SCENARIO").loc[model][xlist]

    if interpolate:
        df_graph = df_graph.interpolate(axis=1)
    ax.stackplot(
        xlist,
        df_graph,
        colors=pal,
    )
    ax.set(xlabel="years", ylabel=df_graph.reset_index().UNIT.unique()[0])
    legend_dict = dict(zip(countrylist, sns.color_palette(palette, len(countrylist))))
    fun_add_legend_to_graph(countrylist, add_legend, legend_dict)
    return plt


def fun_add_legend_to_graph(countrylist: list, add_legend: bool, legend_dict: dict):
    """Adds a legend to a graph (list of countries)

    Parameters
    ----------
    countrylist : list
        list of countries
    add_legend : bool
        Whether you want to add a legend
    legend_dict : dict
        Dictionary with countries and colors (to be used in the legend)
    """
    if add_legend:
        if len(countrylist) < 30:
            plt.legend(legend_dict, loc=[1.02, 0.03])
        else:
            plt.legend(
                legend_dict, loc="upper left", ncol=len(countrylist) // 15, fontsize="8"
            )


def fun_create_fig_and_add_titles(title: str, subtitle: str, figsize: tuple):
    """Create a plt.figur and adds titles

    Parameters
    ----------
    title : str
        Main title
    subtitle : str
        Subtitle
    figsize : tuple
        Figure size

    Returns
    -------
    Union[plt.fig, plt.axes]
        Update figure and axes
    """
    fig, ax = plt.subplots(figsize=figsize)

    # Title below
    if title is not None:
        fig.text(
            0.08,
            0.97,
            title,
            fontsize=15,
            fontname="Montserrat",
            weight="bold",
        )

    # subtitle
    if subtitle is not None:
        fig.text(
            0.08,
            0.91,
            "\n".join([subtitle]),
            ha="left",
            color="grey",
            fontname="Montserrat",
            fontsize=9,
            wrap=True,
        )

    return fig, ax


def fun_prepare_data_for_graph(palette, df_merged, var, countrylist, xlist):
    if countrylist is not None:
        df_graph = df_merged[
            df_merged.index.get_level_values("REGION").isin(countrylist)
        ]
    else:
        df_graph = df_merged

    countrylist = df_graph.index.get_level_values("REGION").unique()
    pal = sns.color_palette(palette, len(countrylist), as_cmap=False)

    df_graph = df_graph.xs(var, level="VARIABLE")
    if xlist is None:
        xlist = df_merged.columns
    else:
        xlist = [x for x in xlist if x in df_merged.columns]

    return countrylist, df_graph, pal, xlist


def fun_plot_fill_between(
    palette: str,
    df_merged: pd.DataFrame,
    var: str,
    countrylist: list,
    by: str = "REGION",
    ls: str = "-",
    xlist: Union[str, None, range] = None,
    title: Union[str, None] = None,
    subtitle: Union[str, None] = None,
    add_legend: Union[bool, None] = True,
    figsize=(14, 8.5),
    alpha=0.5,
) -> plt:
    """Plots `df_merged` (that contains dataframe with downscaled results merged with historical data) for a given variable `var`
    and a list of countries `countrylist` (each country with a different color).

    Parameters
    ----------
    model : Union[None, str]
        Model that you want to plot. If None will plot all the models (same color)
    palette : str
        cmap palette
    df_merged : pd.DataFrame
        Dataframe with downscaled results and historical data
    var : str
        Variable to plot e.g. 'Emissions|CO2|Energy'
    countrylist : list
        List pf countries to plot (each country with a different color)
    ls : str, optional
        Linestyle, by default "-"

    Returns
    -------
    plt
        Plt graph
    """

    countrylist, df_graph, pal, xlist = fun_prepare_data_for_graph(
        palette, df_merged, var, countrylist, xlist
    )

    all_colors, coldict = fun_get_one_colour_for_each_country(
        countrylist, df_graph, pal
    )

    fig, ax = fun_create_fig_and_add_titles(title, subtitle, figsize=figsize)
    myidx = ["REGION", "MODEL", "SCENARIO"]

    for x in df_graph.index.get_level_values(by).unique():
        df_sel = df_graph.xs(x, level=by).interpolate(axis=1)[xlist]
        ax = plt.fill_between(
            df_sel.columns,
            df_sel.min(axis=0),
            df_sel.max(axis=0),
            alpha=alpha,
            color=coldict.get(x, "black"),
        )

    plt.xlabel("years")
    plt.ylabel(df_graph.reset_index().UNIT.unique()[0])
    legend_dict = dict(zip(countrylist, sns.color_palette(palette, len(countrylist))))
    fun_add_legend_to_graph(countrylist, add_legend, legend_dict)

    # Stop matplotlib-repeating-labels-in-legend:
    try:
        plt.legend(
            *[*zip(*{l: h for h, l in zip(*ax.get_legend_handles_labels())}.items())][
                ::-1
            ]
        )
    except:
        pass

    if not add_legend:
        ax.get_legend().remove()
    return plt


def fun_add_hist_data(
    df: pd.DataFrame,
    idx_col: list,
    emi_vars: list,
    energy_vars: Union[None, list],
    iea_flow_dict: dict,
) -> pd.DataFrame:
    # Add standard criteria, only if there is no "CRITERIA" column in the dataframe
    df = fun_add_criteria("standard", df).reset_index().set_index(idx_col)

    myidx = list(df.index.get_level_values("MODEL").unique())
    df = df.rename(dict(zip(myidx, [x.replace("_downscaled", "") for x in myidx])))
    df = df.set_index(["CRITERIA", "FILE"], append=True)

    # Add historical (energy and emissions) data
    (
        hist_data_all_countries,
        df_merged,
    ) = fun_combine_with_hist_emissions_and_energy_data(
        df,
        iea_flow_dict,
        emi_vars=emi_vars,
        energy_vars=energy_vars,
    )

    # Add variables for which we don't have historical data
    df.columns = [int(x) for x in df.columns]
    var_wo_hist_data = [x for x in df.index if x not in df_merged.index]
    df_merged = pd.concat([df_merged, df.loc[var_wo_hist_data]], axis=0)

    return df_merged, hist_data_all_countries


def fun_combine_with_hist_emissions_and_energy_data(
    df, iea_flow_dict, emi_vars, energy_vars=None
):
    # Step 1 - Read energy data
    df_merged_en = None
    if energy_vars is None:
        # All energy data
        energy_vars = fun_get_energy_vars_from_iea_flow_dict(
            df.reset_index().VARIABLE.unique(), iea_flow_dict
        )
    iea_energy_dict = fun_create_iea_dict_from_iea_flow_dict(energy_vars, iea_flow_dict)

    if len(energy_vars):
        hist_data_all_countries, df_merged_en = df_merge_with_historical_data(
            df,
            fun_read_hist_energy(),
            energy_vars,
            iea_energy_dict,
            drop_df_columns=["2010", "2015"],
            unit_conversion=0.041868 / 1e3,
        )

    # Step 2 - Read historical emissions data
    df_emi = fun_read_hist_emissions()
    hist_data_all_countries, df_merged_emi = df_merge_with_historical_data(
        df,
        df_emi,
        emi_vars,
        iea_flow_dict,
        drop_df_columns=["2010", "2015"],
        unit_conversion=1 / 1e3,
    )

    # Step3 - combine the data
    if df_merged_en is not None:
        df_merged = pd.concat([df_merged_emi, df_merged_en], axis=0)
    else:
        df_merged = df_merged_emi
    return hist_data_all_countries, df_merged


def fun_get_energy_vars_from_iea_flow_dict(all_vars: list, iea_flow_dict: dict) -> list:
    """Get energy variables from iea_flow_dict

    Parameters
    ----------
    all_vars : list
        List of variables
    iea_flow_dict : dict
        Dictionary with iea flow/product definitions

    Returns
    -------
    list
        List of energy variables
    """
    energy_vars = [
        x
        for x in iea_flow_dict.keys()
        if iea_flow_dict[x][2] in ["ktoe", ["ktoe"]]
        if x in all_vars
    ]

    return energy_vars


def fun_read_df_iam_from_multiple_df(model: str, datadir: Path) -> pd.DataFrame:
    """Find df_iam (dataframe with IAM results) for a given `model`, in a `datadir`
       folder (with multiple dataframes from different models).

    Parameters
    ----------
    model : str
        Model that you are looking for (e.g `WITCH 5.0`)
    datadir : Path
        Folder with IAMs reuluts e.g. `input_data/EU_climate_advisory_board/multiple_df`

    Returns
    -------
    pd.DataFrame
        Dataframe with selected IAM moldel results (at the regional level)

    Raises
    ------
    ValueError
        _description_
    """
    flag = 0
    file_list = fun_sort_list_order_based_on_element_name(
        list(datadir.iterdir()), model
    )
    while flag < 1:
        for file in file_list:
            df_iam = pd.read_csv(file)
            df_iam.columns = [x.upper() for x in df_iam.columns]
            model_read = (
                df_iam.reset_index().MODEL.unique()[0].replace("_downscaled", "")
            )
            if model == model_read:
                flag = 1
                break
            if file == list(datadir.iterdir())[-1]:
                raise ValueError(f"Cannot find df_iam for {model} model")
    df_iam = df_iam.set_index(["MODEL", "REGION", "VARIABLE", "UNIT", "SCENARIO"])
    return df_iam


def fun_industrial_process_emi_for_aggregated_region(
    aggregated_region_name: str,
    iam_regions: list,
    countrylist: list,
    df_all: pd.DataFrame,
    df_iam: pd.DataFrame,
    # marker="marker",
) -> pd.DataFrame:
    """Calculated industrial process emissions for an aggregated region, by using Final Energy|Industry as proxi.
    (assumes that `industrial process emissions` is proportional to `Final Energy|Industry`).
    We use the formula below (where aggregated region = EU27):
    Ind. Process emissions (Eu27)= Final Energy (Eu27) / Final Energy (markers region) * Ind. Process emissions (markers regions)

    Parameters
    ----------
    aggregated_region_name : str
        _description_
    countrylist : list
        _description_
    df_all : pd.DataFrame
        _description_
    df_iam : pd.DataFrame
        _description_

    Returns
    -------
    pd.DataFrame
        _description_
    """
    if (
        "Emissions|CO2|Industrial Processes"
        not in df_iam.reset_index().VARIABLE.unique()
    ):
        df_iam = fun_create_var_as_sum(
            df_iam,
            "Emissions|CO2|Industrial Processes",
            {
                "Emissions|CO2|Energy and Industrial Processes": 1,
                "Emissions|CO2|Energy": -1,
            },
            _level="VARIABLE",
            unit="Mt CO2-equiv/yr",
        )
    num = (
        df_all[df_all.REGION.isin(countrylist)]
        .set_index(df_iam.index.names)
        .xs(
            "Final Energy|Industry",
            level="VARIABLE",
        )
        .groupby(["MODEL", "UNIT", "SCENARIO"])
        .sum()
    )
    den = (
        df_iam[df_iam.index.get_level_values("REGION").isin(iam_regions)]
        .xs("Final Energy|Industry", level="VARIABLE")
        .groupby(["MODEL", "UNIT", "SCENARIO"])
        .sum()
    )
    eu27_industry_ratio = (num / den).droplevel("UNIT")
    reg_ind_process_emissions = (
        df_iam[df_iam.index.get_level_values("REGION").isin(iam_regions)]
        .xs("Emissions|CO2|Industrial Processes", level="VARIABLE")
        .droplevel("REGION")
        .groupby(["MODEL", "UNIT", "SCENARIO"])
        .sum()
    )
    eu27_ind_process_emi = eu27_industry_ratio * (reg_ind_process_emissions).droplevel(
        "UNIT"
    )
    eu27_ind_process_emi["REGION"] = aggregated_region_name
    eu27_ind_process_emi["VARIABLE"] = "Emissions|CO2|Industrial Processes"
    eu27_ind_process_emi["UNIT"] = "Mt CO2-equiv/yr"
    return pd.concat([df_all, eu27_ind_process_emi.reset_index()], sort=True)


def fun_sns_lineplot(
    df_graph,
    models,
    palette_by="tab10",
    criteria: Union[None, str] = "standard",
    hue: str = "SCENARIO",
):
    # df_graph = df_merged.xs(var, level="VARIABLE").xs("EU27", level="REGION")
    df_graph = fun_select_criteria_if_available(criteria, df_graph)
    df_graph = df_graph.reset_index()
    for x in ["UNIT", "FILE"]:
        if x in df_graph.columns:
            df_graph = df_graph.drop(x, axis=1)
    df = df_graph.melt(["MODEL", "SCENARIO"])

    str_in_value = [x for x in df.value if type(x) is str]
    if len(str_in_value):
        raise ValueError(
            f"We have detected strings in the melted dataframe {set(str_in_value)}. "
            "Please consider slicing your dataframe (e.g. by region / variable)"
        )
    pal = sns.color_palette(palette_by, 10, as_cmap=False)
    color_list = pal.as_hex()

    # color_list=["seagreen", '#FF0000', 'blue', 'orange', 'purple', 'brown', 'pink']
    col_dict = dict(zip(models, cycle(color_list)))
    palettes = [
        sns.light_palette(
            col_dict[model], n_colors=len(df[df.MODEL == model][hue].unique())
        )
        for model in models
    ]

    pal_dict = dict(zip(models, cycle(palettes)))
    sns.set(
        rc={
            "axes.facecolor": "white",
            "figure.facecolor": "white",
            "grid.color": "#EDEDED",
            "axes.edgecolor": "grey",
        }
    )
    for model in models:
        myfig = sns.lineplot(
            x="variable",
            y="value",
            data=df[df.MODEL == model],
            hue=hue,
            legend=False,
            palette=pal_dict[model],
            alpha=0.9,
            linewidth=1,
        )
    # Creating legend with color box
    mylegend = [mpatches.Patch(color=col_dict[model], label=model) for model in models]
    plt.legend(models, handles=mylegend)

    # BLUEPRINT FOR adding bars with ranges
    # https://github.com/IAMconsortium/pyam/blob/main/pyam/plotting.py
    # line 1129
    # and https://stackoverflow.com/questions/5021663/how-to-draw-a-line-outside-of-an-axis-in-matplotlib-in-figure-coordinates
    axs = myfig.axes
    # plt.axvline(2.8, 0,0.17)
    return plt


def prepare_data_for_sns_lineplot(df, criteria):
    df_graph = fun_select_criteria_if_available(criteria, df)
    df_graph = df_graph.reset_index()
    for x in ["UNIT", "FILE"]:
        if x in df_graph.columns:
            df_graph = df_graph.drop(x, axis=1)
    value_vars = [i for i in df.columns if type(i) == int]
    df = df_graph.melt(
        id_vars=["MODEL", "SCENARIO"], value_vars=value_vars, var_name="YEAR"
    )

    str_in_value = [x for x in df.value if type(x) is str]
    if len(str_in_value):
        raise ValueError(
            f"We have detected strings in the melted dataframe {set(str_in_value)}. "
            "Please consider slicing your dataframe (e.g. by region / variable)"
        )
    return df


def fun_sns_lineplot_new(
    df,
    title="",
    y_axis_label="",
    grouping_dim=None,
    palette_by="husl",
    # criteria: Union[None, str] = "standard",
    hue: str = None,
    linestyle_dim="SCENARIO",
    ax=None,
    intersect_algo=False,
    all_in_one_legend=False,
    legend_color_box: Optional[str] = "lower left",
    legend_ls_box: Optional[str] = "upper right",
    lw: float = 2,
    alpha: float = 0.5,
    col_dict: dict = {},
    marker=None,
    # marker='o',
) -> plt:
    sns.set(
        rc={
            "axes.facecolor": "white",
            "figure.facecolor": "white",
            "grid.color": "#EDEDED",
            "axes.edgecolor": "grey",
            # "figure.figsize": (5,10),
        }
    )
    if not ax:
        plt.gcf().canvas.draw()
        axs = plt.gcf().axes
        ax = axs[0]

    if not grouping_dim:
        if hue == "SCENARIO":
            grouping_dim = "MODEL"
            linestyle_dim = "SCENARIO"
        elif hue == "MODEL":
            grouping_dim = "SCENARIO"
            linestyle_dim = "MODEL"
        else:
            raise ValueError(
                f"If `grouping_dim` is None, `hue` needs to be `SCENARIO` or `MODEL`. You specified hue: {hue}"
            )

    group_names = df[grouping_dim].unique()

    if grouping_dim == "SCENARIO":
        linestyle_dim = "MODEL"

    # color settings
    pal = sns.color_palette(palette_by, len(group_names), as_cmap=False)
    color_list = pal.as_hex()
    col_dict = dict(zip(group_names, cycle(color_list)))
    import matplotlib.colors

    pal_dict = {k: list(matplotlib.colors.to_rgb(v)) for k, v in col_dict.items()}

    variables = df.variable.unique() if "variable" in df.columns else []

    # To explore how it works:  CHECK HERE: jupyter_notebooks/exploring_sns_plots.ipynb!!!!

    show_legend = (
        True if legend_ls_box or legend_color_box or all_in_one_legend else False
    )
    for var in variables:
        ax = sns.lineplot(
            x="YEAR",
            y="value",
            data=df[(df.variable == var)],
            hue=grouping_dim,
            # palette=pal_dict[model],
            # alpha=1,
            legend=show_legend,
            linewidth=lw,
            alpha=alpha,
            style=linestyle_dim,
            ax=ax,
            palette=col_dict,  # .get(name,'black'),
            marker=marker,
        )

    # Code below avoids repeating labels:
    start = 0 if all_in_one_legend else len(df[grouping_dim].unique()) + 1
    select = (
        len(df[linestyle_dim].unique()) + len(df[grouping_dim].unique()) + 2
    )  # 2 because we have two dimentions:[grouping_dim, linestyle_dim]
    h, l = ax.get_legend_handles_labels()
    h = h[start:select]
    l = l[start:select]

    # # below avoids duplicate legends
    # h, l = ax.get_legend_handles_labels()
    # mydict=dict(zip(l, h))
    # l,h=list(mydict.keys()), list(mydict.values())
    # ax.legend(handles, labels, bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0)
    ax.legend(h, l, bbox_to_anchor=(0.025, 0.025), loc=3, borderaxespad=0)

    for name in group_names:
        upper = df[df[grouping_dim] == name].groupby("YEAR").max()["value"].values
        lower = df[df[grouping_dim] == name].groupby("YEAR").min()["value"].values
        if len(upper):
            ax.fill_between(
                [float(i) for i in df["YEAR"].unique()],
                upper,
                lower,
                alpha=0.15,
                color=pal_dict[name],
            )

        # below avoids duplicate legends
        handles, labels = ax.get_legend_handles_labels()
        mydict = dict(zip(labels, handles))
        labels, handles = list(mydict.keys()), list(mydict.values())

    # Add bars to the end of the plot showing range
    # have to explicitly draw it to get the tick labels (these change once you add the vlines)
    ax = fun_add_right_bars(df, grouping_dim, ax, intersect_algo, pal_dict)

    # for equal spacing between xmin and first datapoint and xmax and last line

    # create linestyle legend
    # h, l = ax.get_legend_handles_labels()
    if legend_ls_box:
        legend_dict = dict(zip(l, h))
        # ls_legend = ax.legend(
        #     df[linestyle_dim].unique(), linestyle_dim, loc=legend_ls_box
        # )
        ls_legend = ax.legend(
            legend_dict.values(), legend_dict.keys(), loc=legend_ls_box
        )
        ax.add_artist(ls_legend)
    # Creating legend with color box
    if legend_color_box:
        mylegend = [
            mpatches.Patch(color=col_dict[name], label=name) for name in group_names
        ]
        ax.legend(group_names, handles=mylegend, loc=legend_color_box)
    # set labels/title
    ax.set_title(title)
    ax.set_ylabel(y_axis_label)

    return ax


def fun_add_right_bars(df, grouping_dim, ax, intersect_algo, pal_dict):
    first = df["YEAR"].min()
    final = df["YEAR"].max()
    mins = df[df["YEAR"] == 2050].groupby(grouping_dim).min()
    maxs = df[df["YEAR"] == 2050].groupby(grouping_dim).max()
    ymin, ymax = ax.get_ylim()
    ydiff = ymax - ymin

    xmin, xmax = df.YEAR.min(), df.YEAR.max()
    xdiff = xmax - xmin
    xticks = ax.get_xticks()
    xlabels = ax.get_xticklabels()

    extra_space = 0.015  # 1.5% increase seems to be ok per extra line
    _kwargs = {"linewidth": 5}
    y_pos_set = []
    offset = -1
    for i, idx in enumerate(mins.index):
        _ymin = (mins.loc[idx, "value"] - ymin) / ydiff
        _ymax = (maxs.loc[idx, "value"] - ymin) / ydiff
        if intersect_algo:
            offset = 0
            # print(idx, _ymax, _ymin)
            intersect = False
            for line in reversed(y_pos_set):
                if _ymin > line["min"]:
                    # print("higher")
                    if _ymin < line["max"]:
                        print("intersect")
                        intersect = True
                else:
                    if _ymax > line["min"]:
                        intersect = True
                if intersect:
                    offset = line["offset"] + 1
                    break
        else:
            offset += 1
        xpos = final + xdiff * extra_space * (offset + 1)
        y_pos_set.append({"max": _ymax, "min": _ymin, "offset": offset})
        line = ax.axvline(xpos, ymin=_ymin, ymax=_ymax, color=pal_dict[idx], **_kwargs)
        line.set_clip_on(False)
    ax.set_xlim(xmin, xmax)
    return ax


def fun_select_model_scenarios_below_threshold(
    df_merged: pd.DataFrame, reg: str, var: str, year: int, threshold: float
) -> pd.DataFrame.index:
    """Slice dataframe `df_merged` if a given variable `var` is below a `threshold` for a given `year` in a given region `reg`.
    It returns a dataframe index (excluding the Variable name)

    Parameters
    ----------
    df_merged : pd.DataFrame
        Dataframe with data
    reg : str
        Selected region/iso
    var : str
        Variable to be avaluated against threshold
    year : int
        In which year we look at the threshold value
    threshold : float
        Your threshold value

    Returns
    -------
    pd.DataFrame.index
        Dataframe index with model/scenarios/regions below selected threshold in a given year
    """
    for x in ["FILE", "UNIT", "CRITERIA"]:
        if x in df_merged.index.names:
            df_merged = df_merged.droplevel(x)
    df_sel = df_merged.xs(var, level="VARIABLE")
    if reg in df_sel.reset_index().REGION.unique():
        df_sel = df_sel.xs(reg, level="REGION", drop_level=False)
        return df_sel[df_sel[year] < threshold].index


def fun_calculate_kyoto_gases(df_all, idxname):
    myvars = [
        # "Emissions|CO2|Energy",
        # "Emissions|CO2|Industrial Processes",
        # "Emissions|CO2|LULUCF Direct+Indirect",
        "Emissions|CO2",
        "Emissions|Total Non-CO2",
    ]

    if df_all.index.names != idxname:
        if len(df_all.index.names):
            df_all = df_all.reset_index()
        df_all = df_all.set_index(idxname)

    if len(myvars) == len(
        [x for x in myvars if x in df_all.reset_index().VARIABLE.unique()]
    ):
        if (
            "Emissions|Kyoto Gases (incl. indirect AFOLU)"
            not in df_all.reset_index().VARIABLE
        ):
            df_all = fun_create_var_as_sum(
                df_all,
                "Emissions|Kyoto Gases (incl. indirect AFOLU)",
                myvars,
                _level="VARIABLE",
                unit="Mt CO2-equiv/yr",
                # dropna=False
            )

    else:
        raise ValueError(
            "Cannot calculate Kyoto Gases, some variables required are missing"
        )
    if df_all.index.names != idxname:
        df_all = df_all.set_index(idxname)

    return df_all


def fun_read_primap(
    folder: Path,
    # file: str = "Guetschow-et-al-2021-PRIMAP-hist_v2.3.1_no_extrap_no_rounding_20-Sep_2021.csv",
    file:str='Guetschow_et_al_2024-PRIMAP-hist_v2.5.1_final_no_extrap_no_rounding_27-Feb-2024.csv'
) -> pd.DataFrame:
    """Reads PRIMAP dataset

    Parameters
    ----------
    folder : Path
        Folder where to find PRIMAP dataset
    file : str
        Primap file name

    Returns
    -------
    pd.DataFrame
        Primap Dataset
    """
    # NOTE: source:https://zenodo.org/records/10705513
    # -----------------------------------------------------------------------
    # Category code Description
    # ------------- ---------------------------------------------------------
    # M.0.EL        National Total excluding LULUCF
    # 1             Energy
    # 1.A           Fuel Combustion Activities
    # 1.B           Fugitive Emissions from Fuels
    # 1.B.1         Solid Fuels
    # 1.B.2         Oil and Natural Gas
    # 1.B.3         Other Emissions from Energy Production
    # 1.C           Carbon Dioxide Transport and Storage
    #             (currently no data available)
    # 2             Industrial Processes and Product Use (IPPU)
    # 2.  A         Mineral Industry
    # 2.B           Chemical Industry
    # 2.C           Metal Industry
    # 2.D           Non-Energy Products from Fuels and Solvent Use
    # 2.E           Electronics Industry
    #             (no data available as the category is only used for
    #             fluorinated gases which are only resolved at the level
    #             of category IPC2)
    # 2 F           Product uses as Substitutes for Ozone Depleting Substances
    #             (no data available as the category is only used for
    #             fluorinated gases which are only resolved at the level
    #             of category IPC2)
    # 2.G           Other Product Manufacture and Use
    # 2.H           Other
    # M.AG          Agriculture, sum of IPC3A and IPCMAGELV
    # 3.A           Livestock
    # M.AG.ELV      Agriculture excluding Livestock
    # 4             Waste
    # 5             Other
    # -----------------------------------------------------------------------


    # -----------------------------------------------------------------------
    # The "no_rounding" version of the dataset additionally contains data for the "Land Use, Land Use Change, and Forestry" sector and sectors sums including this sectors
    # Code 	        Description 	                                    Gases covered
    # 0 	        National Total including LULUCF 	                CO2, CH4, N2O, f-gases
    # 3 	        Agriculture, Forestry and Other Land Use (AFOLU) 	CO2, CH4, N2O
    # M.LULUCF 	    Land Use, Land Use Change, and Forestry 	        CO2, CH4, N2O
    # -----------------------------------------------------------------------

    # SCENARIOS:
    # HISTCR: In this scenario country-reported data (CRF, BUR, UNFCCC) is prioritized over third-party data (CDIAC, FAO, Andrew, EDGAR, BP).
    # HISTTP: In this scenario third-party data (CDIAC, FAO, Andrew, EDGAR, BP) is prioritized over country-reported data (CRF, BUR, UNFCCC)

    # NOTE (Fabio):
    # HISTCR: Emissions|Kyoto Gases (incl. indirect AFOLU) = 40.6 GtCO2e in 2010 Globally   (MESSAGE = 52.6 GtCO2e)
    # HISTTP  Emissions|Kyoto Gases (incl. indirect AFOLU) = 47.3 GtCO2e in 2010 Globally   (MESSAGE = 52.6 GtCO2e)
    
    df = pd.read_csv(folder / file)
    df["MODEL"] = "Primap"
    df = df.rename(
        columns={
            "scenario (PRIMAP-hist)": "SCENARIO",
            "area (ISO3)": "REGION",
            "unit": "UNIT",
        }
    )
    myidx = [
        "MODEL",
        "REGION",
        "SCENARIO",
        # "area (ISO3)",
        "entity",
        "category (IPCC2006_PRIMAP)",
        # "scenario (PRIMAP-hist)",
        "source",
        "UNIT",
    ]
    df = df.set_index(myidx)
    if 'provenance' in df.columns:
        df=df.drop('provenance', axis=1)
    df.columns = [int(x) for x in df.columns]
    # Rename units
    units_dict={f'{x} * gigagram / a':f'Gg {x} / yr' for x in ['CO2','N20','CH4','NF3','SF6']}
    return df.rename(units_dict)


def fun_ghg_emi_from_primap(
    iso_list: Union[None, list],
    primap_dict: Optional[dict] = None,
    myrange: range = range(1990, 2023),
    scenario: str = "HISTCR",
) -> pd.DataFrame:
    """Returns historical ghg data from PRIMAP for selected countries (`iso_list`) and for a give
    time period range (`myrange`)

    # NOTE (Fabio):
    # HISTCR: Emissions|Kyoto Gases (incl. indirect AFOLU) = 40.6 GtCO2e in 2010 Globally   (MESSAGE = 52.6 GtCO2e)
    # HISTTP  Emissions|Kyoto Gases (incl. indirect AFOLU) = 47.3 GtCO2e in 2010 Globally   (MESSAGE = 52.6 GtCO2e)
    # Better to use HISTTP as the default (Global difference with MESSAGE is consistent with Grassi Issue)

    Parameters
    ----------
    iso_list : Union[None,list]
        List of ISO code (countries) for which you want to get historical data.
        If None will return data for all countries
    primap_dict: dict, optional
        Dictionary of Primap variables to be reported (in IAMc format). Can be imported from downscaler.fixtures
    myrange : range, optional
        Range of historical time periods, by default range(1990, 2020)
    scenario: str
        Scenario from primap db (by default country-reported data is prioritized over third-party data), by default 'HISTCR'
    Returns
    -------
    pd.DataFrame
        Dataframe with historical GHG emissions from PRIMAP
    """

    # file = (
    #     "Guetschow-et-al-2021-PRIMAP-hist_v2.3.1_no_extrap_no_rounding_20-Sep_2021.csv"
    # )
    file='Guetschow_et_al_2024-PRIMAP-hist_v2.5.1_final_no_extrap_no_rounding_27-Feb-2024.csv'
    primap = fun_index_names(fun_read_primap(CONSTANTS.INPUT_DATA_DIR, file), True, int)
    selcols = [int(x) for x in myrange]
    primap = primap.iloc[:, primap.columns.isin(selcols)]
    if not iso_list:
        iso_list = list(primap.reset_index().REGION.unique())

    # NOTE:  M.0.EL -> National Total excluding LULUCF source:https://zenodo.org/record/7585420#.Y_8dWx_MKUk
    if primap_dict is None:
        primap_dict = {
            "Emissions|Kyoto Gases excl. LULUCF": {
                "entity": "KYOTOGHG (AR4GWP100)",
                "category (IPCC2006_PRIMAP)": "M.0.EL",
            },  #  National Total excluding LULUCF
            "Emissions|CO2|LULUCF Direct+Indirect": {
                "entity": "KYOTOGHG (AR4GWP100)",
                "category (IPCC2006_PRIMAP)": "M.LULUCF",  # LULUCF
            },
            "Emissions|CO2 excl. LULUCF": {
                "entity": "CO2",
                "category (IPCC2006_PRIMAP)": "M.0.EL",  #  National Total excluding LULUCF
            },
            "Emissions|CO2|Energy": {
                "entity": "CO2",
                "category (IPCC2006_PRIMAP)": "1",  # Energy
            },
            "Emissions|CO2|Industrial Processes": {
                "entity": "CO2",
                "category (IPCC2006_PRIMAP)":["2.A",'2.B','2.C', '2.E']  # As discussed here: https://github.com/iiasa/ngfs-internal-workflow/discussions/136 
            },
        }
    df_all = pd.DataFrame()
    for var, primap_value in primap_dict.items():
        df_single_var = primap
        for k, v in primap_value.items():
            # df_single_var = fun_xs(df_single_var, {"REGION": iso_list}).xs(v, level=k)
            df_single_var = fun_xs(df_single_var, {"REGION": iso_list})
            if k in df_single_var.index.names:
                group=[x for x in df_single_var.index.names if x!=k]
                df_single_var= fun_xs(df_single_var, {k: v}).groupby(group).sum()
            
        df_single_var["VARIABLE"] = var
        df_single_var["FILE"] = file
        df_all = pd.concat([df_all, df_single_var.reset_index()], axis=0)
    iamc = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT", "FILE"]

    unit = df_all.UNIT.unique()
    if len(unit) == 1 and unit[0] == "Gg CO2 / yr":
        df_all = fun_index_names(df_all, sel_idx=iamc) / 1e3
    else:
        raise ValueError(
            f"Please check the unit of primap db. We expect `Gg CO2 / yr` we found: {unit} "
        )

    # Create emissions including LULUCF
    if (
        "Emissions|Kyoto Gases (incl. indirect AFOLU)"
        not in df_all.reset_index().VARIABLE.unique()
    ):
        df_all = fun_create_var_as_sum(
            df_all,
            "Emissions|Kyoto Gases (incl. indirect AFOLU)",
            [
                "Emissions|Kyoto Gases excl. LULUCF",
                "Emissions|CO2|LULUCF Direct+Indirect",
            ],
            unit=unit[0],
        )

    if "Emissions|CO2" not in df_all.reset_index().VARIABLE.unique():
        df_all = fun_create_var_as_sum(
            df_all,
            "Emissions|CO2",
            ["Emissions|CO2 excl. LULUCF", "Emissions|CO2|LULUCF Direct+Indirect"],
            unit=unit[0],
        )

    # RENAME index
    unit_dict = {
        "kyoto": {
            "Mt CO2-equiv/yr": [
                x for x in df_all.reset_index().VARIABLE.unique() if "Kyoto" in x
            ]
        },
        "co2": {
            "Mt CO2/yr": [
                x for x in df_all.reset_index().VARIABLE.unique() if "CO2" in x
            ]
        },
    }
    df_all_renamed = pd.DataFrame()
    if "UNIT" not in df_all.reset_index().columns:
        df_all["UNIT"] = "missing"
        df_all = df_all.set_index("UNIT", append=True)
    for ren_dict in unit_dict.values():
        for new_unit, variables in ren_dict.items():
            temp = fun_xs(df_all, {"VARIABLE": variables})
            curr_unit = temp.reset_index().UNIT.unique()[0]
            temp = temp.rename(index={curr_unit: new_unit})
            df_all_renamed = pd.concat([df_all_renamed, temp], axis=0)
    # we append all the remaining variables (that are not Kyoto nor co2)
    renamed_var = df_all_renamed.reset_index().VARIABLE.unique()
    not_renamed_var = [
        x for x in df_all.reset_index().VARIABLE.unique() if x not in renamed_var
    ]
    df_all_renamed = pd.concat(
        [df_all_renamed, fun_xs(df_all, {"VARIABLE": not_renamed_var})], axis=0
    )
    return df_all_renamed.xs(scenario, level="SCENARIO", drop_level=False)


def fun_plot_eu_ab_side_by_side(
    df,
    var,
    c,
    t,
    figsize=(30, 10),
    ylim_top=4100,
    color="blue",
    sel_model=None,
    sel_model_color="red",
    alpha=0.25,
    add_title=False,
    marker_2030: Union[dict, None] = None,
    marker_2050: Union[dict, None] = None,
    legend_elements=[
        Line2D([0], [0], color="black", lw=4, label="Historical"),
        Line2D([0], [0], color="b", lw=4, label="Compliant"),
        Line2D([0], [0], color="grey", lw=4, label="Non-compliant"),
    ],
):
    #
    # [x] can you please change the transparency of the blue, so that we can see the grey behind?
    # [x] Also - make the 2010-2019 black so that it looks different.
    # [x] Remove grey background.
    # [x] Remove title
    # [x] Add marker at 55% in 2030
    # [x] Add marker at 300 Mt in 2050 Marker in green - upside-down triangle
    # [x] Add horizontal dashed black line at x=0
    # [x] axis labels are too small
    # [ ] Add legend

    title = var if add_title else None
    title = add_title if isinstance(add_title, str) else title

    dfgraph = df.xs(var, level="VARIABLE").xs(c, level="REGION")

    sel_scenarios = fun_available_scen(dfgraph)
    unit = dfgraph.reset_index().UNIT.unique()[0]
    fig = plt.figure()
    try:
        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=figsize)
    except:
        print("not working")
    # LEFT SIDE
    # dfgraph.T.plot(color="blue", legend=False, ax=ax1, ylabel=unit, title=var)
    if sel_model is not None:
        if (
            isinstance(sel_model, str)
            and sel_model in dfgraph.reset_index().MODEL.unique()
        ):
            df_remind = dfgraph.xs(sel_model, level="MODEL", drop_level=False)
        else:
            # if True in [isinstance(x, dict) for x in sel_model.values()]:
            #     for k,v in sel_model:
            # else:
            df_remind = fun_select_model_scenarios_combinations(dfgraph, sel_model)

        ax = df_remind.T.plot(
            color=sel_model_color, alpha=alpha, legend=False, ax=ax1, title=title
        )
        data_showed_as_color = dfgraph[~dfgraph.index.isin(df_remind.index)]
        if len(data_showed_as_color):
            ax1 = dfgraph[~dfgraph.index.isin(df_remind.index)].T.plot(
                color=color,
                alpha=alpha,
                legend=False,
                ax=ax1,
                title=title,
                label="Scenarios",
            )
        ax1.set_facecolor("white")

        # black lines for hist data:
        hist_time = [x for x in dfgraph.columns if len(dfgraph[x].unique()) == 1]
        # plt.axhline(y=0, color='black', linestyle='-', ax=ax1)

        if len(hist_time):
            dfgraph[hist_time].T.plot(
                color="black",
                legend=False,
                ax=ax1,
                fontsize=15,
                label="Historical data",
            )

        if marker_2030 is not None:
            pd.DataFrame([marker_2030]).T.plot(
                legend=False,
                ax=ax1,
                title=title,
                marker="v",  # upside down triangle
                color="green",
                ms=15,
                label="2030 Target",
            )
            legend_elements = legend_elements + [
                Line2D(
                    [0],
                    [0],
                    marker="v",
                    color="green",  # label=f"-55% by 2030"
                    label="NDC target",
                )
            ]

        if marker_2050 is not None:
            pd.DataFrame([marker_2050]).T.plot(
                legend=False,
                ax=ax1,
                title=title,
                marker="^",  # upside down triangle
                color="green",
                ms=15,
                label="2050 Target",
            )
            legend_elements = legend_elements + [
                Line2D(
                    [0], [0], marker="^", color="green", label="Net-zero GHGs by 2050"
                )
            ]

        # Set dashed vertical line at zero
        pd.DataFrame([{t: 0 for t in dfgraph.columns[:-1]}]).T.plot(
            legend=False, ax=ax1, title=title, color="black", ls="--"
        )

        # handles, labels = ax.get_legend_handles_labels()
        # ax.legend(handles=handles, loc='upper center')

        # Adding legend

        # Create the figure legend
        ax.legend(handles=legend_elements, loc="upper right")

    else:
        ax = dfgraph.T.plot(color=color, legend=False, ax=ax1, title=title, alpha=alpha)
    ax.set_ylabel(unit, fontdict={"fontsize": 18})
    if ylim_top is not None:
        ax.set_ylim(top=ylim_top)
    # RIGHT SIDE:
    fun_bar_plot(var, t, dfgraph, sel_scenarios, ax2)
    figname = var.replace("|", "_")
    plt.suptitle(c)

    # rcParams.update({"figure.autolayout": True})

    fun_save_figure(
        fig,
        CONSTANTS.CURR_RES_DIR("step5") / "Step5e_visuals",
        f"{var.replace('|', '_')}_{c}",
    )
    plt.close()
    plt.close()
    return fig


def fun_save_figure(fig: plt, save_fig_folder: str, figname: str, dpi:Optional[int]=None, size_inches:Optional[tuple]=None):
    for x in ['|', '(', ')', '.']:
        figname = figname.replace(x, "_")
    os.makedirs(save_fig_folder, exist_ok=True)
    print(f"Figure will be saved here: {save_fig_folder}")
    h = get_git_revision_short_hash()
    figname='_'.join(figname.replace(" ", "").replace('__','').split('_'))
    # Try to trim `figname` if the path/name exeeds 265 characters
    if len(str(save_fig_folder / f"{figname}_{str(int(time.time()))}_hash_{h}.png")) >260:
        for y in range(1,10):
            figname='_'.join([x[:-y] for x in figname.replace(" ", "").replace('__','').split('_')])
            if len(str(save_fig_folder / f"{figname}_{str(int(time.time()))}_hash_{h}.png")) <260 or x==9:
                break
    # Raise value error if filename is still too long
    if len(str(save_fig_folder / f"{figname}_{str(int(time.time()))}_hash_{h}.png")) >=260:
        raise ValueError('The figure path/name exceeds 264 characters. Please provide a shorter name')
    
    if size_inches:
        figure = plt.gcf() # get current figure
        figure.set_size_inches(size_inches) #e.g. (8,6) means 800x600 image
    try:
        if dpi is not None:
            fig.savefig(save_fig_folder / f"{figname}_{str(int(time.time()))}_hash_{h}.png", dpi=dpi)
        else:
            fig.savefig(save_fig_folder / f"{figname}_{str(int(time.time()))}_hash_{h}.png")
    except:
        # This one works for plotly express (e.g. ternary graphs) 
        # see https://stackoverflow.com/questions/59815797/how-to-save-plotly-express-plot-into-a-html-or-static-image-file 
        fig.write_html(save_fig_folder/ f"{figname}_{str(int(time.time()))}_hash_{h}.html")


def fun_bar_plot(var, t, dfgraph, sel_scenarios, ax):
    clist = [
        (0, "red"),
        (0.125, "red"),
        (0.25, "orange"),
        (0.5, "green"),
        (0.7, "green"),
        (0.75, "blue"),
        (1, "blue"),
    ]
    rvb = mcolors.LinearSegmentedColormap.from_list("", clist)
    N = len(dfgraph)
    x = np.arange(N).astype(float)
    ax = (
        dfgraph[t]
        .droplevel(["UNIT", "FILE"])
        .plot(
            kind="bar",
            xticks=[],
            # xlabel="",
            color=rvb(x / N),
            ax=ax,
            title=f"{var} - {t}",
        )
    )
    ax.set_xlabel("")
    mylegend = [f"{m}_{x}" for m, s in sel_scenarios.items() for x in s]
    h = [
        mpatches.Patch(color=rvb(mylegend.index(model) / N), label=model)
        for model in mylegend
    ]
    # plt.legend(mylegend, handles=h, fontsize=4.8, ncol=1, loc=[1.01, 0])
    plt.legend(mylegend, handles=h, fontsize=8, ncol=1, loc=[1.01, 0])
    plt.xticks(rotation=90)
    plt.tick_params(
        # axis='x',          # changes apply to the x-axis
        # which='both',      # both major and minor ticks are affected
        # bottom=False,      # ticks along the bottom edge are off
        # top=False,         # ticks along the top edge are off
        labelbottom=False
    )  # labels along the bottom edge are off

    return plt


def fun_get_sub_sectors(
    df: pd.DataFrame, var: str, breakdown_by: str = "fuel", same_level: bool = True
):
    if var.split("|")[-1] == "Non-Biomass Renewables":
        fuels = ["Geothermal", "Hydro", "Solar", "Wind", "Ocean"]
        return [f"{var.replace(var.split('|')[-1],'')}{x}" for x in fuels]

    break_dict = {
        "fuel": [
            "Coal",
            "Oil",
            "Gas",
            "Natural Gas",
            "Nuclear",
            "Biomass",
            "Hydro",
            "Geothermal",
            "Solar",
            "Wind",
            "Other",
        ],
        "ec": [
            "Solids",
            "Liquids",
            "Gases",
            "Heat",
            "Electricity",
            "Hydrogen",
        ],
        "gas": [
            # "Emissions|Total Non-CO2",
            # "Emissions|CO2|LULUCF Direct+Indirect",
            # "Emissions|CO2|Industrial Processes",
            # "Emissions|CO2|Energy",
            "CO2",  # "CH4", "N20", "HFC", "SF6"
        ],
        "emi": ["CO2|AFOLU", "CO2|Energy", "CO2|Ind. Processes", "Total Non-CO2"],
        "cdr": [
            "AFOLU",
            "BECCS",
            "Enhanced Weathering",
        ],  #'Land Use'
        "sector": ["Industry", "Transportation", "Residential and Commercial"],
        "all": [],
        "none": [],
    }
    max_sep = var.count("|")
    if breakdown_by not in break_dict.keys():
        raise ValueError(
            f"breakdown needs to be one of the following: {break_dict.keys()}. Your input: {breakdown_by}  "
        )
    if breakdown_by == "none":
        return [var]
    else:
        sub_sectors = [x for x in df.reset_index().VARIABLE.unique() if f"{var}|" in x]
        if not len(sub_sectors):
            sub_sectors = list(
                fun_all_sub_sectors(var, df.reset_index().VARIABLE.unique()).values()
            )
            sub_sectors = fun_flatten_list(sub_sectors)
    if breakdown_by == "all":
        return sub_sectors

    by = break_dict[breakdown_by]
    sel_sub_sectors = []
    max_sep = 1e9
    if len(sub_sectors):
        for b in by:
            sel_sub_sectors = sel_sub_sectors + [
                x for x in sub_sectors if x.endswith(b)
            ]
            try:
                max_sep = min(max_sep, min([x.count("|") for x in sub_sectors]))
            except:
                print("error")
        # Get sectors at a given level (max_sep)
        if same_level:
            sel_sec = []
            for x in sel_sub_sectors:
                if x.count("|") == max_sep:
                    sel_sec = sel_sec + [x]
            return sel_sec
        else:
            return sel_sub_sectors
    return [""]


def fun_all_sub_sectors(var: str, vars: list):
    res = [
        s
        for s in vars
        if np.prod([True if x in s else False for x in var.rsplit("|")])
    ]
    res = [x for x in res if len(x) > len(var)]
    return {var: res}


def plot_stacked_bar(
    data,
    series_labels,
    category_labels=None,
    show_values=False,
    value_format="{}",
    y_label=None,
    colors=None,
    grid=True,
    reverse=False,
):
    """Plots a stacked bar chart with the data and labels provided.

    Keyword arguments:
    data            -- 2-dimensional numpy array or nested list
                       containing data for each series in rows
    series_labels   -- list of series labels (these appear in
                       the legend)
    category_labels -- list of category labels (these appear
                       on the x-axis)
    show_values     -- If True then numeric value labels will
                       be shown on each bar
    value_format    -- Format string for numeric value labels
                       (default is "{}")
    y_label         -- Label for y-axis (str)
    colors          -- List of color labels
    grid            -- If True display grid
    reverse         -- If True reverse the order that the
                       series are displayed (left-to-right
                       or right-to-left)
    """

    ny = len(data[0])
    ind = list(range(ny))

    axes = []
    cum_size = np.zeros(ny)

    data = np.array(data)

    if reverse:
        data = np.flip(data, axis=1)
        category_labels = reversed(category_labels)

    for i, row_data in enumerate(data):
        color = colors[i] if colors is not None else None
        axes.append(
            plt.bar(ind, row_data, bottom=cum_size, label=series_labels[i], color=color)
        )
        cum_size += row_data

    if category_labels:
        plt.xticks(ind, category_labels)

    if y_label:
        plt.ylabel(y_label)

    plt.legend()

    if grid:
        plt.grid()

    if show_values:
        for axis in axes:
            for bar in axis:
                w, h = bar.get_width(), bar.get_height()
                plt.text(
                    bar.get_x() + w / 2,
                    bar.get_y() + h / 2,
                    value_format.format(h),
                    ha="center",
                    va="center",
                )


def fun_bar_plot_stacked(
    df,
    mycolors,
    title,
    ylabel,
    ax,
    row,
    col,
    kind="area",
    ylim=None,
    smart_legend=True,
    colors=None,
):
    for x in ["UNIT", "FILE"]:
        df = df.droplevel(x) if x in df.index.names else df
    df = df.T
    stacked = False if kind == "line" else True

    legend_dict = {x: x.split("|")[-1] for x in df.columns}
    # Below reduce variable names if possible
    # e.g. use Biomass instead of 'Primary|Energy|Biomass'
    if smart_legend and len(legend_dict):
        if len(set([x.count("|") for x in df.columns])) == 1:
            # Shorter variable names (if all variables have same number of delimiters)
            df = df.rename(columns=legend_dict)
    if mycolors is not None:
        clist = [(mycolors.index(x) / (len(mycolors) - 1), x) for x in mycolors]
        rvb = mcolors.LinearSegmentedColormap.from_list("", clist)
        N = len(df)
        x = np.arange(N).astype(float)
        ax = df.plot(
            kind=kind,
            stacked=True,  # color=['red', 'skyblue', 'green']
            ax=ax[row][col],
            color=rvb(x / N),
            title=title,
        )
    else:
        try:
            if colors is not None:
                custom_name = [i for i in df.columns]
                custom_color = [colors.get(i, "black") for i in custom_name]
                ax = df.plot(
                    kind=kind,
                    stacked=stacked,
                    ax=ax[row][col],
                    title=title,
                    color=custom_color,
                )  # fontsize=16)
            else:
                ax = df.plot(kind=kind, stacked=stacked, ax=ax[row][col], title=title)
            if ylim is not None:
                ax.set_ylim(bottom=ylim[0], top=ylim[1])
            if ylabel is not None and ylabel != "":
                ax.set_ylabel(ylabel)
            if title is not None:
                ax.set_title(title, fontsize=15)
            ax.legend(loc=1, fontsize=15)
        except:
            print("not working")

    plt.tight_layout()
    return plt.plot


def fun_scatter_plot(
    df,
    yvar,
    xvar,
    svar,
    ax,
    row,
    col,
    ylim=None,
    mycolors=None,
    title=None,
):
    for x in ["UNIT", "FILE"]:
        if x in df.index.names:
            df = df.droplevel(x)
    df = df.T
    # legend_list = [x.split("|")[-1] for x in df.columns]
    if mycolors is not None:
        clist = [(mycolors.index(x) / (len(mycolors) - 1), x) for x in mycolors]
        rvb = mcolors.LinearSegmentedColormap.from_list("", clist)
        N = len(df)
        x = np.arange(N).astype(float)
        if svar is not None:
            ax = df.plot(
                kind="scatter",
                x=xvar,
                y=yvar,
                ax=ax[row][col],
                color=rvb(x / N),
                s=df[svar],
            )
        else:
            ax = df.plot(
                kind="scatter", x=xvar, y=yvar, ax=ax[row][col], color=rvb(x / N)
            )  # s=df.col3)
    else:
        if svar is not None:
            ax = df.plot(kind="scatter", x=xvar, y=yvar, ax=ax[row][col], s=df[svar])
        else:
            # ax2=plt.plot(df.xs(xvar, level='VARIABLE').T, df.xs(yvar, level='VARIABLE').T, kind='scatter', ax=ax[row][col], title=title)
            ax = df.plot(kind="scatter", x=xvar, y=yvar, ax=ax[row][col])  # s=df.col3)

    if ylim is not None:
        ax.set_ylim(bottom=ylim[0], top=ylim[1])
    # plt.legend(
    #     legend_list, title="Variables", bbox_to_anchor=(1.0, 1), loc="upper left"
    # )
    if title is not None:
        plt.title(title)
    # plt.savefig('stacked.png')  # if needed
    plt.tight_layout()
    return plt.plot


def fun_discount_rate(
    all_time_cols: Union[list, set, tuple],
    hist_time_cols: Union[list, set, tuple],
    tc: Union[None, int],  # time of convergenc
    start,  # value at the base year
    end,  # value at tc time period
):
    if tc is None:
        discount_rate = start
    else:
        discount_dict = {x: start for x in hist_time_cols}
        discount_dict[tc] = end
        discount_rate = pd.DataFrame([discount_dict])
        [
            discount_rate.insert(len(discount_rate.columns), x, np.nan)
            for x in all_time_cols
            if x not in discount_rate.columns
        ]
        discount_rate = discount_rate.sort_index(axis=1).interpolate(axis=1).loc[0]
    return discount_rate


def fun_add_historic_kyoto(df):
    # kyoto_dict = {
    #     "Emissions|Kyoto Gases": "Emissions|Kyoto Gases (incl. indirect AFOLU)"
    # }
    primap = fun_ghg_emi_from_primap(
        fun_eu27(), primap_dict, myrange=range(1990, 2020)
    )  # .rename(kyoto_dict)

    primap = (
        fun_aggregate_countries(
            primap,
            "EU27",
            ["MODEL", "VARIABLE", "UNIT", "SCENARIO", "FILE"],
            fun_eu27(),
            remove_single_countries=False,
        )
        .reset_index()
        .set_index(df.index.names)
    )
    primap.columns = [str(x) for x in primap.columns]
    df = pd.concat([df, primap], axis=0, sort=True)
    return df


def fun_sort_models_based_on_value(df, t=2025):
    if "MODEL" not in df.index.names:
        raise ValueError(
            f"MODEL must be in your index names. You have provided a df with these index names: {df.index.names}"
        )
    mi = df[t].groupby("MODEL").min().sort_values()
    ma = df[t].groupby("MODEL").max().sort_values()
    me = df[t].groupby("MODEL").median().sort_values()
    mi_dict = {n: m for m, n in enumerate((mi.index))}
    ma_dict = {n: m for m, n in enumerate((ma.index))}
    me_dict = {n: m for m, n in enumerate((me.index))}
    d = {m: mi_dict[m] + ma_dict[m] + me_dict[m] for m in mi_dict.keys()}
    return set(dict(sorted(d.items(), key=lambda item: item[1], reverse=True)).keys())


# fun_sort_models_based_on_value(df, t=2025)


def fun_harmonize_hist_data(
    df: pd.DataFrame,
    var: str,
    tc: Union[int, None],
    hist_emi: Union[pd.DataFrame, dict],  # {'2010':10} This would also work
    method: str = "offset",
    clip_positive: bool = False,
    coerce_errors: bool = False,
) -> pd.DataFrame:
    # hist_emi = hist_emi.fillna(0)
    ALLOWED_OPERATORS = {
        "offset": "sub",
        "ratio": "mul",
    }
    action = "y"
    mindata = df.xs(var, level="VARIABLE").min()
    if not coerce_errors:
        if "mission" in var or mindata.min() < 0:
            if method == "offset" and clip_positive:
                action = input(
                    f"are you sure you want to clip {var} as a positive variable? (y/n)"
                    f"Please Note that minimum data found for this variable is: {mindata[mindata<0]}"
                )
            elif method == "ratio":
                action = input(
                    f"are you sure you want use a ratio harmonization for {var} (y/n)?"
                    f"Please Note that minimum data found for this variable is: {mindata[mindata<0]}"
                )
        if action.lower() not in ["yes", "y"]:
            raise ValueError(f"Script aborted by user")

    # NOTE: please consider removing countrylist from this function (probably not needed)
    if isinstance(hist_emi, dict):
        checkregion = list(df.reset_index().REGION.unique())
        errtxt = "`hist_emi` can be a dict only if `df` contains only one region"
        errtxt2 = "Please consider using a dataframe for `hist_emi`"
        if len(checkregion) != 1:
            raise ValueError(f"{errtxt}. `df` contains: {checkregion}. {errtxt2}")
        hist_emi = pd.DataFrame(hist_emi, index=checkregion)
        hist_emi.index.names = ["REGION"]

    hist_emi = fun_rename_index_name(
        hist_emi, {"ISO": "REGION"}
    )  # .loc[countrylist, :]
    hist_emi.columns = [int(x) for x in hist_emi.columns]
    df.columns = [int(x) for x in df.columns]

    cols = set(hist_emi.columns).intersection(df.columns)
    if not len(cols):
        raise ValueError("There are no columns in common in the df and hist_emi data")

    # Automatically gets base year data
    baseyear = max(cols)

    # base year offset
    oper = "-" if method == "offset" else "/"
    adj = (
        fun_add_multiply_dfmultindex_by_dfsingleindex(
            # fun_xs(df, {"REGION": countrylist}).xs(var, level="VARIABLE", drop_level=False),
            df.xs(var, level="VARIABLE", drop_level=False),
            hist_emi,
            operator=oper,
        )[list(cols)]
        .fillna(0)
        .replace(np.inf, 0)
        .replace(-np.inf, 0)
    )

    missing_cols = [x for x in df.columns if x not in adj.columns]

    # Insert missing columns (future year) using base year data (constant offset)
    [adj.insert(len(adj.columns), x, adj[baseyear]) for x in missing_cols]

    # discount rate equal to zero at the time of convergence
    disc = fun_discount_rate(df.columns, cols, tc, 1, 0)
    if method == "offset":
        # offset multiplied by discount rate
        adj_over_time = adj * disc
    else:
        # ratio will gradually converge to 1 at the time of convergence
        adj_over_time = adj * disc + (adj / adj) * (1 - disc)
        # NOTE In case of no data (na) no adjustments will be made (ratio=1, this is why we fill na with 1)
        # adj_over_time=adj_over_time.fillna(1)
        adj_over_time = (
            (1 / adj_over_time).replace(np.inf, 1).replace(-np.inf, 1).fillna(1)
        )
    # Calculate adjusted emissions
    adjusted_emi = getattr(
        # fun_xs(df, {"REGION": countrylist}).xs(var, level="VARIABLE"), ALLOWED_OPERATORS[method])(adj_over_time)
        df.xs(var, level="VARIABLE"),
        ALLOWED_OPERATORS[method],
    )(adj_over_time)

    adjusted_emi = adjusted_emi.reset_index().set_index(df.index.names)

    blacklist = adjusted_emi.index
    df = df.loc[~df.index.isin(blacklist)]

    if clip_positive:
        return pd.concat([df, adjusted_emi.clip(0, np.inf)], axis=0)
    else:
        return pd.concat([df, adjusted_emi], axis=0)


def fun_common_keys_in_a_list_of_dicts(dicts: List[Dict]) -> set:
    """Finds common keys in a list of dictionaries

    Parameters
    ----------
    dicts : List[Dict]
        List of dictionaries

    Returns
    -------
    set
        Common keys in the list of dictionaries
    """
    common_keys = set(dicts[0].keys())
    for d in dicts[1:]:
        common_keys.intersection_update(set(d.keys()))
    return common_keys


def fun_common_key_value_pairs_in_two_dicts(
    dict1: dict, dict2: dict, common: bool = True
) -> dict:
    # NOTE would be nice to generalize this function for multiple dicts
    """Returns common key value paires in two dictionaries. Value in dictionary can be a list.
    Parameters
    ----------
    dict1 : dict
        First dictionary
    dict2 : dict
        Second dictionary
    common : bool, optional
        Wheter you want to get key:value pairs that are in common or not in common in the two dicts, by default True

    Returns
    -------
    dict
        Dictionary with common (or not in common) key:values pairs
    """
    # Common elements Dictionary Value List
    res = {}
    if not common:
        common_keys = fun_common_keys_in_a_list_of_dicts([dict1, dict2])
        for m in common_keys:
            res[m] = list(
                set(
                    [x for x in dict1[m] if x not in dict2[m]]
                    + [x for x in dict2[m] if x not in dict1[m]]
                )
            )
            if len(res[m]) == 0:
                res.pop(m)
        not_common_keys = list(
            set(
                [x for x in dict1 if x not in dict2]
                + [x for x in dict2 if x not in dict1]
            )
        )
        for m in not_common_keys:
            val = dict1[m] if m in dict1 else dict2[m]
            res[m] = dict1[m] if m in dict1 else dict2[m]
            if len(res[m]) == 0:
                res.pop(m)

    else:
        for key in dict1:
            if key in dict2:
                res[key] = []
                for val in dict1[key]:
                    if val in dict2[key]:
                        res[key].append(val)
    for m in res:
        if type(res[m]) is not list and len(res[m]) == 1:
            res[m] = [res[m]]

    # NOTE:please consjder using:
    # from collections import Counter
    # Counter(dict1.keys())
    # please also check https://stackoverflow.com/questions/32815640/how-to-get-the-difference-between-two-dictionaries-in-python
    return dict(OrderedDict(sorted(res.items())))


def fun_selected_model_scen_below_threshold_based_on_dict(
    df_merged: pd.DataFrame,
    # val_2030: float = 2061,  # (4847 (GHG excl lulucf in 1990) -267.1 (LULUCF in 1990))*(1-0.55) based on CAT data
    val_2030: float = 2085,  #  4633.482972 * 0.45 = 2085.067337 which is the EU 2030 target, based on EEA data (UNFCCC)
    val_2050: float = 300,
    region="EU27",
    var="Emissions|Kyoto Gases (incl. indirect AFOLU)",
    exclude_models_from_filter: list = ["Primap"],
) -> pd.DataFrame:
    """Selects model and scenarios below GHG emissions threshold in 2030 and 2050, for a given `region`,
    by default the EU27

    Parameters
    ----------
    df_merged : pd.DataFrame
        Dataframe with data
    val_2030 : float, optional
        2030 GHG emissions threshold (in MtCO2), by default 2242 (from Climate Action Tracker)
    val_2050 : float, optional
        2050 GHG emissions threshold (in MtCO2), by default 300
    region : str, optional
        Region/iso , by default "EU27"

    Returns
    -------
    pd.Datframe
        Dataframe that matches both the 2030 and 2050 emissions threshold criteria (sliced for selected region/var), and a datafrme.index (excluding variable, region)
    """
    # NOTE Using "Emissions|Kyoto Gases (excl. LULUCF)" does not work because dependent on future data (lots of afforestation in climate scenario will make target harder to reach).
    # Therefore we use approach below, which calculate GHG below 1990 data (incl lulucf), which is dependent just on historical data
    df_exceptions = pd.DataFrame()
    for m in exclude_models_from_filter:
        if m in df_merged.reset_index().MODEL.unique():
            df_exceptions = pd.concat(
                [df_exceptions, df_merged.xs(m, level="MODEL", drop_level=False)],
                axis=0,
            )

    filter_dict = {
        "2030 target": {"var": f"{var}", "year": 2030, "val": val_2030},
        "2050 target": {"var": f"{var}", "year": 2050, "val": val_2050},
    }

    res_filter_dict = {
        k: fun_select_model_scenarios_below_threshold(
            df_merged, region, d["var"], d["year"], d["val"]
        )
        for k, d in filter_dict.items()
    }
    # if set(res_filter_dict.values())=={None}:
    #     return [None, None]
    sel_index = list(set.intersection(*(set(val) for val in res_filter_dict.values())))
    mydata = (
        df_merged.droplevel(["UNIT", "FILE"])
        .xs(var, level="VARIABLE")
        .xs(region, level="REGION", drop_level=False)
    )
    if "CRITERIA" in mydata.index:
        mydata = mydata.droplevel("CRITERIA")
    available_index = (mydata).index.tolist()
    sel_index = [x for x in sel_index if x in available_index]
    # df_res = fun_drop_duplicates(mydata.replace(0, np.nan).loc[sel_index])
    df_res = mydata.loc[sel_index]
    print(f"{len(df_res)} scenarios")
    if len(df_exceptions):
        df_res = pd.concat([df_res, df_exceptions], axis=0)
    if len(df_res):
        return df_res.xs(region, level="REGION"), sel_index
    else:
        return pd.DataFrame(), sel_index


def fun_select_eu_scenarios(
    eu_2030_kyoto, eu_2050_kyoto, res, df_sel_all_variables, var
):
    sel_df = fun_selected_model_scen_below_threshold_based_on_dict(
        df_sel_all_variables, val_2030=eu_2030_kyoto, val_2050=eu_2050_kyoto, var=var
    )[0]
    sel_EU_scenarios = fun_available_scen(sel_df) if len(sel_df) else None

    if sel_EU_scenarios is not None:
        dict_common = {
            "in line": True,
            "not in line": False,
        }
        for k, v in dict_common.items():
            r = fun_common_key_value_pairs_in_two_dicts(
                sel_EU_scenarios, fun_available_scen(df_sel_all_variables), common=v
            )
            print(
                f"\n {len(fun_flatten_list(r.values()))} scenarios {k} with EU targets: {r}"
            )

        df_sel_all_variables = fun_select_model_scenarios_combinations(
            df_sel_all_variables, sel_EU_scenarios
        ).sort_index(axis=1)
        res["selected C1 scenarios + eu targets"] = df_sel_all_variables.copy(deep=True)
    else:
        print("There are no scenarios compatible with EU targets")
    return res


def fun_select_passed_c1_scenarios(selected_scenarios, df_merged):
    df_pass_scenarios = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR
        # / "EU_climate_advisory_board_2023"
        / selected_scenarios
    )
    df_pass_scenarios.loc[:, "MODEL"] = [
        x.replace("/", "_") for x in df_pass_scenarios.MODEL
    ]
    sel_pass_scenarios = fun_available_scen(
        df_pass_scenarios.set_index(["MODEL", "SCENARIO"])
    )

    # Filter out data based on the two criteria above:
    # Criteria 2 (ED's regional/global vetting)

    df_sel_all_variables = fun_select_model_scenarios_combinations(
        df_merged, sel_pass_scenarios
    ).sort_index(axis=1)

    # C1 scenarios that are included in our dataset
    sel_pass_scenarios = fun_available_scen(df_sel_all_variables)
    return df_sel_all_variables


def fun_add_variables_and_harmonize(
    sel_vars,
    add_hist_data,
    use_eea_data,
    use_iea_data_from_ed,
    harmonize_eea_data_until,
    harmo_vars,
    interpolate,
    harmonize_hist_data,
    idx_col,
    df,
    iea_flow_dict,
    iea_var_dict,
    keep_step5_emissions,
):
    df = fun_add_secondary_energy(df)

    # RECALCULATE PRIMARY ENERGY as the sum of fuels
    df = fun_recalculate_primary_energy_as_sum_of_fuels(df)

    # NOTE Add Energy and Emissions (variable definitions) in one dictionary
    df, df_merged, hist_data_all_countries = fun_add_historic_data(
        sel_vars, add_hist_data, idx_col, df, iea_flow_dict, iea_var_dict
    )

    # NOTE on EMISSIONS HARMONIZATION
    # Normally we use
    # - IEA for Energy data (EJ/yr) and Energy related CO2 emissions
    # - PRIMAP for non-co2 emissions

    # For the EU-AB project we use
    # - EEA (European Environment Agency) data for all emissions variables
    # - IEA (International Energy Agency) for final energy data (if needed)

    # Read historical data
    iea_data = fun_read_all_iea_data(
        use_iea_data_from_ed,
        harmonize_eea_data_until,
        harmo_vars,
        iea_flow_dict,
        iea_var_dict,
    )
    emi_data, non_co2 = fun_read_hist_kyoto_data(use_eea_data, harmonize_eea_data_until)
    
    # Create Total Non-co2 emissions:
    mydict={'Emissions|Kyoto Gases (incl. indirect AFOLU)':1, 'Emissions|CO2':-1}
    if 'Emissions|Total Non-CO2' not in emi_data.reset_index().VARIABLE.unique():
        emi_data=fun_create_var_as_sum(emi_data, 'Emissions|Total Non-CO2', mydict, unit='Mt CO2-equiv/yr')

    # Get data until harmonize_eea_data_until
    emi_data = emi_data.loc[
        :, [x for x in emi_data if int(x) <= harmonize_eea_data_until]
    ]
    non_co2 = non_co2.loc[:, [x for x in non_co2 if int(x) <= harmonize_eea_data_until]]
    iea_data = iea_data.loc[
        :, [x for x in iea_data if int(x) <= harmonize_eea_data_until]
    ]

    # only for native models below:
    df_merged = fun_create_var_as_sum_only_for_models_where_missing(
        df_merged,
        "Emissions|CO2|LULUCF Direct+Indirect",
        {
            "Emissions|CO2": 1,
            "Emissions|CO2|Energy": -1,
            "Emissions|CO2|Industrial Processes": -1,
            "Emissions|CO2|Other": -1,  # reported by POLES GECO scenarios
            "Carbon Sequestration|Direct Air Capture": 1,  # reported by REMIND 3.2 -> Now "Emissions|CO2|LULUCF Direct+Indirect" coincides with `Emissions|CO2|AFOLU` if excluding `indirect_afolu_emi_assumption`
        },
        unit="Mt CO2/yr",
    )

    # DROP Emissions|CO2|Energy' from POLES GECO and recalculate ?
    # if so please add 'Emissions|CO2|Other' to create_var_as_sum

    selcols = range(2010, 2105, 5)
    # selcols= list(range(1990,harmonize_eea_data_until+1) )+list(selcols) if add_hist_data else selcols
    # Interpolate data to calculate 2019 value
    if interpolate:
        y = df_merged.columns
        lnsp = np.linspace(y.min(), y.max(), (1 + y.max() - y.min()))
        # interp_range = fun_find_previous_next_item(harmonize_eea_data_until, lnsp)
        interp_range = fun_find_nearest_values(
            list(df_merged.columns), harmonize_eea_data_until, n_max=2
        )
        if harmonize_eea_data_until not in interp_range:
            df_merged = fun_interpolate(
                df_merged[selcols],  # .replace(0, np.nan),
                False,
                # range(2015, 2021),
                range(int(interp_range[0]), int(interp_range[1])),
                interpolate_columns_present=True,
            )

        # Use 2020 data for historical afolu data (COVID does not affect 2020 AFOLU emissions)
        afolu_vars = [
            "Emissions|CO2|AFOLU",
            "Emissions|CO2|LULUCF Direct+Indirect",
            "Emissions|CO2|LULUCF Indirect",
        ]
        df_merged = fun_fill_na_with_previous_next_values_for_selected_vars(
            df_merged, afolu_vars
        )

    # Constant offset variables:
    constant_offset_vars = [
        "Emissions|CO2|LULUCF Direct+Indirect",
        "Emissions|CO2|LULUCF",
    ]

    # step2 harmonize "Emissions|CO2" with historical data (and other energy variables)
    harmo_str = "harmo" if harmonize_hist_data else "NOT_HARMONIZED"
    # selcols=[harmonize_eea_data_until]+list(range(2010,2105,5))
    # selcols.sort()
    df_merged_not_harmo = df_merged.copy(deep=True)

    # NOTE there will be no `ratio` harmonization if historical data are equal to zero.
    # In this case we keep on using the unharmonized data
    # e.g. this is the case for SVK, VNM, HKG, TWN, KOR (all primary enegry variables-> for which there are missing in Primary energy data historical IEA dataset
    # but this is also the case for IND "Primary Energy|Geothermal" (for which data are equal to zero in the IEA)
    vars_available = df_merged.reset_index().VARIABLE.unique()
    if harmonize_hist_data:
        for var in harmo_vars:
            if var in vars_available:
                df_merged = fun_harmonize_hist_data_general(
                    use_eea_data,
                    use_iea_data_from_ed,
                    # df_merged[selcols], # we need these selected columns
                    df_merged,
                    iea_data,
                    emi_data,
                    constant_offset_vars,
                    var,
                    iea_var_dict,
                )

    if "Emissions|CO2" in df_merged.reset_index().VARIABLE.unique():
        df_merged = df_merged.drop("Emissions|CO2", level="VARIABLE")
    df_merged = fun_create_var_as_sum(
        df_merged,
        "Emissions|CO2",
        {
            "Emissions|CO2|Energy": 1,
            "Emissions|CO2|Industrial Processes": 1,
            "Emissions|CO2|LULUCF Direct+Indirect": 1,  # Because we do not have the breakdown (direct/indirect from EEA)
            "Carbon Sequestration|Direct Air Capture": -1,
        },
        unit="Mt CO2/yr",
    )

    # Create "Emissions|Total Non-CO2" only only for model/scenario combinations where is missing
    # (POLES geco scenarios (with native eu27 region) do not contain this variable (because we have not run step5c))
    # We do not add indirect afolu emissions as we harmonize CO2 to match inventories (that include indirect afolu emissions -> assume constant offset over time)
    if "Emissions|Kyoto Gases" in df_merged.reset_index().VARIABLE.unique():
        df_merged = fun_create_var_as_sum_only_for_models_where_missing(
            df_merged,
            "Emissions|Total Non-CO2",
            {"Emissions|Kyoto Gases": 1, "Emissions|CO2": -1},
            unit="Mt CO2-equiv/yr",
        )

    if "Emissions|Total Non-CO2" not in df_merged.reset_index().VARIABLE.unique():
        raise ValueError(
            "Cannot find `Emissions|Total Non-CO2` in your dataframe. Please check your data."
        )

    if harmonize_hist_data:
        # Harmonize non-CO2 emissions
        df_merged = fun_harmonize_hist_data(
            df_merged, "Emissions|Total Non-CO2", 2050, non_co2
        )

    # Blueprint: add missing IEA countries:

    df_merged = df_merged.replace(np.inf, 0).replace(-np.inf, 0)
    tempvar = "Emissions|Kyoto Gases (incl. indirect AFOLU)"
    if tempvar in df_merged.reset_index().VARIABLE.unique():
        df_merged = df_merged.drop(tempvar, level="VARIABLE")
    df_merged = fun_calculate_kyoto_gases(df_merged, df_merged.index.names)
    # We drop 'Emissions|CO2|LULUCF Indirect' as they are not used  anylonger for calculating Kyoto Gases
    # blackl=df_merged.xs('Emissions|CO2|LULUCF Indirect', level='VARIABLE', drop_level=False).index
    # df_merged=  df_merged[~df_merged.index.isin(blackl)]

    # Add Exogenous international transport from Miles -> GHG*
    # We call these new variables:
    # - `GHG incl. International transport`
    # - `GHG incl. International transport (intra-eu only)`

    if callable(CONSTANTS.CURR_RES_DIR):
        CURR_RES_DIR = CONSTANTS.CURR_RES_DIR("Step_5b_emissions_by_sector_5b.py")
    else:
        CURR_RES_DIR = CONSTANTS.CURR_RES_DIR

    bunkers = pd.read_csv(
        CURR_RES_DIR
        / "International_transport_from_Miles_2023_04_06.csv"
        # / "International_transport_from_Miles_2023_04_03.csv"
    )
    bunkers = fun_index_names(bunkers, True, int)

    # 1) if "Emissions|CO2|Energy|Demand|Bunkers|Intra-EU" is available harmonized base year to match data from Miles
    df_merged = fun_harmonize_existing_bunkers_using_external_dataset(
        harmonize_eea_data_until, df_merged, bunkers
    )

    models_with_native_bunkers = []
    if (
        "Emissions|CO2|Energy|Demand|Bunkers|Intra-EU"
        in df_merged.reset_index().VARIABLE.unique()
    ):
        models_with_native_bunkers = list(
            df_merged.xs(
                "Emissions|CO2|Energy|Demand|Bunkers|Intra-EU", level="VARIABLE"
            )
            .reset_index()
            .MODEL.unique()
        )

    # 2) if missing, use data from Miles

    df_missing_models = fun_add_bunkers_from_miles_to_ghg(
        fun_xs(df_merged, {"MODEL": models_with_native_bunkers}, exclude_vars=True),
        bunkers,
    ).dropna(how='all')

    df_merged = pd.concat([df_merged, df_missing_models], axis=0, sort=True)

    # We drop variables no longer used
    blackl = fun_xs(
        df_merged,
        {
            "VARIABLE": [
                # "Emissions|CO2|LULUCF Indirect",  # No need to make future assumption, because CO2 emissions match inventory (including indirect emissions)
                "Emissions|CO2|AFOLU",
                # "Emissions|CO2|Energy",  # Excluded, because we just report `Emissions|CO2`
                # "Emissions|CO2|Industrial Processes",  # Excluded, because we just report `Emissions|CO2`
                "Emissions|CH4",  # Excluded, because we just report `Total Non-co2`
                "Emissions|HFC",  # Excluded, because we just report `Total Non-co2
                "Emissions|N2O",  # Excluded, because we just report `Total Non-co2
                "Emissions|SF6",  # Excluded, because we just report `Total Non-co2
            ]
        },
    ).index
    df_merged = df_merged[~df_merged.index.isin(blackl)]

    # RECALCULATE PRIMARY ENERGY (after the harmonization) as the sum of fuels
    df_merged = fun_recalculate_primary_energy_as_sum_of_fuels(df_merged)
    fossils = ["Primary Energy|Coal", "Primary Energy|Oil", "Primary Energy|Gas"]

    for x in ["", "|w/ CCS", "|w/o CCS"]:
        if f"Primary Energy|Fossil{x}" in vars_available:
            df_merged = fun_recalculate_existing_var_as_sum(
                df_merged,
                f"Primary Energy|Fossil{x}",
                [f"{fos}{x}" for fos in fossils],
                unit="EJ/yr",
                # dropna=False
            )

    # RECALCULATE Primary ENERGY|fuel as the sum of with w/o CCS
    for f in ["Coal", "Gas", "Biomass"]:
        mv = f"Primary Energy|{f}"
        subs = [f"Primary Energy|{f}|w/ CCS", f"Primary Energy|{f}|w/o CCS"]
        if mv in df_merged.reset_index().VARIABLE.unique():
            if len(fun_xs(df_merged, {"VARIABLE": subs})):
                df_merged = fun_recalculate_existing_var_as_sum(
                    df_merged,
                    mv,
                    subs,
                    unit="EJ/yr",
                )

    # RECALCULATE SECONDARY ENERGY|ELECTRICITY (after the harmonization) as the sum of fuels
    for mv in ["Primary Energy", "Secondary Energy|Electricity"]:
        recalc_vars = [f"{mv}{x}" for x in ["", "Non-Biomass Renewables"]]
        for v in recalc_vars:
            if v == "Emissions":
                subs = [
                    "Emissions|CO2|Energy",
                    "Emissions|Total Non-CO2",
                    "Emissions|CO2|AFOLU",
                    "Emissions|CO2|Industrial Processes",
                ]
            else:
                subs = fun_get_sub_sectors(df_merged, v, "fuel")
            if subs != [""]:
                df_merged = fun_recalculate_existing_var_as_sum(
                    df_merged,
                    v,
                    subs,
                    unit="EJ/yr",
                    # dropna=False
                )

    recalc_vars = {
        "Secondary Energy|Electricity|Trade": {
            "vars": {"Secondary Energy|Electricity": 1, "Final Energy|Electricity": -1},
            "unit": "EJ/yr",
        },
        "Carbon Sequestration|CCS": {
            "vars": [
                "Carbon Sequestration|CCS|Biomass",
                "Carbon Sequestration|CCS|Fossil",
                "Carbon Sequestration|CCS|Industrial Processes",
            ],
            "unit": "Mt CO2/yr",
        },
    }
    for k, v in recalc_vars.items():
        if k in df_merged.reset_index().VARIABLE.unique():
            df_merged = df_merged.drop(k, level="VARIABLE")
        vars_to_check = (
            list(v["vars"].keys()) if isinstance(v["vars"], dict) else v["vars"]
        )
        if len(fun_xs(df_merged, {"VARIABLE": vars_to_check})):
            df_merged = fun_create_var_as_sum(df_merged, k, v["vars"], unit=v["unit"])

    df_merged = fun_drop_duplicates(df_merged)
    selcols = list(range(2010, 2105, 5))

    if interpolate and harmonize_eea_data_until not in selcols:
        selcols = [harmonize_eea_data_until] + selcols
        selcols.sort()

    if (
        "Emissions|CO2|Energy and Industrial Processes"
        in df_merged.reset_index().VARIABLE.unique()
    ):
        df_merged = df_merged.drop(
            "Emissions|CO2|Energy and Industrial Processes", level="VARIABLE"
        )

    df_merged = fun_create_var_as_sum(
        df_merged,
        "Emissions|CO2|Energy and Industrial Processes",
        ["Emissions|CO2|Energy", "Emissions|CO2|Industrial Processes"],
        unit="Mt CO2/yr",
    )

    # Replace with NA if energy is missing: this only works for the EU27!!!!!!!!!!!
    if set(df_merged.reset_index().REGION.unique()) == {"EU27"}:
        check = df_merged.xs(
            "Emissions|CO2|Energy and Industrial Processes",
            level="VARIABLE",
            drop_level=False,
        )[2019].reset_index()
        idxna = check[check[2019] < 260].set_index(df_merged.index.names).index
        df_merged.loc[idxna, :] = np.nan

    # Placeholder for calculating Kyoto excluding indirect emissions:
    # fill 2015, 2010 indirect emissions (or hist data from Grassi)
    # kyoto= kyoto  inc. indirect emissions -indirect emissions
    # re-harmonized kyoto based on historical data (excl. indirect emissions) # OR maybe no need if we exclude indirect emission before.

    # Drop emissions VARIABLES that are not harmonized (variables that contains more than one value in 2019, excluding np.nan):
    var_list_to_be_dropped = []
    mylist=[x for x in df_merged.reset_index().VARIABLE.unique() if "mission" in x]
    mylist=[x for x in mylist if x not in keep_step5_emissions]
    for var in mylist:
        res = (
            np.round(df_merged.xs(var, level="VARIABLE")[harmonize_eea_data_until], 3)
            .dropna()
            .unique()
        )
        if len(res) != 1:
            if var not in [
                "Emissions|CO2",
                "Emissions|CO2|Energy",  # need this for policies adjustments (should update test case)
                "Emissions|CO2|Industrial Processes",  # need this for NGFS
                "Emissions|CO2|LULUCF Direct+Indirect",  # need this for NGFS
                "Emissions|CO2|LULUCF Indirect",  # need this for NGFS
                "Emissions|Total Non-CO2",  # need this for NGFS
                "Emissions|Kyoto Gases (incl. indirect AFOLU)",
            ]:
                var_list_to_be_dropped = var_list_to_be_dropped + [var]
    df_merged = df_merged.drop(var_list_to_be_dropped, level="VARIABLE")

    for m in ["Euro-Calliope 2.0", "GCAM-PR 5.3"]:
        if m in df_merged.reset_index().MODEL.unique():
            df_merged.loc[m, list(range(2055, 2105, 5))] = np.nan

    return (
        # df,
        df_merged,
        # hist_data_all_countries,
        harmo_str,
        selcols,
        df_merged_not_harmo,
    )


def fun_harmonize_existing_bunkers_using_external_dataset(
    base_year: int, df_merged: pd.DataFrame, df_external: pd.DataFrame
):
    miles_dict = {
        "Emissions|CO2|Energy|Demand|Bunkers|Intra-EU": {
            "miles": "Domestic (intra-MS) aviation & navigation",
            "new_var_name": "GHG incl. International transport (intra-eu only)",
        },
        "Emissions|CO2|Energy|Demand|Bunkers": {
            "miles": "Total",
            "new_var_name": "GHG incl. International transport",
        },
    }
    # Available regions from external dataset (this will return EU27)
    av_region=list(df_external.reset_index().REGION.unique())
    for k, v in miles_dict.items():
        if k in fun_xs(df_merged, {"REGION":av_region}).reset_index().VARIABLE.unique():
            models = list(
                df_merged.xs(k, level="VARIABLE").reset_index().MODEL.unique()
            )
            emi_bunkers = df_merged.xs(k, level="VARIABLE", drop_level=False)
            buncols = [
                x for x in range(1990, base_year + 1, 1) if x in df_external.columns
            ]
            baseyeardata = df_external.xs(v["miles"], level="VARIABLE")[buncols]
            emi_bunkers = fun_harmonize_hist_data(
                fun_index_names(emi_bunkers),
                k,
                None,
                fun_index_names(baseyeardata, True, str).droplevel(
                    ["MODEL", "SCENARIO", "UNIT"]
                ),
                method="ratio",
                coerce_errors=True,
            )
            emi_bunkers = fun_index_names(emi_bunkers, True, int)
            emi_bunkers = pd.concat(
                [
                    emi_bunkers,
                    df_merged.xs(
                        "Emissions|Kyoto Gases (incl. indirect AFOLU)",
                        level="VARIABLE",
                        drop_level=False,
                    ),
                ]
            )

            emi_bunkers = fun_create_var_as_sum(
                fun_xs(emi_bunkers, {"MODEL": models}),
                v["new_var_name"],
                ["Emissions|Kyoto Gases (incl. indirect AFOLU)", k],
                unit="Mt CO2-equiv/yr",
            )
            # drop index in common (we want to update the harmonized intra-eu emissions) and append the results (we want to add GHG inc. International transport (intra-eu only))
            df_merged = pd.concat(
                [
                    df_merged.drop(
                        set(emi_bunkers.index).intersection(df_merged.index)
                    ),
                    emi_bunkers,
                ],
                axis=0,
                sort=True,
            )

    return df_merged


def fun_add_bunkers_from_miles_to_ghg(
    df_merged: pd.DataFrame, bunkers: pd.DataFrame
) -> pd.DataFrame:
    if not len(df_merged):
        print("dataset is empty - will not add bunkers from miles")
        return pd.DataFrame()
    df_merged = df_merged.copy(deep=True)
    # dictionary with  {variables_in_df_merged: variables_in_bunkers file from Miles}
    bunk_var_dict = {
        "GHG incl. International transport": "Total",
        "GHG incl. International transport (intra-eu only)": "Domestic (intra-MS) aviation & navigation",
    }

    for k, v in bunk_var_dict.items():
        bunkers_sel = (
            bunkers.loc[:, df_merged.columns]
            .droplevel(["MODEL", "SCENARIO", "UNIT"])
            .xs(v, level="VARIABLE")
        )
        ghg_star = fun_add_multiply_dfmultindex_by_dfsingleindex(
            df_merged.xs(
                "Emissions|Kyoto Gases (incl. indirect AFOLU)", level="VARIABLE"
            ),
            bunkers_sel,
            operator="+",
        )
        ghg_star["VARIABLE"] = k
        ghg_star = ghg_star.reset_index().set_index(df_merged.index.names)
        df_merged = pd.concat([df_merged, ghg_star], axis=0)
    return fun_xs(
        df_merged,
        {
            "VARIABLE": [
                "GHG incl. International transport",
                "GHG incl. International transport (intra-eu only)",
            ]
        },
    )


def fun_recalculate_primary_energy_as_sum_of_fuels(df):
    df = df.copy(deep=True)
    myidx = df.index.names
    df = fun_index_names(df, True)
    df = fun_drop_duplicates(df)
    if "Primary Energy" in df.index.get_level_values("VARIABLE"):
        blacklist = df.xs("Primary Energy", level="VARIABLE", drop_level=False).index
        df = df.loc[~df.index.isin(blacklist)]
    # Recalculate primary energy as the sum of sub-sectors
    primary_en_vars = fun_get_sub_sectors(df, "Primary Energy", "fuel")
    df = fun_create_var_as_sum(
        fun_index_names(df),
        "Primary Energy",
        primary_en_vars,
        unit="EJ/yr",
    )
    df = df.reset_index().set_index(myidx)
    return df


def fun_create_var_as_sum_only_for_models_where_missing(
    df: pd.DataFrame,
    new_var: str,
    var_list_or_dict: Union[dict, list],
    _level: str = "VARIABLE",
    unit: Union[None, str] = None,
    coerce_errors=True,
    verbose=True,
    exogenous_value=0,
):
    """Create `new_var` as sum of `var_list_or_dict` only for model/scenario/regions combination where `new_var` is missing.
    If `new_var` is already available for some models/scenarios/regions, we leave it as it is

    Parameters
    ----------
    df : pd.DataFrame
        _description_
    new_var : str
        _description_
    var_list_or_dict : Union[dict, list]
        _description_
    _level : str, optional
        _description_, by default "VARIABLE"
    unit : Union[None, str], optional
        _description_, by default None

    Returns
    -------
    _type_
        _description_
    """
    df_all = pd.DataFrame()
    for region in df.reset_index().REGION.unique():
        df_region = df.xs(region, level="REGION", drop_level=False)
        df_with_var = pd.DataFrame()
        df_without_var = df_region
        if new_var in df_region.reset_index()[_level].unique():
            # 1) Models/scenarios where `new_var` already exists (we keep it as it is)
            df_with_var = fun_select_model_scenarios_combinations(
                df_region,
                fun_available_scen(
                    df_region.xs(new_var, level=_level, drop_level=False)
                ),
            )
            # 2) Models/scenarios where `new_var` does not exists
            df_without_var = df_region[~df_region.index.isin(df_with_var.index)]
        check_vars = (
            var_list_or_dict
            if isinstance(var_list_or_dict, list)
            else var_list_or_dict.keys()
        )
        proceed = (
            True
            if len(fun_xs(df_without_var, {"VARIABLE": list(check_vars)}))
            else False
        )
        if len(df_without_var):
            if proceed:
                df_without_var = fun_create_var_as_sum(
                    df_without_var,
                    new_var,
                    var_list_or_dict,
                    unit=unit,
                    exogenous_add_value=exogenous_value,
                )
            else:
                if not coerce_errors:
                    raise ValueError(f"Could not find {check_vars} in your dataframe")
                elif verbose:
                    print(
                        f"Could not find {check_vars} in your dataframe for region {region}"
                    )
        # Merge both dataframes
        df_all = pd.concat([df_all, df_with_var, df_without_var], axis=0)
    return df_all


def fun_harmonize_hist_data_general(
    use_eea_data,
    use_iea_data_from_ed,
    df_merged,
    iea_data,
    eea_data,
    constant_offset_vars,
    var,
    iea_var_dict,
):
    if "mission" in var:
        if use_eea_data:
            # Emissions variables from eea
            myvar = (
                f"{var} (UNFCCC)"
                if f"{var} (UNFCCC)" in eea_data.reset_index().VARIABLE.unique()
                else var
            )
            hist_var = eea_data.xs(myvar, level="VARIABLE")
        else:
            hist_var = eea_data.xs(var, level="VARIABLE")
        hist_var = hist_var.droplevel(["MODEL", "SCENARIO", "UNIT"])
    elif use_iea_data_from_ed:
        # IEA energy variables from ed
        hist_var = iea_data.xs(var, level="VARIABLE").droplevel(
            ["MODEL", "SCENARIO", "UNIT"]
        )
        if "World" in hist_var.index:
            hist_var = hist_var.drop("World")
    else:
        hist_var = fun_get_historical_emissions(
            var,
            iea_data,
            iea_var_dict,
        )
        # if 'mission' in var:
        #     hist_var=hist_var/ 1e3
    method = "offset"
    if (
        df_merged.xs(var, level="VARIABLE").reset_index().UNIT.unique()[0] == "EJ/yr"
        or var == "Emissions|CO2|Energy"
    ):
        method = "ratio"

    clip = df_merged.xs(var, level="VARIABLE").reset_index().UNIT.unique()[0] == "EJ/yr"
    # Harmonize CO2 emissions
    conv = 2050 if var not in constant_offset_vars else None
    if len(hist_var):
        df_merged = fun_harmonize_hist_data(
            df_merged,
            var,
            conv,
            hist_var,
            method=method,  # clip_positive=clip
            # method="offset",  # TEMPORARY, JUST TO CHECK!!!!!!!!!!!
            # clip_positive=clip,
            coerce_errors=True,
        )
    else:
        print(f"No historical data found for {var}")
    return df_merged


def fun_read_hist_kyoto_data(use_eea_data, harmonize_eea_data_until):
    eea_data = pd.DataFrame()
    primap = pd.DataFrame()
    if use_eea_data:
        eea_data = pd.read_csv(CONSTANTS.INPUT_DATA_DIR / "input_reference_EEA.csv")
        time_cols = [str(x) for x in range(1, 10)]
        eea_data.columns = [
            x.upper() if str(x[0]) not in time_cols else x for x in eea_data.columns
        ]
        eea_data = eea_data.set_index(
            ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]
        )
        selcols = [x for x in eea_data.columns if int(x) <= harmonize_eea_data_until]
        eea_data = eea_data[selcols]
        non_co2 = eea_data.xs(
            "Emissions|Total Non-CO2 (AR4) (UNFCCC)", level="VARIABLE"
        ).droplevel(["MODEL", "SCENARIO", "UNIT"])
        non_co2.columns = [int(x) for x in non_co2.columns]
        eea_data = fun_create_var_as_sum(
            eea_data, "Emissions|CO2|LULUCF Direct+Indirect", ["Emissions|CO2|LULUCF"]
        )
        emidata = eea_data

    else:
        primap = fun_read_primap(CONSTANTS.INPUT_DATA_DIR)
        non_co2 = primap.xs("KYOTOGHG (AR4GWP100)", level="entity").xs(
            "M.0.EL", level="category (IPCC2006_PRIMAP)"
        ) - primap.xs("CO2", level="entity").xs(
            "M.0.EL", level="category (IPCC2006_PRIMAP)"
        )
        non_co2 = non_co2.droplevel(["MODEL", "source", "UNIT"]).xs(
            "HISTCR", level="SCENARIO"
        )
        non_co2 = non_co2 / 1e3
        emidata = fun_ghg_emi_from_primap(None, None).droplevel("FILE")
    return emidata, non_co2  # , primap


def fun_read_all_iea_data(
    use_iea_data_from_ed:bool,
    harmonize_eea_data_until:float,
    harmo_vars,
    iea_flow_dict,
    iea_var_dict,
):
    # read IEA data
    if use_iea_data_from_ed:
        iea_data = fun_read_iea_data_from_iamc_format(
            harmonize_eea_data_until, harmo_vars
        )
    else:
        iea_data = fun_read_hist_emissions()
        energy_harmo_vars = fun_get_energy_vars_from_iea_flow_dict(
            harmo_vars, iea_flow_dict
        )
        iea_energy_dict = fun_create_iea_dict_from_iea_flow_dict(
            energy_harmo_vars, iea_flow_dict
        )
        if len(iea_energy_dict):
            iea_data = pd.concat([iea_data, fun_read_hist_energy()])
            iea_var_dict.update(iea_energy_dict)
    return iea_data


def fun_fill_na_with_previous_next_values_for_selected_vars(df_merged, sel_vars):
    for x in sel_vars:
        if x in df_merged.reset_index().VARIABLE.unique():
            afolu = fun_fill_na_with_previous_next_values(
                df_merged.xs(x, level="VARIABLE", drop_level=False)
            )
            blackl = afolu.index
            df_merged = pd.concat(
                [df_merged[~df_merged.index.isin(blackl)], afolu], axis=0
            )
    return df_merged


# TODO Rename this here and in `main`
def fun_read_iea_data_from_iamc_format(
    harmonize_eea_data_until, harmo_vars, file_name="input_reference_iea_2022.csv"
):
    result = pd.read_csv(CONSTANTS.INPUT_DATA_DIR / file_name)
    result = result.set_index(["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"])
    result = result.xs("EJ/yr", level="UNIT", drop_level=False)
    selcolsiea = [x for x in result.columns if int(x) <= harmonize_eea_data_until]
    result = result[selcolsiea]

    ccs_variables = [x for x in harmo_vars if "w/o CCS" in x]
    rename_dict = {
        x: result.xs(x.replace("|w/o CCS", ""), level="VARIABLE", drop_level=False)
        .reset_index()
        .VARIABLE.unique()[0]
        for x in ccs_variables
    }

    for k, v in rename_dict.items():
        result = fun_create_var_as_sum(result, k, [v], unit="EJ/yr")

    return result


def fun_add_secondary_energy(df):
    add_var = {
        "Secondary Energy|Solids": [
            "Secondary Energy|Solids|Biomass",
            "Secondary Energy|Solids|Coal",
        ],
        "Secondary Energy|Residential and Commercial": [
            "Secondary Energy|Residential and Commercial|Solids|Biomass",
            "Secondary Energy|Residential and Commercial|Solids|Coal",
        ],
        "Secondary Energy|Industry": [
            "Secondary Energy|Industry|Solids|Biomass",
            "Secondary Energy|Industry|Solids|Coal",
        ],
    }
    for k, v in add_var.items():
        if k in df.reset_index().VARIABLE.unique():
            df = df.drop(k, level="VARIABLE")
        if len(fun_xs(df, {"VARIABLE": v})):
            df = fun_create_var_as_sum(df, k, v, unit="EJ/yr")
    return df


def fun_add_share_of_hydrogen(df_merged):
    hydrogen_to_fen = df_merged.xs(
        "Secondary Energy|Hydrogen", level="VARIABLE"
    ) / df_merged.xs("Final Energy", level="VARIABLE")
    hydrogen_to_fen["VARIABLE"] = "Share of hydrogen on Final Energy"
    hydrogen_to_fen = hydrogen_to_fen.rename(index={"EJ/yr": "ratio"}, level="UNIT")
    hydrogen_to_fen = hydrogen_to_fen.reset_index().set_index(df_merged.index.names)
    df_merged = pd.concat([df_merged, hydrogen_to_fen], axis=0, sort=True)
    return df_merged


def fun_add_trade_variables(df_merged):
    # (List of variables from Ed on teams on 10 March 2023)
    imports_var = [
        "Trade|Primary Energy|Biomass|Volume",
        "Trade|Primary Energy|Coal|Volume",
        "Trade|Primary Energy|Fossil",
        "Trade|Primary Energy|Gas|Volume",
        "Trade|Primary Energy|Oil|Volume",
        "Trade|Secondary Energy|Hydrogen|Volume",
        "Trade|Secondary Energy|Liquids|Biomass|Volume",
    ]

    imports = -fun_create_var_as_sum(
        df_merged, "Energy Imports", imports_var, unit="EJ/yr"
    ).xs("Energy Imports", level="VARIABLE") / df_merged.xs(
        "Primary Energy", level="VARIABLE"
    )
    imports = imports.rename(index={"EJ/yr": "ratio"}, level="UNIT")
    imports["VARIABLE"] = "Share of energy imports on Primary Energy"
    imports = imports.reset_index().set_index(df_merged.index.names)
    df_merged = pd.concat([df_merged, imports], axis=0, sort=True)
    return df_merged


def fun_add_cdr_variables(df: pd.DataFrame) -> pd.DataFrame:
    for v in ["Carbon Capture|Biomass", "Carbon Sequestration|CCS|Biomass"]:
        if "Carbon Removal|BECCS" not in df.reset_index().VARIABLE.unique():
            if v in df.reset_index().VARIABLE.unique():
                df = fun_create_var_as_sum(
                    df, "Carbon Removal|BECCS", [v], unit="MtCO2/yr"
                )
                afolu = (
                    fun_create_var_as_sum(
                        df,
                        "Carbon Removal|AFOLU",
                        {"Emissions|CO2|LULUCF Direct+Indirect": -1},
                        unit="MtCO2/yr",
                    )
                    .xs("Carbon Removal|AFOLU", level="VARIABLE", drop_level=False)
                    .clip(0, np.inf)
                )
                df = pd.concat([df, afolu], axis=0)
    return df


def fun_add_historic_data(
    sel_vars, add_hist_data, idx_col, df, iea_flow_dict, iea_var_dict
):
    iea_emi_vars = [x for x in sel_vars if "Emissions" in x or "sequestration" in x]
    for v in ["Kyoto", "AFOLU", "CH4", "HFC", "N2O", "Industrial Processes"]:
        iea_emi_vars = [x for x in iea_emi_vars if v not in x]
    iea_energy_vars = [x for x in sel_vars if "Emissions" not in x and "Energy" in x]
    if add_hist_data:
        if "Emissions|Kyoto Gases (incl. indirect AFOLU)" in sel_vars:
            # NOTE We use this version of PRIMAP with data until 2019, because this one also has lulucf data.
            # the newer one has data until 2021 but does not have LULUCF data
            df = fun_add_historic_kyoto(df)
        df_merged, hist_data_all_countries = fun_add_historic_iea_data(
            idx_col, df, iea_emi_vars, iea_energy_vars, iea_flow_dict, iea_var_dict
        )
    else:
        df_merged = df
        for x in ["FILE", "CRITERIA"]:
            if x in df_merged.columns:
                df_merged = df_merged.set_index([x], append=True)
        hist_data_all_countries = pd.DataFrame()
    df_merged.columns = [int(x) for x in df_merged.columns]
    return df, df_merged, hist_data_all_countries


def fun_add_historic_iea_data(
    idx_col, df, iea_emi_vars, iea_energy_vars, iea_flow_dict, iea_var_dict
):
    if len(iea_energy_vars + iea_emi_vars):
        iea_flow_dict.update(iea_var_dict)
        df_merged, hist_data_all_countries = fun_add_hist_data(
            df,
            idx_col,
            iea_emi_vars,
            iea_energy_vars,
            iea_flow_dict,
        )
    else:
        df_merged = df
        for x in ["FILE", "CRITERIA"]:
            if x in df_merged.columns:
                df_merged = df_merged.set_index([x], append=True)
        hist_data_all_countries = pd.DataFrame()
    return df_merged, hist_data_all_countries


def fun_create_var_as_sum_if_missing(
    df_init, var, var_list_dict, unit, coerce_errors=True
):
    if "UNIT" in df_init.index.names and unit is not None:
        df_init = df_init.droplevel("UNIT")
    df = fun_create_var_as_sum_only_for_models_where_missing(
        df_init, var, var_list_dict, unit=unit, coerce_errors=coerce_errors
    )

    df = df.xs(var, level="VARIABLE", drop_level=False)
    if unit is not None:
        df.loc[:, "UNIT"] = unit
    if var in df_init.reset_index().VARIABLE.unique():
        df_init = df_init.drop(var, axis=0, level="VARIABLE")

    return fun_drop_duplicates(pd.concat([df_init, fun_index_names(df, False, None)]))


def fun_get_iam_regions_associated_with_countrylist(
    project: str, countrylist: list, model: str
) -> dict:
    """Provides a dictionary with iam_regions associated with a list of countries `countrylist`

    Parameters
    ----------
    project : str
        Your project folder (where to look for the model mapping) e.g. 'NGFS_2023'
    countrylist : list
        List of countries for which you want to get a list of associated iams regions
    model : str
        Your chosen IAM e.g. `'REMIND-MAgPIE 3.1-4.6'`

    Returns
    -------
    dict
        Dictionary with: {country : iam_region}
        Example -> {'AUT': 'EU 28r'}
    """
    country_dict = {"EU27": fun_eu27(), "EU28": fun_eu28()}
    countrylist = countrylist or all_countries
    countrylist = set(fun_flatten_list([country_dict.get(c, [c]) for c in countrylist]))
    return {
        c: fun_country2region(
            model,
            c,
            CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv",
            CONSTANTS.INPUT_DATA_DIR / project / "default_mapping.csv",
        )
        for c in countrylist
    }


def fun_read_energy_iea_data_in_iamc_format(
    iea_flow_dict, save_to_csv=False, verbose=True, df=None, conversion_from_ktoe_to_ej = 0.041868 / 1e3
):
    # Adding definition of primary enegry variables 
    # We use same `product` as 'Secondary Energy|Electricity', but we use a different the flow 'Total energy supply'
    # df = fun_read_large_iea_file(CONSTANTS.INPUT_DATA_DIR/'Extended_IEA_en_bal_2021.csv' )
    secondary=[x for x in iea_flow_dict.keys() if 'Secondary Energy|Electricity' in x ]
    primary=SliceableDict(iea_flow_dict).slice(*tuple(secondary))
    primary2={k.replace('Secondary Energy|Electricity','Primary Energy'):[i for i in [['Total energy supply']]+v[1:]] for k,v in primary.items()}
    iea_flow_dict={**iea_flow_dict, **primary2}
    
    if df is None:
        raw_iea_df = fun_read_hist_energy()
    else:
        raw_iea_df=df
    
    df_all = pd.DataFrame()
    for var in iea_flow_dict.keys():
        d = fun_create_iea_dict_from_iea_flow_dict([var], iea_flow_dict)
        df = fun_xs(
            raw_iea_df, {"FLOW": d[var]["flow"], "PRODUCT": d[var]["product"]}
        )  # .loc['AUT']
        if len(df):
            df["VARIABLE"] = var
            df = fun_index_names(df.droplevel(["PRODUCT", "FLOW"]), True, str)
            df = fun_remove_np_in_df_index(df)
            df_all = pd.concat([df_all, df], axis=0, sort=True)
        elif verbose:
            print(
                f"Could not find {var} in `raw_iea_df` (based on `iea_flow_dict` dictionary)"
            )
    df_all["MODEL"] = "IEA"
    df_all["SCENARIO"] = "Historic data"
    df_all["UNIT"] = "EJ/yr"
    df_all = fun_index_names(
        df_all.reset_index(),
        True,
        str,
        ["MODEL", "SCENARIO", "ISO", "VARIABLE", "UNIT"],
    )
    df_all.index.names = ["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]
    df_all = df_all.apply(pd.to_numeric, errors="coerce") * conversion_from_ktoe_to_ej
    df_all = df_all.groupby(["MODEL", "SCENARIO", "REGION", "VARIABLE", "UNIT"]).sum()
    if save_to_csv:
        df_all.to_csv(CONSTANTS.INPUT_DATA_DIR / "input_reference_iea_2019_old.csv")
    return df_all


def fun_check_values_within_time_range_dt(
    val: int, range_min: int = 2005, range_max: int = 2105, dt: int = 5
) -> list:
    """Check whether a value `val` is between two other integers, based on a range(`range_min`, `range_max`+`dt`, `dt`).
    Can be used to check in which time interval a given year is.
    Example: fun_check_values_within_time_range_dt(2019) returns ->[2015, 2016, 2017, 2018, 2019, 2020]

    Parameters
    ----------
    val : int
        value to be checked
    range_min : int, optional
        Min value of the time range, by default 2005
    range_max : int, optional
        Max value of the time range, by default 2105
    dt : int, optional
        Time steps used in time range, by default 5

    Returns
    -------
    list
        _description_
    """
    myrange = list(range(range_min, range_max, dt))
    return [
        list(range(x, x + dt + 1)) for x in myrange if val in list(range(x, x + dt + 1))
    ][0]


def fun_check_downscaled_results_larger_than_region(
    iam_region,
    iso,
    df_downs: pd.DataFrame,
    df_iam: pd.DataFrame,
    var_list: list,
    upper_threshold: float = 1,
) -> Union[dict, pd.DataFrame]:
    """Checks if  downscaled results are larger than regional data (if `upper_threshold`=1), for a given list of variables `var_list`.
    Or you can change the `upper_threshold` -> e.g. if `upper_threshold=2` returns model/scenario conbinations with downscaled results that are at least twice as much as regional data.
    Returns a dictionary with model/scenarios combinations where `df_downs / df_iam >  upper_thershold` and a dataframe with `df_downs / df_iam`.

    Parameters
    ----------
    iam_region : _type_
        Region in IAM results
    iso : bool
        Country (iso code) to be checked in downscaled results (to be compared with regional iam data)
    df_downs : pd.DataFrame
        Downscaled results
    df_iam : pd.DataFrame
       IAMs results
    var_list: list
        list of variables to be ckecked
    upper_threshold: float
        Upper thershold (e.g. if ==1 checks if downscaled results are larger than 1*regional results), by default 1
    Returns
    -------
    Union[dict,pd.DataFrame]
        Dictionary with downscaled results above region for a given `var_list` and a dataframe with the % of country within region

    Raises
    ------
    ValueError
        If `iso` code not present in downscaled results
    ValueError
        If `iam_region` not present in regional IAMs results
    """
    df_downs = df_downs.copy(deep=True)
    df_iam = df_iam.copy(deep=True)

    # Check index
    df_iam = fun_check_iamc_index_and_region(iam_region, df_iam)
    df_downs = fun_check_iamc_index_and_region(iso, df_downs)

    ratio_all = fun_divide_downs_results_by_regional_results(
        iam_region, iso, df_downs, df_iam
    )

    check_vars = {}
    df_all = pd.DataFrame()
    for v in var_list:
        if v in ratio_all.reset_index().VARIABLE.unique():
            ratio_v = ratio_all.xs(v, level="VARIABLE")
            check = ratio_v[ratio_v > upper_threshold].dropna(how="all")
            if len(check):
                check_vars[v] = [fun_available_scen(check)]
                df = fun_select_model_scenarios_combinations(
                    ratio_all.xs(v, level="VARIABLE", drop_level=False),
                    fun_available_scen(check),
                )
                df_all = pd.concat([df_all, df], axis=0, sort=True)
        else:
            print(f"{v} not available in `df_iam` or `df_downs`")
    return check_vars, df_all


def fun_divide_downs_results_by_regional_results(
    iam_region: str, iso: str, df_downs: pd.DataFrame, df_iam: pd.DataFrame
) -> pd.DataFrame:
    """Returns downscaled results (for a given `iso`) divided by IAM results (for a given `iam_region`), for all variables and all model/scenarios combinations.

    Parameters
    ----------
    iam_region : str
        _description_
    iso : str
        _description_
    df_downs : pd.DataFrame
        _description_
    df_iam : pd.DataFrame
        _description_

    Returns
    -------
    pd.DataFrame
        _description_
    """
    df_downs = df_downs.copy(deep=True)
    df_iam = df_iam.copy(deep=True)
    df_iam = df_iam.xs(iam_region, level="REGION")
    df_downs = df_downs.xs(iso, level="REGION")
    return (df_downs / df_iam).dropna(how="all").replace(np.inf, np.nan)


def fun_sectoral_checks(models_dict, df):
    res_dict = {}
    models = set(fun_flatten_list(models_dict.values()))
    for model in models:
        print(model)
        for project in fun_invert_dictionary(models_dict)[model]:
            df_iam = fun_read_df_iam_from_multiple_df(
                model,
                Path(f"input_data/{project}/multiple_df"),
            )
            cols = [str(x) for x in range(1990, 2200) if str(x) in df_iam.columns]
            df_iam = df_iam[cols]
            targets = df_iam.reset_index().SCENARIO.unique()
            for step in check_consistency_dict:
                for i, j in check_consistency_dict[step].items():
                    res = fun_sectoral_consistency_check(
                        fun_xs(df, {"SCENARIO": list(targets)}).xs(
                            model, level="MODEL", drop_level=False
                        ),
                        df_iam,
                        i,
                        j,
                        coerce_errors=False,
                    )
                    if res[0] > 1.03:
                        res_dict[model] = {step: res}


def fun_regional_country_mapping_as_dict(model: str, project: str, iea_countries_only:bool=False) -> dict:
    """Returns a dictionary with regional-country mapping for a given `model` from a given `project`

    Parameters
    ----------
    model : str
        your selected model
    project : str
        Project (where to find the `default_mapping.csv`)
    iea_countries_only : Optional[bool]
        Wheter we want to select only iea countries, defaults to False
    Returns
    -------
    dict
        Regional-country mapping
    """
    mapping=fun_country_map(
            model,
            CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv",
            CONSTANTS.INPUT_DATA_DIR / f"{project}/default_mapping.csv",
        )
    if iea_countries_only:
        mapping =mapping[mapping.ISO.isin(iea_countries)]
    mydict =mapping.dropna().reset_index().set_index(["ISO"])["REGION"].to_dict()
    return fun_invert_dictionary({k: [v] for k, v in mydict.items()})


def fun_create_regional_variables_as_sum_of_countrylevel_data(
    model: str, project: str, df_merged: pd.DataFrame, new_reg_vars: list
) -> pd.DataFrame:
    """Creates regional data by summing up country level data for a list of variables `new_reg_vars`.
    It calculates the data for all regions based on the regional mapping for a given `model` from a specific `project`.
    It returns the updated dataframe.

    Parameters
    ----------
    model : str
        Your model
    project : str
        Your project folder (where to find the `default_mapping.csv` file)
    df_merged : pd.DataFrame
        Your dataframe with downscaled data.
    new_reg_vars : list
        List of variables (for which you want to calculate regional data)

    Returns
    -------
    pd.DataFrame
        Updated Dataframe (including regional data).
    """
    df_merged = df_merged.copy(deep=True)
    cmap = fun_country_map(
        model,
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv",
        CONSTANTS.INPUT_DATA_DIR / f"{project}/default_mapping.csv",
    ).dropna()
    regions = cmap.REGION.unique()
    for v in new_reg_vars:
        for r in regions:
            clist = list(cmap[cmap.REGION == r].ISO.unique())  # countries
            var_agg_region = fun_aggregate_countries(
                fun_xs(
                    df_merged.reset_index()
                    .set_index(["MODEL", "REGION", "VARIABLE", "UNIT", "SCENARIO"])
                    .xs(v, level="VARIABLE", drop_level=False),
                    {"REGION": clist},
                ),
                f"{model}|{r[:-1]}",
                ["MODEL", "VARIABLE", "UNIT", "SCENARIO"],
                clist,
                remove_single_countries=True,
            )
            var_agg_region = var_agg_region.reset_index().set_index(
                df_merged.index.names
            )
            df_merged = pd.concat([df_merged, var_agg_region], axis=0)
    return df_merged


def fun_line_plots_colors_and_markers(
    df: pd.DataFrame,
    var: str,
    c: Union[str,list],
    color_dim: str = "MODEL",
    marker_dim: str = "SCENARIO",
    palette: str = "tab10",
    growth_rate=False,
) -> plt:
    ms = ("o", "s", "d", "X", "*", "P", "1", "2", "3", "4", "8", 4, 5, 6, 7, 0, 1, 2, 3)
    marker = itertools.cycle(ms)
    
    if isinstance(c, list) or growth_rate:
        print(f'we plot indexed variable for all countries, with  {df.columns[0]}=1')
        df=fun_xs(fun_growth_index(df, df.columns[0]), {'VARIABLE':var}).droplevel('UNIT')
        df=df.assign(UNIT='index').set_index('UNIT', append=True)
        df=fun_xs(df.xs(var, level='VARIABLE'), {'REGION':c})#.droplevel('REGION')
        
    else:
        df = df.xs((c, var), level=("REGION", "VARIABLE"))
    models = df.reset_index()[color_dim].unique()
    scenarios = df.reset_index()[marker_dim].unique()

    pal = sns.color_palette(palette, len(models), as_cmap=False)
    color_list = pal.as_hex()
    col_dict = dict(zip(models, cycle(color_list)))
    scen_marker_dict = {s: next(marker) for s in scenarios}

    fig, ax = plt.subplots()

    for m in models:
        datamodel = df.xs(m, level=color_dim, drop_level=False)
        for s in scenarios:
            if s in datamodel.reset_index()[marker_dim].unique():
                data = datamodel.xs(s, level=marker_dim, drop_level=False)
                for x in ["UNIT", "FILE", "CRITERIA"]:
                    if x in data.index.names:
                        data = data.droplevel(x)
                data.T.plot(
                    color=col_dict[m], marker=scen_marker_dict[s], ax=ax, legend=None
                )
    # ax.legend(handles=col_legend+mark_legend, loc="upper right")
    col_legend = [
        Line2D([0], [0], marker="", color=col_dict[m], label=m) for m in models
    ]
    mark_legend = [
        Line2D([0], [0], marker=scen_marker_dict[s], color="black", label=s, ls="")
        for s in scenarios
    ]

    # Create a legend for `color_dim`.
    color_dim = color_dim.lower().capitalize()
    first_legend = plt.legend(handles=col_legend, loc="lower right", title=color_dim)
    # Add the legend manually to the current Axes.
    ax = plt.gca().add_artist(first_legend)

    # Create another legend for the `marker_dim`.
    marker_dim = marker_dim.lower().capitalize()
    plt.legend(handles=mark_legend, loc="lower left", title=marker_dim)
    unit = df.reset_index().UNIT.unique()[0]
    title=f"{var} - {c}" if isinstance(c, str) or growth_rate else f"Indexed {var} ({df.columns[0]}=1)"
    plt.title(title) 
    plt.ylabel(unit)
    return fig


def fun_analyse_main_drivers_across_scenarios(
    df: pd.DataFrame,
    var: str,
    c: str,
    n: int = 150,
    exclude_similar_variables: bool = False,
):
    """Returns pd.Series with all variables with the highest (squared correlation) found for a given  dependent variable `var`, and for a given country `c`.
    The variables that are found as main drivers across all models/scenarios will appear on top
    (the value is the count of model/scenarios combinations where the independent variable is found as a main (top `n` e.g. top 10) squared correlation)

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe to be analysed
    var : str
        Dependent variable
    c : str
        Selected country
    n : int, optional
        Search for Top `n` variables with the highest correlation , by default 150
    exclude_similar_variables : bool, optional
        Exclude variables with a similar name of `var`, by default False

    Returns
    -------
    pd.Series
        Series with list of variables with the highest correlation. Value is the count of model/scenario combinations when a variable is
    """
    res = pd.concat(
        [
            (
                pd.DataFrame(
                    {
                        scen: {
                            x: model
                            for x in fun_find_drivers(
                                df,
                                var,
                                c,
                                scen,
                                model,
                                n=n,
                                exclude_similar_variables=exclude_similar_variables,
                            )
                        }
                        for scen in df.reset_index().SCENARIO.unique()
                    }
                )
            )
            for model in df.reset_index().MODEL.unique()
        ],
        axis=1,
    )
    return res.T.count().sort_values(ascending=False).replace(0, np.nan).dropna()


def fun_find_drivers(df, var, c, scen, model, n=200, exclude_similar_variables=True):
    # NOTE this function was inspired by https://safjan.com/features-with-strong-correlation/
    df = df.copy(deep=True)
    df = fun_index_names(df, True, int)

    for x in ["UNIT", "CRITERIA", "FILE"]:
        if x in df.index.names:
            df = df.droplevel(x)
        elif x in df.columns:
            df = df.drop(x, axis=1)
    corr = (
        df.xs(c, level="REGION")
        .xs(scen, level="SCENARIO")
        .xs(model, level="MODEL")
        .T.corr()
    )

    corr_matrix = corr.replace(1, np.nan)
    # Get the top n pairs with the highest correlation
    top_pairs = corr_matrix.unstack().sort_values(ascending=False)[: n * 2]

    # Create a list to store the top pairs without duplicates
    unique_pairs = []

    # Iterate over the top pairs and add only unique pairs to the list
    for pair in top_pairs.index:
        condition = True
        if exclude_similar_variables:
            condition = not set(pair[1].rsplit("|")).intersection(
                set(pair[0].rsplit("|"))
            )
        if pair[0] != pair[1] and condition and (pair[1], pair[0]) not in unique_pairs:
            unique_pairs.append(pair)

    # Create a dataframe with the top pairs and their correlation coefficients
    top_pairs_df = pd.DataFrame(columns=["feature_1", "feature_2", "corr_coef"])
    for i, pair in enumerate(unique_pairs[:n]):
        top_pairs_df.loc[i] = [pair[0], pair[1], corr_matrix.loc[pair[0], pair[1]]]

    if var is not None:
        # Returns the top pairs associated to a given var
        return list(
            top_pairs_df[(top_pairs_df.feature_2 == var)].feature_1.unique()
        ) + list(top_pairs_df[(top_pairs_df.feature_1 == var)].feature_2.unique())
    # Otherwise return all top_pairs
    return top_pairs_df


def fun_log_log_graphs(
    model: str = "REMIND-MAgPIE 3.0-4.4",
    region: str = "EU 28r",
    countrylist: list = fun_eu28(),
    sector: str = "Final Energy",
    palette: list = "GnBu",
    grey_countries: Optional[list] = ["CHN", "IND", "USA"],
    black_countries: Optional[list] = None,
    # If project=None will not show regional IAMs results
    project: Optional[str] = "NGFS_2022",
    # If file_suffix=None will not show future projections
    file_suffix: Optional[str] = "NGFS_2022_Round_2nd_version2_harmo",
    scenario="h_ndc",
    scale: float = 1e4,
    style: Optional[int] = None,
    ls_dict: dict = None,  # {'ENSHORT_HIST':'--'}
    step1_sensitivity: bool = False,
    step1_extended_sens: bool = False,
    sel_method: Optional[list] = None,
    remove_countries_wo_hist_data: bool = True,
    func_type="log-log",
):
    grey_countries = grey_countries if grey_countries is not None else []
    black_countries = black_countries if black_countries is not None else []

    if style is None:
        plt.rcParams["axes.facecolor"] = "white"
    else:
        plt.style.use(plt.style.available[style])

    # Historical data
    df = pd.read_csv(CONSTANTS.INPUT_DATA_DIR / "Historical_data.csv")
    ktoe_to_ej = 0.041868 * 1e-3
    df["EI"] = df["TFC"] / df["GDP|PPP"] * 100 * ktoe_to_ej
    df_graph = df[df.ISO.isin(grey_countries + countrylist)] if countrylist is not None else df
    df_graph=df_graph[["ISO", "GDPCAP", "EI", "TIME"]]
    df_graph = df_graph.set_index("ISO")
    df_graph = df_graph[df_graph.GDPCAP != "missing"]
    df_graph.loc[:, "GDPCAP"] = [float(x) for x in df_graph.GDPCAP]

    # Define default country-level colors in a dictionary:
    coldict = {c: "black" for c in black_countries}
    coldict.update({c: "grey" for c in grey_countries})
    # cmap = plt.get_cmap(palette, len(countrylist))
    # countrycol=set([x for x in countrylist if x not in black_countries+grey_countries]+list(df_graph.index.unique()))
    # cmap = plt.get_cmap(palette, len(countrycol))
    countrycol = set(countrylist + list(df_graph.index.unique()))
    countrycol = [x for x in countrycol if x not in black_countries + grey_countries]
    cmap = plt.get_cmap(palette, len(countrycol))
    for n, c in enumerate(countrycol):
        # if c in grey_countries:
        #     coldict.update({c:'grey'})
        # elif c in black_countries:
        #     coldict.update({c:'black'})
        # else:
        coldict.update({c: cmap(n)})

    if sector != "Final Energy":
        scale = 100
        d = SliceableDict(iea_flow_dict)
        # NOTE groupby should be pu inside  fun_read_energy_iea_data_in_iamc_format
        a = fun_read_energy_iea_data_in_iamc_format(
            d.slice(sector, dict_y_den[sector]), verbose=False
        )
        a = fun_index_names(a, True, int)
        num = (
            fun_xs(a, {"REGION": countrylist})
            .xs(sector, level="VARIABLE")
            .droplevel(["MODEL", "SCENARIO", "UNIT"])
        )
        den = (
            fun_xs(a, {"REGION": countrylist})
            .xs(dict_y_den[sector], level="VARIABLE")
            .droplevel(["MODEL", "SCENARIO", "UNIT"])
        )
        ei = (num / den).unstack().dropna()
        ei.index.names = ["TIME", "ISO"]
        ei = ei.reset_index().set_index(["TIME", "ISO"]).rename({0: "EI"}, axis=1)

        df_graph = df_graph.reset_index().set_index(["TIME", "ISO"])
        # df_graph.loc[:, 'EI']= ei.loc[df_graph.index, 'EI']
        df_graph = pd.concat([ei, df_graph.loc[ei.index, ["GDPCAP"]]], axis=1)
        df_graph = df_graph.reset_index("TIME").dropna()

    # Plotting historical data below (grey, black and all colorful countries) as dots
    for n, c in enumerate(df_graph.index.unique()):
        plt.scatter(
            x=df_graph.loc[c, "GDPCAP"],
            y=scale * df_graph.loc[c, "EI"],
            color=coldict.get(c, cmap(n)),
        )
        if coldict.get(c, cmap(n)) in ["grey", "black"]:
            plt.text(
                1.05 * df_graph[df_graph.TIME == 1980].loc[c, "GDPCAP"],
                1.05 * scale * df_graph[df_graph.TIME == 1980].loc[c, "EI"],
                c,
                color=coldict.get(c, cmap(n)),
                fontsize=10,
                weight="bold",
            )

    # Future downscaled results below (lines)
    if sector != "Final Energy":
        scale = 1000
    if file_suffix is not None and scenario is not None and project is not None:
        step1_file = fun_get_step1_iamc_fname(model, region, project, file_suffix)

        fun_plotting_downscaled_results_log_log(
            project,
            model,
            region,
            set(countrylist + grey_countries + black_countries),
            step1_file,
            scale,
            df_graph,
            coldict,
            cmap,
            scenario,
            ls_dict=ls_dict,
            step1_sensitivity=step1_sensitivity,
            step1_extended_sens=step1_extended_sens,
            sel_method=sel_method,
            sector=sector,
            remove_countries_wo_hist_data=remove_countries_wo_hist_data,
            func_type=func_type,
        )

    # Plotting IAM results below as red line
    if project is not None:
        fun_regional_log_log_graph(model, region, project, scale, scenario, sector)
        plt.title(f"{sector} / {dict_y_den[sector]} [{region} - {scenario}]")
    else:
        plt.title(f"{sector} / {dict_y_den[sector]}")
    # Apply style, title etc.
    if style is None:
        plt.grid(c="#E3E3E3")
        ax = plt.axes()
        ax.spines["bottom"].set_color("black")
        ax.spines["top"].set_color("black")
        ax.spines["right"].set_color("black")
        ax.spines["left"].set_color("black")

    plt.ylim(top=1.05e2, bottom=0.3)
    plt.xlim(right=2e2, left=1e-1)
    # if project:
    #     plt.title(f"{sector} / {dict_y_den[sector]} [{region} - {scenario}]")
    # else:
    #     plt.title(f"{sector} / {dict_y_den[sector]}")
    ylab = "EJ / Billion USD" if sector == "Final Energy" else "Percent"
    xlab="Thousand USD per capita"

    if func_type=='log-log':
        plt.yscale("log")
        plt.xscale("log")
        ylab=f"{ylab} (log scale)"
        xlab=f"{xlab} (log scale)"
    else:
        plt.ylim(top=1.05e2, bottom=0)

    plt.ylabel(f"{ylab}")
    plt.xlabel(f"{xlab}")
    # plt.legend(df_graph.index.unique())
    plt.tight_layout()
    return plt


def fun_get_step1_iamc_fname(model, region, project, file_suffix):
    if isinstance(file_suffix, list) and len(file_suffix) == 1:
        print(f"Plotting: {file_suffix[0]}")
        step1_file = file_suffix[0].replace(f"{region}_", "")
        step1_file = step1_file.replace(f"{model}_", "")
        step1_file = step1_file.replace(".csv", "")
    else:
        step1_file = fun_step1_file_name(file_suffix, project, model)
    return step1_file


def fun_plotting_downscaled_results_log_log(
    project,
    model,
    region,
    countrylist,
    file_suffix,
    scale,
    df_graph,
    coldict,
    cmap,
    scenario,
    ls_dict: dict = None,  # {'ENSHORT_HIST':'--'}
    step1_sensitivity: bool = False,
    step1_extended_sens: bool = False,
    sel_method: Optional[list] = None,
    sector: Optional[str] = "Final Energy",
    remove_countries_wo_hist_data: bool = True,
    func_type='log-log'
):
    if remove_countries_wo_hist_data:
        countrylist = df_graph.dropna().index.unique()

    if step1_extended_sens:
        fname = (
            CONSTANTS.CURR_RES_DIR("Energy_demand_downs_1.py")
            / project
            / "IAMC_DATA"
            / f"{model}_{region}_{file_suffix}_IAMC_sensitivity_analysis.csv"
        )
        df_downs2 = fun_read_csv({model: fname}, True, int)[model]
        # if sel_method
        # df_downs2=fun_xs(df_downs2, {'METHOD':sel_method})
        if sel_method is not None:
            av_methods=df_downs2.reset_index().METHOD.unique()
            if sel_method not in av_methods:
                txt=f"Your selected method {sel_method} is not present in the dataframe. Methods available: {av_methods}"
                raise ValueError(f"{txt}. Or maybe this error could be fixed by passing `step1_extended_sens=False` ")
            df_downs2 = fun_xs(df_downs2, {"METHOD": sel_method + ["none"]})

        df_downs2=fun_xs(df_downs2, {'FUNC':func_type})
        # num=df_downs2.xs('Final Energy', level='VARIABLE')
        # den=df_downs2.xs('GDP|PPP', level='VARIABLE')
        # country_ei=num.droplevel(['CONVERGENCE','METHOD'])/den.droplevel(['CONVERGENCE','METHOD'])
        # country_ei=country_ei.xs(scenario, level='TARGET').droplevel('MODEL').unstack('ISO')

        # # Number of methods used in sensitivity analysis
        # no_methods=len(country_ei.loc[2010,country_ei.reset_index().ISO.unique()[0]])

        num = df_downs2.xs("GDP|PPP", level="VARIABLE")
        den = df_downs2.xs("Population", level="VARIABLE")
        country_gdpcap = num.droplevel(["CONVERGENCE", "METHOD"]) / den.droplevel(
            ["CONVERGENCE", "METHOD"]
        )
        country_gdpcap = (
            country_gdpcap.xs(scenario, level="TARGET")
            .droplevel("MODEL")
            .unstack("ISO")
        )

        # blendcols = [x for x in df_downs2 if "BLEND" in x]
        # if not ls_dict:
        #     ls_dict = {}

        # blendcols_plot = blendcols + list(ls_dict.keys())
        # blendcols_plot=[x for x in blendcols_plot if 'EI_2' not in x]

        # for n, c in enumerate(df_graph.index.unique()):
        methodlist = [x for x in df_downs2.reset_index().METHOD.unique() if x != "none"]

        # Renanme str(2100) as int(2100) and Create convlist
        convlist = [
            x for x in df_downs2.reset_index().CONVERGENCE.unique() if x != "none"
        ]
        df_downs2 = df_downs2.rename(
            {x: int(x) for x in convlist if isinstance(x, str)}, level="CONVERGENCE"
        )
        convlist = [
            x for x in df_downs2.reset_index().CONVERGENCE.unique() if x != "none"
        ]

        scale = scale / 10 if sector == "Final Energy" else scale
        already_present = []  # ISO labels already present in the plot
        for method in methodlist:
            num = df_downs2.xs(sector, level="VARIABLE")
            den = df_downs2.xs(dict_y_den[sector], level="VARIABLE")
            num = (
                num.xs(method, level="METHOD")
                if method in num.reset_index().METHOD.unique()
                else num.xs("none", level="METHOD")
            )
            den = (
                den.xs(method, level="METHOD")
                if method in den.reset_index().METHOD.unique()
                else den.xs("none", level="METHOD")
            )
            for conv in convlist:
                try:
                    numconv = (
                        num.xs(conv, level="CONVERGENCE")
                        if conv in num.reset_index().CONVERGENCE.unique()
                        else num.xs("none", level="CONVERGENCE")
                    )
                except:
                    numcomv = 1
                    print("error")
                denconv = (
                    den.xs(conv, level="CONVERGENCE")
                    if conv in den.reset_index().CONVERGENCE.unique()
                    else den.xs("none", level="CONVERGENCE")
                )
                country_ei = numconv / denconv
                country_ei = (
                    country_ei.xs(scenario, level="TARGET")
                    .droplevel("MODEL")
                    .unstack("ISO")
                )

                # for n, c in enumerate(country_ei.reset_index().ISO.unique()):
                # if not remove_countries_wo_hist_data:
                #     countrylist= country_ei.reset_index().ISO.unique()

                country_ei = country_ei[
                    country_ei.index.get_level_values("ISO").isin(countrylist)
                ]
                country_gdpcap = country_gdpcap[
                    country_gdpcap.index.get_level_values("ISO").isin(countrylist)
                ]
                
                # GDP per capita correction (and EI correction if sector=="Final Energy")
                country_gdpcap, country_ei=correct_gdp_and_ei(df_graph, country_gdpcap, country_ei, sector)

                # Plot ISO name for countries with highest/lowest EI or GDPCAP

                # We get an full list of txt_country (for which we should add ISO legend)
                [
                    fun_add_iso_label_to_plot(
                        countrylist,
                        scale,
                        coldict,
                        cmap,
                        country_gdpcap,
                        already_present,
                        x,
                    )
                    for x in [country_ei, country_ei.xs(2010, drop_level=False)]
                ]

                for n, c in enumerate(countrylist):
                    if c in country_ei.reset_index().ISO.unique():
                        # sel_ls = (
                        #     ls_dict.get(x, "-") if ls_dict is not None else "-"
                        # )  # if k == "df_downs" else "dotted"
                        sel_ls = "-"
                        plt.plot(
                            country_gdpcap.loc[country_ei.index].xs(c, level="ISO"),
                            scale * country_ei.xs(c, level="ISO"),
                            # country_ei.xs(c, level="ISO"),
                            color=coldict.get(c, cmap(n)),
                            alpha=0.5,
                            lw=1,
                            ls=sel_ls,
                        )

        # This one needs to read from step1 sensitivity file
        # BLUEPRINT
        # 1 -[x] Read in IAMc format
        # 2 -[ ] Select sector and y_den
        # 3 -[ ] add GDP|PPP and population to df
        # 4 -[x] Create `num` and `den` and Melt both of them to PD.Series with [TIME,ISO] as multindex
        # 5 -[ ] Plot

        # _tocols = [
        #     "ENSHORT_REF_to_ENLONG_RATIO_2050_thenENLONG_RATIO",
        #     "ENSHORT_REF_to_ENLONG_RATIO_2100_thenENLONG_RATIO",
        # ]
        # _from = "ENSHORT_REF"
        # for _to in _tocols:
        #     res_dict[f"{_from}_{_to}"] = fun_blending(
        #         df_downs, step1_sensitivity, None, None, _from, _to
        #     )
    else:
        df_downs = pd.read_csv(
            CONSTANTS.CURR_RES_DIR("Energy_demand_downs_1.py")
            / f"{model}_{region}_{file_suffix}.csv"
        )

        # df=df[df_downs.FUNC==func_type]
        df_downs=df_downs[df_downs.FUNC==func_type]
        df_downs = fun_blending(df_downs, step1_sensitivity, None, None)
        # df_downs= fun_blending_with_sensitivity(df_downs)
        res_dict = {}

        res_dict["df_downs"] = df_downs[df_downs.TARGET == scenario]
        df_downs_scen = df_downs[df_downs.TARGET == scenario]
        blendcols = [x for x in df_downs_scen if "BLEND" in x]
        if not ls_dict:
            ls_dict = {}

        blendcols_plot = blendcols + list(ls_dict.keys())
        blendcols_plot = [x for x in blendcols_plot if "EI_2" not in x]
        if not step1_sensitivity:
            conv_dict, scen_dict, sensitivity_dict = fun_read_config_file(
                CONSTANTS.INPUT_DATA_DIR / project / "scenario_config.csv"
            )
            selected=[x.upper() for x in fun_invert_dictionary(conv_dict)[scenario]][0]
            selected=fun_conv_settings(False)['Final'][selected]
            blendcols_plot=[x for x in blendcols if str(selected) in x]

        if sel_method is not None:
            if any([x in sel_method for x in ['ENSHORT_REF', 'ENLONG_RATIO', 'ENSHORT_HIST', 'ENLONG']]):
                blendcols_plot=sel_method

        for k, df in res_dict.items():
            for x in blendcols_plot:
                for n, c in enumerate(df_graph.index.unique()):
                    if not step1_extended_sens:
                        country_ei, country_gdpcap = fun_energy_intensity(df[df.TARGET==scenario], x, s_num=sector, s_den=dict_y_den[sector])
                    
                    # GDP correction to match 2010 data                   
                    # country_gdpcap, country_ei=correct_gdp_and_ei(df_graph, country_gdpcap, country_ei, sector)
                    country_gdpcap, country_ei=correct_gdp_and_ei(df.reset_index('TIME'), country_gdpcap, country_ei, sector)
                    # NOTE: check the UNIT 
                    if c in df.reset_index().ISO.unique():
                        sel_ls = ls_dict.get(x, "-") if k == "df_downs" else "dotted"
                        plt.plot(
                            country_gdpcap.xs(c ).unique(),
                            scale * country_ei.xs(c).unique(),
                            color=coldict.get(c, cmap(n)),
                            alpha=0.5,
                            lw=2,
                            ls=sel_ls,
                        )
    return plt.plot


def fun_add_iso_label_to_plot(
    countrylist, scale, coldict, cmap, country_gdpcap, already_present, country_ei
) -> plt.plot:
    txt_country = []
    d = {
        "country_ei": country_ei,
        "country_gdpcap": country_gdpcap,  #
        # 'spike':country_ei/country_ei.loc[2010]
    }

    for k, x in d.items():
        # country_ei[country_ei.eq(country_ei.max())|country_ei.eq(country_ei.min())].reset_index().ISO.unique()
        # minval= x[x.eq(x.min())].reset_index().ISO.unique()[0]
        txt_country += list(x[x.eq(x.max()) | x.eq(x.min())].reset_index().ISO.unique())
    txt_country = set(txt_country)

    txt_dict = {}
    for c in txt_country:
        c_dict = {}
        for k, x in d.items():
            name = k if k != "spike" else "country_ei"
            if c in list(x[x.eq(x.max())].reset_index().ISO.unique()):
                if k == "spike":
                    c_dict[name] = (x[x.eq(x.max())] * d["country_ei"].loc[2010])[0]
                else:
                    c_dict[name] = x[x.eq(x.max())].droplevel(0).loc[c]

            elif c in list(x[x.eq(x.min())].reset_index().ISO.unique()):
                if k == "spike":
                    c_dict[name] = (x[x.eq(x.min())] * d["country_ei"].loc[2010])[0]
                else:
                    c_dict[name] = x[x.eq(x.min())].droplevel(0).loc[c]
                    # txt_dict[f"{name}_{c}"]=x[x.eq(x.min())].droplevel(0).loc[c]
            txt_dict[c] = c_dict

    for n, c in enumerate(countrylist):
        # if c in country_ei.reset_index().ISO.unique():
        if c in txt_country and c not in already_present:
            try:
                gdpcap_val = txt_dict[c].get("country_gdpcap")
                if gdpcap_val is None:
                    gdpcap_val = country_gdpcap.xs(c, level="ISO").loc[
                        country_ei[country_ei.eq(txt_dict[c]["country_ei"])].index[0][0]
                    ]
                ei_val = txt_dict[c].get("country_ei")
                if ei_val is None:
                    ei_val = country_ei.xs(c, level="ISO").loc[
                        country_gdpcap[
                            country_gdpcap.eq(txt_dict[c]["country_gdpcap"])
                        ].index[0][0]
                    ]
                plt.text(
                    1.02 * gdpcap_val,
                    1.02 * scale * ei_val,
                    c,
                    color=coldict.get(c, cmap(n)),
                    fontsize=10,
                    weight="bold",
                )
            except:
                pass
            already_present += already_present + [c]
    return plt


def fun_energy_intensity(
    df_downs_scen: pd.DataFrame,
    col: str,
    s_num: str = "Final Energy",
    s_den: str = "GDP|PPP",
) -> pd.DataFrame:
    """Calculates energy intensity based on the results from step 1 dataframe (without using alpha and beta from regression)

    Parameters
    ----------
    df_downs_scen : pd.DataFrame
        Dataframe from step1
    col : str
        Column you are looking at e.g. `ENSHORT_HIST`
    s_num : str, optional
        Sector you are looking at,  by default 'Final Energy'
    s_den : str, optional
        Denominator to calculate the intensity of sector `s_num`, by default 'GDP|PPP'

    Returns
    -------
    pd.DataFrame
        Returns Energy intensity and gdp per capita
    """
    if s_den == "GDP|PPP":
        den = df_downs_scen[df_downs_scen.SECTOR == s_num][s_den]
    else:
        den = df_downs_scen[df_downs_scen.SECTOR == s_den][col]
    country_ei = (df_downs_scen[(df_downs_scen.SECTOR == s_num)][col] / den) / 10
    country_gdpcap = (
        df_downs_scen[(df_downs_scen.SECTOR == "Final Energy")]["GDP|PPP"]
        / df_downs_scen[df_downs_scen.SECTOR == "Final Energy"]["Population"]
    )

    return country_ei, country_gdpcap


def fun_regional_log_log_graph(model, region, project, scale, scenario, sector):
    iam_dir = CONSTANTS.INPUT_DATA_DIR / project / "multiple_df"
    df_iam = fun_read_df_iam_from_multiple_df(model, iam_dir)
    df_iam = fun_index_names(df_iam, True, int)
    df_iam = df_iam.xs(f"{model}|{region[:-1]}", level="REGION").droplevel("UNIT")
    iam_ei = (
        1e2
        * df_iam.xs(sector, level="VARIABLE")
        / df_iam.xs(dict_y_den[sector], level="VARIABLE")
    )

    iam_gdpcap = df_iam.xs("GDP|PPP", level="VARIABLE") / df_iam.xs(
        "Population", level="VARIABLE"
    )

    scale = scale / 1e3 if sector == "Final Energy" else 1
    return plt.plot(
        iam_gdpcap.xs(scenario, level="SCENARIO").T,
        (scale * iam_ei.xs(scenario, level="SCENARIO")).T,
        color="red",
        marker=None,
        lw=5,
    )


def fun_step1_file_name(step5_file: str, project: str, model: str) -> str:
    """Infer step1 csv file name from a given file name coming from a different step (e.g. step5).
    Returns the step1 file name as f"{model}_{date}_harmo.csv", by trying to infer the date from the `step5_file` name.

    Parameters
    ----------
    step5_file : str
        Your initial file name (e.g. from step5)
    project : str
        Project name (e.g. NGFS_2023)
    model : str
        Model name

    Returns
    -------
    str
        Step1 file name

    Raises
    ------
    ValueError
        If cannot find a date in the `step5_file` name
    """
    mytext = step5_file.replace(model, "").replace(project, "").split("_")
    step1_file = [x for x in mytext if x.isnumeric()][:3]
    step1_file = f"{'_'.join(step1_file)}_harmo"
    if step1_file == "_harmo":
        raise ValueError(
            f"Unable to infer the `step1_file` name, based on the `file` name you provided : {step5_file}. \n "
            "`file` Should contain a date e.g. 2023_05_28 if you want to plot the log-log graphs. "
            "Otherwise please select `log_log_graphs=False`."
        )
    return step1_file


def fun_find_nearest_values(mylist: list, target_value: float, n_max=2):
    res = {}
    for x in mylist:
        #  x/1e9 avoid repeating the same results for different x values
        res[(x - target_value) ** 2 + x / 1e9] = x
    r = list(res.keys())
    r.sort()

    res_sort = [res[r[i]] for i in range(n_max)]
    res_sort.sort()
    return res_sort


def fun_read_df_from_step1(file, countrylist, models, project):
    df = pd.DataFrame()
    missing_reg = []
    missing_targets = {}
    for model in models:
        df_iam = fun_read_df_iams(project, [model])
        iam_scenarios = df_iam.reset_index().SCENARIO.unique()
        step1_file = file  # fun_step1_file_name(file, project, model)
        regions_all = fun_regional_country_mapping_as_dict(model, project)
        regions_to_be_downs = {k: v for k, v in regions_all.items() if len(v) > 1}
        regions = regions_to_be_downs
        if countrylist is not None:
            regions = fun_get_iam_regions_associated_with_countrylist(
                project, countrylist, model
            )
            regions = fun_invert_dictionary({k: [v] for k, v in regions.items()})
            # Exclude Native countries (we do not have csv file for native countries)
            regions = {k: v for k, v in regions.items() if k in regions_to_be_downs}
        for region in regions:
            read_file = [x for x in step1_file if region in x]
            if len(read_file) == 0 and len(regions[region]) > 1:
                missing_reg = missing_reg + [region]
            elif len(read_file) > 1:
                txt = "We found multiple step1 files for the"
                raise ValueError(f" {txt} {region} region: {read_file}")
            elif len(read_file) == 0:
                txt = f"We cannot find {region} region for {model}. We only found: {step1_file} in the most recent files."
                action = input(
                    f"{txt}: {missing_targets}. \n Do you want to continue y/n?"
                )
                if action.lower() not in ["yes", "y"]:
                    raise ValueError("Aborted by the user")
                return pd.DataFrame()
            else:
                df_read = pd.read_csv(CONSTANTS.CURR_RES_DIR("step1") / read_file[0])
                df_read["MODEL"] = model
                av_scen = df_read.TARGET.unique()
                # Check available targets:
                if len(set(av_scen) ^ (set(iam_scenarios))):
                    missing_targets[region] = set(av_scen) ^ (set(iam_scenarios))
                df = pd.concat([df, df_read])
        if len(missing_reg):
            t0 = f"Unable to find these regions for {model}:"
            t1 = "NOTE: All of these regions comprise more than 1 country (they should be all downscaled)."
            t2 = f"However we only checked regions associated to this countrylist: {countrylist}."
            t3 = " If you want to check all regions please specify `countrylist=None`"
            final_t = f"{t0} {missing_reg}, in these step1 files: {step1_file}. {t1} "
            final_t = f"{final_t} {t2} {t3}" if countrylist else final_t
            raise ValueError(final_t)
        if len(missing_targets):
            txt = f"Some scenarios are missing for some regions in this file {file}"
            action = input(f"{txt}: {missing_targets}. \n Do you want to continue y/n?")
            if action.lower() not in ["yes", "y"]:
                raise ValueError("Aborted by the user")
            # elif action in ["no", "n"]:
            #     raise ValueError(f"{model} is not available in the default mapping")
            # else:
            #     df_mapping[f"{model}.REGION"] = df_mapping[action]

            # raise ValueError(f"{txt}: {missing_targets}")

    return df


def fun_get_models(project: str, sub_folder: str = "multiple_df", nrows=1) -> list:
    """
    Retrieve the list of unique models from CSV files within a specified project directory.

    This function reads the first `nrows` rows of each CSV file located in the `input_data/project/multiple_df` 
    directory. It extracts and compiles a list of unique models from the files.
    
    Parameters
    ----------
    project : str
        Project folder (e.g. NGFS_2023)
    sub_folder : str, optional
        Folder with regional IAMs data (saved as CSV files), by default "multiple_df"
    nrows : int, optional
        The number of rows to read from each CSV file, by default 1. Reading only the first few rows 
        can improve performance if the files are large.

    Returns
    -------
    List[str]
        A list of unique model names found in the specified CSV files.
    
    Notes
    -----
    - The function assumes that each CSV file contains a column with the name 'Model' (case insensitive).
    - It scans the specified subdirectory within the project's input data directory for CSV files 
      and compiles a list of unique model names across all files.
    - The CONSTANTS.INPUT_DATA_DIR is assumed to be a predefined constant pointing to the base input directory.

    Example
    -------
    ```python
    models = fun_get_models("NGFS_2023")
    print(models)
    ```
    """
    folder = CONSTANTS.INPUT_DATA_DIR
    l = [x for x in os.listdir(folder / project / sub_folder) if ".csv" in x]
    models=[]
    for x in l:
        if nrows is not None:
            mydf=pd.read_csv(folder / project / sub_folder / x, nrows=nrows)
        else:
            mydf=pd.read_csv(folder / project / sub_folder / x)
        sel_col=[i for i in mydf.columns if i.lower()=='model'][0]
        models+=list(mydf[sel_col].unique())
    return list(set(models))


def fun_get_scenarios(project: str, sub_folder: str = "multiple_df") -> list:
    """Returns the list of scenarios for a given `project`. It reads the CSV files
    contained in the `input_data/project/multiple_df` folder, and returns a list of scenarios.

    Parameters
    ----------
    project : str
        Project folder (e.g. NGFS_2022)
    sub_folder : str, optional
        Folder with regional IAMs data (saved as CSV files), by default "multiple_df"

    Returns
    -------
    list
        list of scenarios
    """
    folder = CONSTANTS.INPUT_DATA_DIR
    l = [x for x in os.listdir(folder / project / sub_folder) if ".csv" in x]
    return pd.concat(
        [pd.read_csv(folder / project / sub_folder / x) for x in l]
    ).SCENARIO.unique()


def fun_from_step2_to_step1b_format_single_var(
    df_all: pd.DataFrame, var: str, reverse: bool = False
) -> pd.DataFrame:
    """Changes df format from step2 to step1b for a given variable `var` (e.g. ENSHORT_REF). If `reverse=True` it does
    the opposite (changes from step1b to step2 format)

    Parameters
    ----------
    df_all : pd.DataFrame
        Your dataframe
    var : str
        Your column variable (e.g. ENSHORt_REF)
    reverse : bool, optional
        Reverse operation, by default False

    Returns
    -------
    pd.DataFrame
        _description_
    """
    if reverse:
        return df_all.rename(
            {x: f"{x}{var}" for x in df_all.reset_index().VARIABLE.unique()}, axis=0
        )[var].unstack()
    short = pd.DataFrame(
        df_all[[x for x in df_all.columns if var in x]].stack()
    ).rename({0: var}, axis=1)
    short.index.names = ["TIME", "ISO", "VARIABLE"]
    return short.rename(
        {x: x.replace(var, "") for x in short.reset_index().VARIABLE.unique() if x.endswith(var)}, axis=0
    )


def fun_from_step2_to_step1b_format(
    df: pd.DataFrame,
    target: Optional[str],
    cols: list = ["ENSHORT_REF", "ENLONG_RATIO"],
    reverse: bool = False,
) -> pd.DataFrame:
    """Changes `df` format from step2 to step1b for all variables in `cols`. If `reverse=True` it does
    the opposite (changes from step1b to step2 format)

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe
    target:str
        Scenario (e.g. 'h_cpol')
    cols : list, optional
        List of columns/variables, by default ['ENSHORT_REF', 'ENLONG_RATIO']
    reverse : bool, optional
        Reverse operation, by default False

    Returns
    -------
    pd.DataFrame
        Updated dataframe format
    """
    res = df.copy(deep=True)
    
    # Detect the `long_term` variable
    long_term=[x for x in cols if x!='ENSHORT_REF'][0]
    # Three lines below: rename `long_term` (variable name) as 'LONG_TERM'
    cols=[ 'LONG_TERM' if long_term in x else x  for x in cols]
    renamedict={x:x.replace(long_term,'LONG_TERM') for x in res.columns if x.endswith(long_term)} 
    res= res.rename(renamedict, axis=1)

    rename_dict = {"VARIABLE": "SECTOR"}
    
    if reverse:
        res = res.droplevel("TARGET") ## this should be `res`!! Not `df`
        res = fun_rename_index_name(res, {v: k for k, v in rename_dict.items()})
        # Line below to be deleted, just for testing
        # fun_from_step2_to_step1b_format_single_var(res, cols[1], reverse=reverse)
    res = pd.concat(
        [
            fun_from_step2_to_step1b_format_single_var(res, x, reverse=reverse)
            for x in cols
        ],
        axis=1,
    )
    if not reverse:
        if target is None:
            raise ValueError("You need to provide a target if `reverse==False`")
        res = fun_rename_index_name(res, rename_dict)
        res["TARGET"] = target
        res = res.reset_index().set_index(["TIME", "ISO", "TARGET", "SECTOR"])
        # Two lines below: rename 'LONG_TERM'  as `long_term` (we go back to original `long_term` variable name)
        renamedict={x:long_term if 'LONG_TERM' in x else x  for x in res.columns}
    if reverse:
        renamedict={x:x.replace('LONG_TERM',long_term) if 'LONG_TERM' in x else x  for x in res.columns}    
    res=res.rename(renamedict, axis=1)
    return res


def run_sector_harmo_enhanced(
    df: pd.DataFrame,
    d: dict,
    x: str,
    df_iam: Optional[pd.DataFrame] = None,
    w:int=1 # 1 is our standard assumption (we do not sum sub-sector -> sum_anyway=False ). 0 means we sum anyway
    # d2:dict= None,
) -> pd.DataFrame:
    """Run sectorial harmonization in your `df` based on a dictionary (`d`), for a given
    variable (`x`), e.g "ENSHORT_REF". Harmoniziation with regional IAMs results will be
    skipped if `df_iam` is None.

    It harmonizes the sub-sectors with the steps below:
    - step0: Makes sure that the main sector `k` is present in the datarame. If not it creates it as the sum of sub-sectors
    - step1: Makes sure that the main sector `k` is consistent with regional IAM results (if `df_iam` is provided)
    - step2: Rescales the sub-sectors `v` proportionally
    - step3: Makes sure that each sub-sector is consistent with regional IAMs results (if `df_iam` is provided)

    NOTE: This function is similar to `run_sector_harmo`. Compared to `run_sector_harmo ` this one
    pereforms a better regional IAMs harmonization (for all variables that are also present in df_iam).
    For this reason it also take a bit longer compared to `run_sector_harmo`.
    Apart from that, the two functions are very similar in terms of performance (try to ensure that
    the sum of sub-sectors matches the main sector).

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe in Step1b format.
        (you can convert your dataframe in step1b format using `fun_from_step2_to_step1b_format` and
        `fun_step2_format_from_iamc`)
    d : dict
        Dictionary with the main sectors as keys and a list of subsectors as values.
        Example:  {Final Energy: ["Final Energy|Electricity", "Final Energy|Liquids"]}
        Dictionary can be created using the function `fun_sub_sectors_dict`
    x : str
        Variable that you want to harminize(e.g. `ENSHORT_REF` or `ENLONG_RATIO`)
    df_iam : pd.DataFrame, Optional
        Dataframe with regional IAMs results (to ensure consisency with regional IAMs results).
        If df_iam is not provided harminzation with IAMs results will be skipped, by default None
    d2: pd.DataFrame, Optional:
        Another dictionary that can be used to clip values, by default None. Example: 'Final Energy|Transportation|Liquids' 
        should be smaller than 'Final Energy|Liquids' (that can be found in dictionart `d`) and smaller than 
        'Final Energy|Transportation' (that is not present in `d` but can be found in dictionary `d2`)   


    Returns
    -------
    pd.DataFrame
        Updated dataframe
    """
    if df_iam is None:
        print(
            "Will skip harmonization with regional IAMs results, because you did not pass a `df_iam`"
        )
    df = df.copy(deep=True)
    dropcols=['COMMIT_HASH', 'COUNTRYLIST','FUNC']
    df_drop=df.iloc[:,df.columns.isin(dropcols)]
    for col in dropcols:
        if col in df.columns:
            df=df.drop(col, axis=1)
    for k, v in d.items():
        # step0 Create variable `k` as the sum of sub-sectors  (`v`)
        if k=='Final Energy':
            df = fun_sum_of_sub_sectors(df, x, k, v, sum_anyway=False)
        else:
            a=fun_sum_of_sub_sectors(df, x, k, v, sum_anyway=False)
            b=fun_sum_of_sub_sectors(df, x, k, v, sum_anyway=True)
            df = a*w+b*(1-w)
        # if k still not present, we skip the block below and return the dataframe
        if k in df[x].reset_index().SECTOR.unique():
            # step1 make sure that `k` (e.g. final energy) matches regional iam results
            df = (
                fun_harmonize_df_with_IAM(df, df_iam, x, k)
                if df_iam is not None
                else df
            )

            # step2 - Rescale the sub-sectors results proportionally, based on the share of  the sum of liquids,gases  divided by the total in each country
            if not len(fun_xs(df, {"SECTOR": v})[x].dropna(how="all")):
                # if the sub-sectors (v) are not present, we create them as the sum of sub-sub-sectors (vv)
                for vv in v:
                    if vv in d:
                        try:
                            df = fun_sum_of_sub_sectors(
                                df, x, vv, d[vv], sum_anyway=True
                            )
                        except:
                            print("fun_sum_of_sub_sectors not working")
                    else:
                        print(f"{vv} not in d.keys(): {d.keys()}")

            v_updated0 = df.xs(k, level="SECTOR")[x].fillna(0)
           
            den = (
                fun_xs(df, {"SECTOR": v})[x]
                .groupby(["TIME", "ISO", "TARGET", "METHOD"])
                .sum()
            )
            # NOTE: den is the sum of sub-sectors by country
            # `ratio` is the % share of each sub-sector (relative to the total of sub-sectors)
            ratio = (
                    fun_xs(df, {"SECTOR": v})[x]
                    # / fun_xs(df, {"SECTOR": v}).groupby(["TIME", "ISO"]).sum()[x]
                    / den.replace(0,np.nan)
                )

            # NOTE: `ratio` (percentage of each sub-sector) will be multiplied by the main sector `v_updated0`:
            # If ratio is Nan we assume is equal to 1
            v_updated = ratio.fillna(1)*v_updated0 # NOTE keep `ratio` on the left hand side!! This maintain the same index
            v_updated = v_updated.dropna()  # this line is needed

            if len(v_updated) == 0:
                raise ValueError(
                    "Unable to rescale sectors, `v_updated` is empty, please check your data"
                )
            try:
                v_updated = v_updated.reset_index().set_index(df.index.names)
            except:
                print("error in updating the data")
            try:
                df.loc[v_updated.index, x] = v_updated
            except:
                print("error in updating the data")

            # step3 - Same as step1 (but for each of the sub_sectors). NOTE: create a function for step1 so that can be re-used here
            for vv in v:
                df = (
                    fun_harmonize_df_with_IAM(df, df_iam, x, vv)
                    if df_iam is not None
                    else df
                )

    return pd.concat([df, df_drop], axis=1)


def fun_sum_of_sub_sectors(df, x, k, v, sum_anyway: bool = False):
    df = df.copy(deep=True)
    idx = ["TIME", "ISO", "TARGET"]
    if "METHOD" in df.reset_index().columns:
        idx = idx + ["METHOD"]
    sum = fun_xs(df, {"SECTOR": v}).groupby(idx)[[x]].sum()
    sum["SECTOR"] = k
    if k in df.reset_index().SECTOR.unique():
        if sum_anyway:
            # df = df.drop(k, level="SECTOR")
            res = pd.concat([df, sum.reset_index().set_index(df.index.names)], axis=0)
            df.loc[res.index, x] = res.loc[:, x]
    else:
        df = pd.concat([df, sum.reset_index().set_index(df.index.names)], axis=0)
    return df


def fun_harmonize_df_with_IAM(df, df_iam, x, k):
    df = df.copy(deep=True)
    regions = df_iam.reset_index().REGION.unique()
    if len(regions) != 1:
        txt = "df_iam should contain only one region. It contains"
        raise ValueError(f"{txt} {len(regions)}: {regions}")
    iam_sectors = df_iam.reset_index().VARIABLE.unique()
    df_sectors = df.reset_index().SECTOR.unique()
    txt = "This variable will be not harmonized to match regional IAMs results"
    if k not in iam_sectors:
        print(f"Cannot find {k} in `df_iam`. {txt}")
        return df
    if k not in df_sectors:
        print(f"Cannot find {k} in `df`. {txt}")
        return df

    # Check if there are missing data/years in the `df.index`. (because the df.stack() method drops np.nan)
    num = (
        df.xs(k, level="SECTOR")[x].groupby(["TIME", "TARGET"]).sum().unstack("TARGET")
    )

    # Use proxi variable if data is equal to zero across all countries for a given variable `k`
    if np.prod(num == 0)[0]:
        if len(df.reset_index().TIME.unique())>1:
            if len(df.xs(k, level='SECTOR')[x].replace(0, np.nan).dropna())>1:
                df = fun_fill_zero_data_with_proxi_variable(df, x, k)

    # Append time `t` if  missing in the df.index, using values from closest time periods (e.g. if 2045 is missing, append dataframe using 2040 values)
    df = fun_append_missing_time_index(df, df_iam, x, k)
    set_idx = ["TIME", "TARGET"]
    if "METHOD" in df.reset_index().columns:
        set_idx = ["TIME", "TARGET"] + ["METHOD"]
    u = [x for x in set_idx if "TIME" not in x]

    num = df.xs(k, level="SECTOR")[x].groupby(set_idx).sum().unstack(u)

    # Calculates ratio to harmonize df with regiobal iam results
    try:
        ratio = 1 / (
            num.replace(0,np.nan) / df_iam.xs(k, level="VARIABLE").droplevel(["UNIT", "REGION", "MODEL"]).T
        )
    except:
        a=1
    ratio = ratio.replace(np.inf, np.nan)
    k_updated = (
        df.xs(k, level="SECTOR")[x].unstack("TIME").reset_index().set_index(u + ["ISO"])
        * ratio.T
    )
    k_updated = k_updated.stack(dropna=False)
    k_updated.index.names = ["TIME" if x is None else x for x in k_updated.index.names]

    if isinstance(k_updated, pd.Series):
        k_updated = pd.DataFrame(k_updated).rename({0: x}, axis=1)
    k_updated["SECTOR"] = k
    k_updated.reset_index().set_index(df.index.names)
    k_updated = k_updated.reset_index().set_index(df.index.names)
    if 2005 in k_updated.index and 2005 not in df.index:
        k_updated = k_updated.drop(2005)
    df.loc[df.index.isin(k_updated.index),x]=k_updated.loc[k_updated.index, x]
    return df


def fun_append_missing_time_index(
    df: pd.DataFrame, df_iam: pd.DataFrame, col: str, var: str
) -> pd.DataFrame:
    """Append missing time index in a dataframe (long format , e.g. step1b format)

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe in step1b format
    df_iam : pd.DataFrame
        Regional IAMs results (needed to check time availability)
    col : str
        Column e.g. `ENSHORT_REF`
    var : str
        Variable e.g. `Final Energy`

    Returns
    -------
    pd.DataFrame
        Updated dataframe
    """
    num = (
        df.xs(var, level="SECTOR")[col]
        .groupby(["TIME", "TARGET"])
        .sum()
        .unstack("TARGET")
    )
    if 2005 in df_iam.columns:
        df_iam=df_iam.drop(2005, axis=1)
    time_missing = list(set([t for t in num.index]) ^ (set(df_iam.columns)))
    time_missing = [col for col in time_missing if col not in [2005]]

    # If there are missing data, fill them with nearest time values (e.g. if 2050 is missing, use values from 2045)
    if len(time_missing):
        for t in time_missing:
            # nearest value for time t (e.g. if 2050 is missing, use values from 2045)
            tvalue = fun_find_nearest_values(num.index, t, n_max=1)[0]
            df_append=(df.xs(var, level="SECTOR", drop_level=False)
                .xs(tvalue, level="TIME", drop_level=False)
                .rename({tvalue: t}))
            df = pd.concat([df, df_append])
            # df = df.append(
            #     df.xs(var, level="SECTOR", drop_level=False)
            #     .xs(tvalue, level="TIME", drop_level=False)
            #     .rename({tvalue: t})
            # )
    return df


def fun_fill_zero_data_with_proxi_variable(
    df: pd.DataFrame, col: str, var: str
) -> pd.DataFrame:
    """Use proxi variable if `var` contains data equal to zero data for all countries. Proxi variable
    is found by using `fun_fuzzy_match`.

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe in step1b format
    col : str
        Column e.g. `ENSHORT_REF`
    var : str
        Variable e.g. `Final Energy`

    Returns
    -------
    pd.DataFrame
        Updated dataframe
    """
    num = (
        df.xs(var, level="SECTOR")[col]
        .groupby(["TIME", "TARGET"])
        .sum()
        .unstack("TARGET")
    )
    proxi_vars = fun_fuzzy_match(
        list(df.drop(var, level="SECTOR").reset_index().SECTOR.unique()), var
    )
    proxi_vars = proxi_vars + ["Final Energy"]
    for proxi_var in proxi_vars:
        if np.prod(num == 0)[0]:
            # No data at all in the dataframe (all equal to zero), we use a proxi variable
            proxi = df.xs(var, level="SECTOR", drop_level=False) + df.xs(
                proxi_var, level="SECTOR", drop_level=True
            )
            proxi = proxi.reset_index().set_index(df.index.names)
            df.loc[proxi.index, col] = proxi.loc[:, col]
            num = (
                df.xs(var, level="SECTOR")[col]
                .groupby(["TIME", "TARGET"])
                .sum()
                .unstack("TARGET")
            )
            if not np.prod(num == 0)[0]:
                break
    return df


def fun_sub_sectors_dict(df: pd.DataFrame, sect_adj: list, col: str = "SECTOR"):
    d = {}
    for x in sect_adj:
        d.update(fun_all_sub_sectors(x, df.reset_index()[col].unique()))
    return {k: v for k, v in d.items() if len(v)}


def fun_get_files_by_model(
    models: list,
    project: str,
    folder: str,
    search=None,
    countrylist=None,
    project_in_file_name=False,
) -> dict:
    """Returns a dictionary with `model` as key and the most
    recent csv file as value (from a given `folder` and a given `project`).

    Parameters
    ----------
    models : list
        List of models
    project : str
        Your selected project e.g. 'NGFS_2023'
    folder : str
        Folder with downscaled results

    Returns
    -------
    dict
        Dictionary with model as key and most recent file as value.
    """   
    files_dict = {}
    # Sort all files in folder by time of creation
    # https://stackoverflow.com/questions/168409/how-do-you-get-a-directory-listing-sorted-by-creation-date-in-python
    files_all = [
        str(x).rsplit("\\")[-1] for x in sorted(folder.iterdir(), key=os.path.getmtime)
    ]

    scenarios = fun_get_scenarios(project)
    for model in models:
        # All files containing a given model
        filesm = [x for x in files_all if model in x and 'csv' in x]
        if folder == CONSTANTS.CURR_RES_DIR("step1"):
            filesm = [
                f
                for f in filesm
                if pd.read_csv(f"{folder}/{f}", nrows=1).TARGET[0] in scenarios
            ]
            regions_all = fun_regional_country_mapping_as_dict(model, project)
            regions_to_be_downs = {k: v for k, v in regions_all.items() if len(v) > 1}
            regions = regions_to_be_downs
            regions = fun_get_iam_regions_associated_with_countrylist(
                project, countrylist, model
            )
            regions = fun_invert_dictionary({k: [v] for k, v in regions.items()})
            # Exclude Native countries (we do not have csv file for native countries)
            regions = {k: v for k, v in regions.items() if k in regions_to_be_downs}

            files = [x for region in regions for x in filesm if region in x]
        else:
            files=filesm
        fun = fun_check_if_all_characters_are_numbers
        if project_in_file_name:
            files=[x for x in files if project in x]
        dates = ["_".join([i for i in x.rsplit("_") if fun(i)]) for x in files]
        match={x:re.search(r'\d{4}_\d{2}_\d{2}', x) for x in dates}
        dates=list({k:v.group() for k,v in match.items() if v is not None }.values())
        date = [x for x in dates if len(x.split("_")) == 3][-1]
        if search is not None:
            files = [f for f in files if search in f]
        # files = [f for f in files if date in f and "csv" in f] # date, csv, project, "WITH_POLICY", "None"
        if len(files) == 1:
            files_dict[model] = files[0]
        else:
            for x in [date, "csv", project, "WITH_POLICY", "None"]:
                res = [f for f in files if x in f]
                if len(res):
                    files = res
                if len(files) == 1:
                    files_dict[model] = files[0]
                    break  # break loop as soon as we found unique file
        if folder == CONSTANTS.CURR_RES_DIR("step1"):
            harmo_files = [x for x in files if "harmo" in x]
            files = harmo_files if len(harmo_files) else files
            if not len(harmo_files):
                print(
                    f"NOTE: we are using unharmonized files for {model} (we could not find `harmo.csv` files)."
                )
            files_dict[model] = files
        elif len(files) != 1:
            txt = "Unable to automatically detect the most recent step5 file for"
            txt2 = f"This file is the most recent: {files[-1]} \n do you want to continue (y/n)? Or please type your selected file"
            # raise ValueError(f"{txt} {model}. We found multiple files: {files}. {txt2}")

            action = input(f"{txt} {model}. We found multiple files: {files}. {txt2}")
            if action.lower() in ["yes", "y"]:
                files_dict[model] = files[-1]
            elif action in files:
                files_dict[model] = action
            else:
                raise ValueError(
                    f"Simulation aborted by the user (user input={action})"
                )

    return files_dict


def fun_read_results(
    project: str,
    step: str,
    files: Optional[list] = None,
    countrylist: Optional[list] = None,
    models: Optional[list] = None,
    search: Optional[str] = None,
    rename_dict:Optional[dict]=None,
):
    if isinstance(files, dict):
        df=fun_index_names(pd.concat(pd.read_csv(x) for x in files.values()), True, int)
        if rename_dict:
            df=df.rename(rename_dict)
        scenarios=df.reset_index().SCENARIO.unique()
        # Remove `Downscaling[]` from  Model names
        models=df.reset_index().MODEL.unique()
        model_dict = {x: x.replace("Downscaling[", "").replace("]", "").replace('_downscaled','') for x in models}
        df = df.rename(model_dict)
        return df, list(files.values()), files, scenarios
    
    folder = CONSTANTS.CURR_RES_DIR(step)

    if not models:
        models = fun_get_models(project)

    if isinstance(files, str):
        search = files.split(".csv")[0]
    if files is not None and len(files) == 1:
        search = files[0].split(".csv")[0]

    df = pd.DataFrame() 
    files_dict = {}  
    if isinstance(files, str):
        for suf in ['.csv', '.xlsx']:
            if os.path.exists(folder/f"{search}{suf}"):
                df=fun_read_csv_or_excel(folder/f"{search}{suf}", None)
                files_dict={m:f"{search}{suf}" for m in models}
                files=[files]
            # if "SCENARIO" in df.index.names:
            #     scenarios = df.reset_index().SCENARIO.unique()
            # elif "TARGET" in df.columns:
            #     scenarios = df.TARGET.unique()
            # return df, [f"{search}.csv"], {m:f"{search}{suf}" for m in models}, scenarios
    
    # Get latest file avilable for each model: 
    project_in_file_name = CONSTANTS.CURR_RES_DIR('step5')==CONSTANTS.CURR_RES_DIR(step) 
    if not files:
        files_dict = fun_get_files_by_model(
            models, project, folder, search, countrylist, project_in_file_name=project_in_file_name
        )
        files = set(fun_flatten_list(list(files_dict.values())))
        # print(f"Will use this `files_dict`: {files_dict}")
    elif not(files_dict):
        if isinstance(files, str):
            # if a string we allow for reading `not harmo` files:
            files_dict = fun_get_files_by_model(
                models, project, folder, search, countrylist
            )
            files = set(fun_flatten_list(list(files_dict.values())))
            # print(f"Will use this `files_dict`: {files_dict}")
        elif search is not None:
            files_dict = fun_get_files_by_model(
                models, project, folder, search, countrylist
            )
        # elif isinstance(files, list):
        else:
            files_dict = {m: files for m in models}
    if not len(df):
        if "1_Final_Energy" in str(folder):
            for m, file in files_dict.items():
                df = pd.concat(
                    [df, fun_read_df_from_step1(file, countrylist, [m], project)]
                )
        else:
            for m, file in files_dict.items():
                df = pd.concat([df, fun_read_csv_or_excel(file, [m], folder=folder)])
                print(f"reading {file}")

    print("done with reading dataframes")

    if rename_dict:
        df=df.rename(rename_dict)

    if "SCENARIO" in df.index.names:
        scenarios = df.reset_index().SCENARIO.unique()
    elif "TARGET" in df.columns:
        scenarios = df.TARGET.unique()

    if "MODEL" in df.reset_index().columns:
        models = df.reset_index().MODEL.unique()

    # Remove `Downscaling[]` from  Model names
    model_dict = {x: x.replace("Downscaling[", "").replace("]", "").replace('_downscaled','') for x in models}
    if set(model_dict.values()) != set(model_dict.keys()):
        # if model_dict has the same keys/values pair there is no need to rename models
        df = df.rename(model_dict)
        models = df.reset_index().MODEL.unique()
    if len(df) == 0:
        raise ValueError(
            f"There are no downscaled results available in {step} for: {countrylist} from models: {models}"
        )

    return df, files, files_dict, scenarios


def fun_step2_format_from_iamc(df_iamc: pd.DataFrame) -> pd.DataFrame:
    """Re-shape dataframe ising a step2 format, by taking a dataframe in IAMC format as input

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe in IAMC format

    Returns
    -------
    pd.DataFrame
        Dataframe in step2 data format
    """
    expected_idx_names = ["MODEL", "SCENARIO", "ISO", "VARIABLE", "UNIT"]
    if df_iamc.index.names != expected_idx_names:
        raise ValueError(
            f"`df.index.names` should be {expected_idx_names}, you provided {df_iamc.index.names} "
        )
    df_iamc = df_iamc.droplevel(["MODEL", "SCENARIO", "UNIT"])
    df_iamc = df_iamc.stack().unstack(level=1).reset_index()
    if "TIME" not in df_iamc.columns:
        df_iamc = df_iamc.rename({"level_1": "TIME"}, axis=1)
    return df_iamc.set_index(["TIME", "ISO"]).sort_index()


def run_sector_harmo_enhanced_iamc(
    project: str,
    df: pd.DataFrame,
    dicts: List[dict],
    df_iam: Optional[pd.DataFrame] = None,
    no_iter: int = 2,
) -> pd.DataFrame:
    """Returns dataframe (`df`) with harmonized sub-sectors based on a list of dictionaries `dict`
    and a given `project` (from which we get the regional mapping).

    Parameters
    ----------
    project : str
        Your project e.g. `NGFS_2023`
    df : pd.DataFrame
        Your dataframe. Dataframe should be in IAMc format
    dicts : List[dict]
        List of dictionaries with main sectors as `keys` and a list of sub-sectors as `values`.
    df_iam : Optional[pd.DataFrame], optional
        Dataframe with IAM results in IAMc format. If None, variables will be not harmonized
        to match regional IAMs results, by default None
    no_iter: int
        Number of iterations for sub-sectors adjustments, by default 2

    Returns
    -------
    pd.DataFrame
        Updated dataframe
    """

    df = df.copy(deep=True)
    fun_check_iamc_index(df)

    # Drop duplicates and get units for each variable
    df = fun_drop_duplicates(df)
    units = fun_get_variable_unit_dictionary(df)

    rename_iso_as_region = "REGION" in df.index.names
    models = df.reset_index().MODEL.unique()
    scenarios = df.reset_index().SCENARIO.unique()

    res = pd.DataFrame()
    for m in models:
        resm = pd.DataFrame()
        df_selm = df.xs(m, level="MODEL", drop_level=False)
        regions_dict = fun_regional_country_mapping_as_dict(m, project)
        for s in scenarios:
            df_selr = df_selm.xs(s, level="SCENARIO", drop_level=False)
            for r, clist in regions_dict.items():
                df_sel = fun_xs(df_selr, {"REGION": clist})
                df_sel = fun_step2_format_from_iamc(
                    fun_rename_index_name(df_sel, {"REGION": "ISO"})
                )
                df_sel = fun_from_step2_to_step1b_format(df_sel, s, cols=[""])
                for _ in range(no_iter):
                    for d in dicts:
                        df_sel = run_sector_harmo_enhanced(df_sel, d, "", df_iam)
                # TODO
                df_sel = fun_from_step2_to_step1b_format(
                    df_sel, s, cols=[""], reverse=True
                )
                df_sel["TARGET"] = s
                df_sel = df_sel.set_index("TARGET", append=True)
                resm = pd.concat([resm, df_sel])
        resm["MODEL"] = m
    res = pd.concat([res, resm.set_index("MODEL", append=True)])

    # Convert back to IAMc format
    res = res.stack().unstack("TIME")

    # Reshape df
    res = res.reset_index().set_index(["MODEL", "ISO", "TARGET", "VARIABLE"])

    # Add units
    d = {k: [v] for k, v in units.items()}
    res = fun_add_units(res, None, fun_invert_dictionary(d))

    if rename_iso_as_region:
        res = fun_rename_index_name(res, {"ISO": "REGION"})
    return res


def fun_read_df_iams(project: str, models: Optional[List[str]] = None) -> pd.DataFrame:
    """Reads IAMs results for a list of `models` from a given `project`

    Parameters
    ----------
    project : str
        Your project e.g. "NGFS_2023
    models : Optional[List[str]]
        List of models

    Returns
    -------
    pd.DataFrame
        Dataframe with IAMs results in IAMc format
    """
    if models is None:
        models = fun_get_models(project)
    f = fun_read_df_iam_from_multiple_df
    return pd.concat(
        [f(m, CONSTANTS.INPUT_DATA_DIR / project / "multiple_df") for m in models]
    )


def fun_international_variables(
    project: str, var: str, 
    models: Optional[List[str]] = None,
    df_iam:pd.DataFrame=None,
    av_region:Optional[List[str]]=None,
    reg_name:str="World", iso_name:str="reg sum",
) -> pd.DataFrame:
    """Calculates international variables as the difference between World and the sum
     across regions for a giveb variable `var`.

    Parameters
    ----------
    project : str
        Your project e.g. 'NGFS_2023'
    models : List[models]
        List of models for which you want to calculate bunkers emissions
    av_region: Optional[List[str]]
        List of available regions. If None we we read it from df_iam, by default None
    reg_name: str
        Region name from which we substract the sum across regions (or countries)
        
    Returns
    -------
    pd.DataFrame
        _description_
    """
    if models is None:
        models = fun_get_models(project)

    group = ["MODEL", "SCENARIO", "VARIABLE", "UNIT"]

    if not df_iam:
        df_iam = fun_read_df_iams(project, models)
    idxcol = df_iam.index.names
    res_all = pd.DataFrame()
    for model in models:
        res = {}
        if not av_region:
            av_reg = df_iam.xs(model, level="MODEL").reset_index().REGION.unique()
        i_dict = {reg_name: reg_name, iso_name: [x for x in av_reg if model in x]}
        d = {"VARIABLE": var, "MODEL": model}
        for k, v in i_dict.items():
            d["REGION"] = v
            res[k] = fun_xs(df_iam, d)
        tot = res[reg_name].groupby(group).sum() - res[iso_name].groupby(group).sum()
        res_all = pd.concat([res_all, tot])

    res_all["REGION"] = reg_name
    return res_all.reset_index().set_index(idxcol)


def fun_all_energy_intensities2(
    df: pd.DataFrame,
    reverse: bool = False,
    mycols: list = ["ENSHORT_REF", "ENLONG_RATIO"],
):
    dfin = df.copy(deep=True)
    dfin = dfin.reset_index(["TARGET", "SECTOR"])
    ei_all = pd.DataFrame()

    oper = "mul" if reverse else "div"
    suf = "EN" if reverse else "EI"

    for x in mycols:
        ei = {}
        sectors = list(dfin.SECTOR.unique())
        for s in sectors:
            if s != "Final Energy":
                ei[s] = getattr(dfin[dfin.SECTOR == s][x], oper)(
                    dfin[dfin.SECTOR == dict_y_den[s]][x]
                )
            else:
                ei[s] = getattr(dfin[dfin.SECTOR == s][x], oper)(
                    dfin[dfin.SECTOR == s]["GDP|PPP"]
                )

        string = x.replace("EI_", "") if reverse else f"{suf}_{x}"

        if reverse:
            ei = {
                k: v * dfin[dfin.SECTOR == k]["GDP|PPP"]
                if dict_y_den[k] == "Final Energy"
                else v
                for k, v in ei.items()
            }

            ei = {
                k: v
                if dict_y_den[k] in ["Final Energy", "GDP|PPP"]
                else v * ei[dict_y_den[dict_y_den[k]]]
                for k, v in ei.items()
            }

        ei = pd.DataFrame(pd.concat(list(ei.values()), axis=0)).rename(
            {0: string}, axis=1
        )
        if not reverse:
            ei = ei.clip(0, 1)
        ei_all = pd.concat([ei_all, ei], axis=1)
    checkcols = set(ei_all.columns) & (set(dfin.columns))
    if len(checkcols):
        check = (ei_all / dfin).dropna(how="all", axis=1)
        if list(check.min().unique()) == [1] and list(check.max().unique()) == [1]:
            return dfin
        else:
            raise ValueError(
                f"{checkcols} columns were already present in the `dfin`, and they do not match expected calculations"
            )
    return pd.concat([dfin, ei_all], axis=1)


def fun_traceback_all_step1_sectors(mystring):
    res = {}
    while dict_y_den.get(mystring, 0) != 0:
        res[mystring] = dict_y_den.get(mystring, 0)
        mystring = res[mystring]
    return list(res.values())


def fun_all_energy_intensities3(
    df: pd.DataFrame,
    reverse: bool = False,
    mycols: list = ["ENSHORT_REF", "ENLONG_RATIO"],
):
    dfin = df.copy(deep=True)
    dfin = dfin.reset_index(["TARGET", "SECTOR"])
    ei_all = pd.DataFrame()

    oper = "div" if not reverse else "mul"
    suf = "EI" if not reverse else "EN"

    for x in mycols:
        ei = {}
        # if not reverse:
        for s in dfin.SECTOR.unique():
            if s != "Final Energy":
                ei[s] = getattr(dfin[dfin.SECTOR == s][x], oper)(
                    dfin[dfin.SECTOR == dict_y_den[s]][x]
                )
            else:
                ei[s] = getattr(dfin[dfin.SECTOR == s][x], oper)(
                    dfin[dfin.SECTOR == s]["GDP|PPP"]
                )

        string = x.replace("EI_", "") if reverse else f"{suf}_{x}"

        if reverse:
            ei = dfin.set_index("SECTOR", append=True)[x].unstack("SECTOR")
            ei = pd.concat(
                [ei, dfin[dfin.SECTOR == "Final Energy"][["GDP|PPP"]]], axis=1
            )

            f = fun_traceback_all_step1_sectors
            coldict = {s: f(s) for s in dfin.SECTOR.unique()}
            myserie = pd.concat(
                [
                    pd.DataFrame(ei[coldict[s] + [s]].product(axis=1)).rename(
                        {0: s}, axis=1
                    )
                    for s in dfin.SECTOR.unique()
                ],
                axis=1,
            )
            ei = pd.DataFrame(myserie.stack()).droplevel(2).rename({0: string}, axis=1)
        else:
            myserie = pd.concat(list(ei.values()), axis=0)
            ei = pd.DataFrame(myserie).rename({0: string}, axis=1)
        ei_all = pd.concat([ei_all, ei], axis=1)
    return pd.concat([dfin.loc[dfin.index], ei_all.loc[dfin.index]], axis=1)


def fun_smooth_enlong(
    df: pd.DataFrame,
    tc_list: list = [2050, 2100],
    _from: str = "ENSHORT_REF",
    _to: str = "ENLONG_RATIO",
    _then: str = "ENLONG_RATIO",
    over="GDPCAP",
    use_linear_method=False,
):
    dfin = df.copy(deep=True)

    sectors = dfin.reset_index().SECTOR.unique()

    # Exclude Hydrogen and Heat (for which we don't have energy intensities)
    # for them will  use ENLONG_RATIO projections
    excl = fun_filter_list_of_strings(sectors, ["Hydrogen", "Heat"])
    excl_df = fun_xs(dfin, {"SECTOR": excl})  # [[col_name]]
    dfin = fun_xs(dfin, {"SECTOR": excl}, exclude_vars=True)

    # Calculates all energy intensities (for all countries/sectors)
    f = fun_all_energy_intensities2
    dfin = f(dfin)

    colnames = []
    # Calculates smooth enlong for a list of time of convergence (tc_list)
    for t in tc_list:
        __from = {"col": _from, "t": 2010}
        __to = {"col": _to, "t": t}
        # _from = {"col": list(d['from'].keys())[0], "t": list(d['from'].values())[0]}
        # _to = {"col": list(d['to'].keys())[0], "t": list(d['to'].values())[0]}
        # _cont = d['then']

        # NOTE: this is on a linear (TIME) basis not on a log-log basis
        dfin = fun_smooth_enlong_single_tc(
            dfin.set_index(["TARGET", "SECTOR"], append=True),
            __from,
            __to,
            _continue=_then,
            over=over,
            use_linear_method=use_linear_method,
        )

        col_name = f"{_from}_to_{_to}_{t}_then{_then}"
        # NOTE commented code below CAN BE USED IF you want to use linear projections until 2050, without `_continue=ENLONG_RATIO`
        # # We do another interpolation e.g. from `__to['t']` to 2100 if __to['t']<2100
        # # This  avoids strong energy reductions when using a linear model with 2050 as time of convergence.
        # if __to['t']<2100:
        #     dfin = fun_smooth_enlong_single_tc(
        #             dfin,
        #             {'col':col_name, 't':__to['t']},
        #             {'col':__to['col'], 't':2100},
        #             _continue=_then,
        #             over=over,
        #             use_linear_method=use_linear_method,
        #         )
        col_name = f"EI_{col_name}"
        colnames = colnames + [col_name]
        # fun_all_energy_intensities(dfin.reset_index().set_index(idx), True, [col_name])
        # Below we convert energy intensities to EJ values
        idx = ["TIME", "ISO", "TARGET", "SECTOR"]
        try:
            dfin = f(dfin.reset_index().set_index(idx), True, [col_name])
            idx = excl_df.index
            if col_name not in excl_df.columns:
                excl_df[col_name] = "to be filled"
            # excl_df.loc[:,col_name]=df.loc[idx,["ENLONG_RATIO"]]
            excl_df.loc[:, col_name] = fun_xs(df, {"SECTOR": excl}).loc[
                :, "ENLONG_RATIO"
            ]

        except:
            a = 1

    # colnames= colnames + [x.replace('EI_','') for x in colnames]
    # return pd.concat([dfin, excl_df])
    return pd.concat(
        [dfin.set_index(["TARGET", "SECTOR"], append=True), excl_df]
    )  # [colnames]


def fun_smooth_enlong_single_tc(
    dfin, _from, _to, _continue="ENLONG_RATIO", over="GDPCAP", use_linear_method=False
):
    # NOTE: we interpolate `_from` (e.g. 2010) -> `_to` (e.g. 2050).
    # However, if  over=='GDPCAP' and the  maximum GDPCAP value occurs before 2050 (e.g. 2030), ->  then `_to` will be equal to 2030 in that country (otherwise 2050).
    # -> if you want to go back and use the same `_to` for each country you can use `to_dict= {c:_to['t'] for c in dfin.reset_index().ISO.unique()}`

    dfin = dfin.copy()
    func = dfin.FUNC.unique()
    if len(func) != 1:
        raise ValueError(
            f"We found None or more than one functions in the dataframe: {func}"
        )
    # dfin = dfin.set_index("SECTOR", append=True)  # TRY TO MOVE IT HERE
    dfin["GDPCAP"] = dfin["GDP|PPP"] / dfin["Population"]
    dfin['GDPCAP_original'] =      dfin["GDPCAP"].copy(deep=True)

    # Clip GDPCAP values to 2010 data to avoid artifacts with smooth_enlong calcualtions (e.g. GNQ, for SHAPE_2023 runs)
    clip_dict = dfin.loc[2010]['GDPCAP'].droplevel(['TARGET','SECTOR']).to_dict()
    gdp_clipped=fun_clip_df_by_dict(dfin['GDPCAP'],clip_dict,'ISO', lower=True )
    dfin.loc[:,'GDPCAP']=gdp_clipped
    
    if over == "TIME":
        dfin["TIME2"] = dfin.index.get_level_values("TIME")
        over = "TIME2"
    npcols = []
    if not use_linear_method and func[0] == "log-log":
        npcols += [f"EI_{_to['col']}", f"EI_{_from['col']}", over]
        # npcols = [x for x in npcols if "TIME" not in x]
        dfin.loc[:, npcols] = np.log(dfin.loc[:, npcols])

    try:
        # coeff_ang = (
        #     dfin.loc[_to["t"], f"EI_{_to['col']}"]
        #     - dfin.loc[_from["t"], f"EI_{_from['col']}"]
        # ) / (_to["t"] - _from["t"])

        # Until when we interpolate. Normally is `_to['t']`, unless maxiumum GDPCAP happens earlier.
        if over == "GDPCAP":
            to_dict = {
                c: dfin.xs((c, "Final Energy"), level=("ISO", "SECTOR"))["GDPCAP"]
                .loc[range(2010, _to["t"] + 5,5)]
                .idxmax()[0]
                for c in dfin.reset_index().ISO.unique()
            }
        else:
            to_dict = {c: _to["t"] for c in dfin.reset_index().ISO.unique()}

        # if over=='TIME2' and use_linear_method:
        #     a='checkthis'

        # if "TIME" in over: # THIS ONE NEVER HAPPENS AS WE CALL IT TIME2
        #     den = _to["t"] - _from["t"]
        #     if not use_linear_method:
        #         check_this_one='here'
        # else:
        # den = dfin.loc[_to["t"], over] - dfin.loc[_from["t"], over]
        den = pd.concat(
            [
                (dfin.loc[to_dict[c], over] - dfin.loc[_from["t"], over]).xs(
                    c, drop_level=False
                )
                for c in dfin.index.get_level_values("ISO").unique()
            ]
        )

        # to_s = dfin.loc[_to["t"], f"EI_{_to['col']}"]
        # `to_s` = EI_ENLONG_RATIO in 2050 (or where max gdp occurs)
        to_s = pd.concat(
            [
                dfin.loc[to_dict[c], f"EI_{_to['col']}"].xs(c, drop_level=False)
                for c in dfin.index.get_level_values("ISO").unique()
            ]
        )
        # `from_s` = EI_ENSHORT_REF in 2010
        from_s = dfin.loc[_from["t"], f"EI_{_from['col']}"].copy(deep=True)
        # from_s = pd.concat(
        #     [
        #         dfin.loc[to_dict[c], f"EI_{_from['col']}"].xs(c, drop_level=False)
        #         for c in dfin.index.get_level_values("ISO").unique()
        #     ]
        # )
        coeff_ang = (to_s - from_s) / den
        d = coeff_ang.to_dict()
        dfin = dfin.reset_index("TIME")  # TRY TO MOVE IT HERE
        # dfin=dfin.set_index('SECTOR', append=True).reset_index('TIME')
        dfin["coeff_ang"] = [d[x] for x in dfin.index]
        col_name = f"EI_{_from['col']}_to_{_to['col']}_{_to['t']}_then{_continue}"
        temp_diff = dfin.loc[:, over] - dfin.loc[dfin["TIME"] == _from["t"], over]
        dfin[f"{over}_DIFF"] = "to be filled"
        dfin.loc[dfin.index, f"{over}_DIFF"] = temp_diff.loc[dfin.index]
        # dfin[f"{over}_DIFF"] = dfin[over] - dfin.loc[dfin[over]==_from['t'], over]
        dfin[col_name] = dfin["coeff_ang"] * dfin[f"{over}_DIFF"]
        # dfin=dfin.reset_index().set_index(['TIME','ISO'])
        d1 = dfin[dfin["TIME"] == _from["t"]][f"EI_{_from['col']}"]
        dfin = dfin.reset_index().set_index(["ISO", "TARGET", "SECTOR"])
        dfin["add"] = [d1[x] for x in dfin.index]
        dfin[col_name] = dfin[col_name] + dfin["add"]
        drop_cols = ["coeff_ang", f"{over}_DIFF", "add"]
        if "TIME2" in dfin.columns:
            drop_cols = drop_cols + ["TIME2"]
            npcols = [x for x in npcols if x not in drop_cols]
        for col in drop_cols:
            dfin = dfin.drop(col, axis=1)
        dfin = dfin.reset_index().set_index(["TIME", "ISO", "TARGET", "SECTOR"])

        # Replace with ENLONG values if t>tc and MAX GDPCAP is in tc
        # CAREFUL, WITHOUT THE BELOW WE CAN HAVE ZERO ENERGY CONSUMPTION (if tc<2100)
        # if over=="GDPCAP":
        d1 = dfin[col_name].to_dict()
        d2 = dfin[f"EI_{_continue}"].to_dict()
        dfin.loc[:, col_name] = [
            # d1[x] if x[0] <= _to["t"] or to_dict[x[1]]!=_to["t"] else d2[x] for x in dfin.index
            d1[x] if x[0] <= to_dict[x[1]] else d2[x]
            for x in dfin.index
        ]
        if list(func)[0] == "log-log" and not use_linear_method:
            npcols = npcols + [col_name]
            dfin.loc[:, npcols] = np.exp(dfin.loc[:, npcols].astype(float))
        # Go back to real GDPCAP data (not clipped)
        dfin["GDPCAP"]=dfin['GDPCAP_original'].copy(deep=True)
        return dfin.drop('GDPCAP_original', axis=1)
    except:
        a = 1
        return


def get_git_revision_hash() -> str:
    return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode("ascii").strip()


def get_git_revision_short_hash() -> str:
    return (
        subprocess.check_output(["git", "rev-parse", "--short", "HEAD"])
        .decode("ascii")
        .strip()
    )


def is_git_repository_clean(repo_path="."):
    try:
        # Check if there are any modified files
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            cwd=repo_path,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        if result.returncode == 0:
            # If there is no modified output, the repository is clean
            return not result.stdout.strip()
        else:
            # An error occurred, the repository might not be a Git repository
            return False
    except FileNotFoundError:
        # Git executable not found
        return False


def is_dirty():
    """This function checks if your repo is clean (no files changed/to be committed)"""
    repository_path = CONSTANTS.INPUT_DATA_DIR / "../downscaler"
    if is_git_repository_clean(repository_path):
        print("The repository is clean and up-to-date.")
    else:
        raise ValueError("The repository is not clean or has uncommitted changes.")


def fun_iamc_format_from_step2(df_all: pd.DataFrame) -> pd.DataFrame:
    """Returns data in IAMC format, starting from step2 data format.
    To revert data back to step2 format please use function `fun_step2_format_from_iamc`
    Parameters
    ----------
    df_all : pd.DataFrame
        Dataframe in step2 data format

    Returns
    -------
    pd.DataFrame
        Dataframe in IAMC format
    """
    if df_all.index.names != ["TIME", "ISO"]:
        raise ValueError(
            f"`df.index.names` should be ['TIME', 'ISO'], you provided {df_all.index.names} "
        )
    df_all_iamc = df_all.stack().unstack("TIME")
    df_all_iamc.index.names = ["ISO", "VARIABLE"]
    return df_all_iamc


def fun_blending_with_sensitivity(
    df_res: pd.DataFrame, over: str = "TIME"
) -> pd.DataFrame:
    df_res = df_res.copy(deep=True)
    set_i = ["TARGET", "SECTOR"]
    cols_std = ["ENSHORT_REF", "ENLONG_RATIO", "LOG_REG_GDPCAP", "REG_GDPCAP"]
    cols = cols_std + [x for x in df_res.columns if "then" in x and "EI_" not in x]
    res_blend = {}
    for method in df_res.reset_index().METHOD.unique():
        for target in df_res.reset_index().TARGET.unique():
            df_temp = df_res.xs(
                (method, target), level=("METHOD", "TARGET"), drop_level=False
                ).dropna(how="all")
            df_temp = df_temp.loc[:, df_temp.columns.isin(cols)]
            selcols = (
                ["ENLONG_RATIO"]
                if method == "wo_smooth_enlong"
                else [x for x in df_temp.columns if x not in cols_std]
            )
            for col in selcols:
                # below sensitivity=False for all
                res_blend[f"{method}_{col}SPLITHERE{target}"] = fun_blending(
                    df_temp, False, _to=col, _over=over
                )
                # res_blend[{'method':f"{method}_{col}", 'target':target}] = fun_blending(df_temp, False, _to=col)

    f = fun_from_step2_to_step1b_format
    target = None  # "CD_Links_SSP1_v2NPi2020_1000-con-prim-dir-ncr"
    for k, v in res_blend.items():
        cols = [x for x in v.columns if "BLEND" in x and "EI" not in x]
        res_blend[k] = f(v.set_index(set_i, append=True), target, cols, reverse=True)

    # Convert to IAMc format
    f = fun_iamc_format_from_step2
    df_all = pd.concat(
        f(v)
        .assign(METHOD="".join(k.split("SPLITHERE")[:-1]))
        .assign(TARGET=k.split("SPLITHERE")[-1])
        for k, v in res_blend.items()
    )
    # df_all = pd.concat(f(v).assign(METHOD=k['method']).assign(TARGET=k['target']) for k, v in res_blend.items())
    # df_all = pd.concat(f(v).assign(METHOD=k) for k, v in res_blend.items())

    df_all = df_all.reset_index()
    df_all["CONVERGENCE"] = df_all.VARIABLE
    df_all = df_all.set_index(["ISO", "VARIABLE", "TARGET", "CONVERGENCE", "METHOD"])
    ren_var = {x: x[:-10] for x in df_all.reset_index().VARIABLE.unique()}
    ren_conv = {x: x[-10:-6] for x in df_all.reset_index().VARIABLE.unique()}
    df_all = df_all.rename(ren_var, level="VARIABLE")
    df_all = df_all.rename(ren_conv, level="CONVERGENCE")
    if over != "TIME":
        conv_dict = {
            x: f"{x}_{over}" for x in df_all.reset_index().CONVERGENCE.unique()
        }
        df_all = df_all.rename(conv_dict, level="CONVERGENCE")
    return df_all


def fun_add_regional_gdpcap_in_step1b(df2: pd.DataFrame) -> pd.DataFrame:
    reg = df2.groupby(["TIME", "SECTOR", "TARGET", "METHOD"]).sum()
    reg["REG_GDPCAP"] = reg["GDP|PPP"] / reg["Population"]
    clist = df2.reset_index().ISO.unique()
    reg2 = pd.concat([reg.assign(ISO=c).set_index("ISO", append=True) for c in clist])
    reg = reg2.reset_index().set_index(df2.index.names)["REG_GDPCAP"]
    df2 = pd.concat([df2, reg], axis=1)
    df2["LOG_REG_GDPCAP"] = np.log(df2[["REG_GDPCAP"]])
    return df2


def fun_phase_out_in_all_countries(df:pd.DataFrame, scen:str, num:str, den:str, c_list:Optional[list], interpolate_yearly:bool, phase_out:bool, threshold:float=0)->pd.DataFrame:

    # compared to `fun_phase_out_in_dates` this function:
    # - allows to calculate shares 
    # - optionally interpolates df to get yearly data
    # - it calculates dates for a list of countries
    res_all=pd.DataFrame()
    c_list= c_list or list(df.reset_index().REGION.unique())
    c_list=list(set(c_list)&set(iea_countries)) # c_list within IEA countries
    var=num
    dfsel=df.copy(deep=True)
    if den:
        var=f"Share of {var}"
        dfsel=(df.xs(num, level='VARIABLE')/df.xs(den, level='VARIABLE')).assign(VARIABLE=var)
        dfsel=dfsel.set_index('VARIABLE', append=True)
        dfsel=fun_xs(dfsel, {'REGION':c_list})
    
    dfsel=fun_xs(dfsel, {'VARIABLE':var,'SCENARIO':scen}).dropna(how='all', axis=0)
    if interpolate_yearly:
        dfsel=fun_interpolate(dfsel.replace(np.nan, 0),False, range(2010, 2101),True)
    c_list= dfsel.reset_index().REGION.unique()
    for c in c_list:
        dfselc=dfsel.dropna(how='all', axis=0).xs(c, level='REGION', drop_level=False)
        res_all=pd.concat([res_all, fun_phase_out_in_dates(dfselc,var,c, scen, phase_out, threshold=threshold)])
    # TO BE added: capture k,v loop (maybe outside of this function)
    res_all=res_all.assign(median=np.median(res_all,axis=1))
    # res_all=res_all.assign(mean=np.mean(res_all,axis=1))
    return res_all.sort_values('median')


def fun_carbon_budget_step5e(df, scen, clist=None, var='Emissions|CO2', _from=2020,_to=2030):
    if not clist:
        clist = list(df.reset_index().REGION.unique())
    A=df.xs([var, scen], level=['VARIABLE','SCENARIO'], drop_level=False)
    return (fun_xs(fun_interpolate(A,False, range(2010, 2101),True)[range(_from, _to)].sum(axis=1), {'REGION':clist}).droplevel('FILE').groupby(['SCENARIO','REGION','UNIT']).median().sort_values().droplevel('UNIT')).T.droplevel('SCENARIO')


def fun_max_energy_per_capita(dfsel:pd.DataFrame)->pd.Series:
    """Returns maximum GDP per capita occurring in each country (across model/scenario/convergence/method/time) 
    unit=[TJ/person/year].

    Parameters
    ----------
    dfsel : pd.DataFrame
        Dataframe in IAMC format

    Returns
    -------
    pd.Series
        Maximum enegry per capita ever reached across different countries.
    """    
    for x in ['CONVERGENCE','METHOD']:
        if x in dfsel.index.names:
            dfsel=dfsel.droplevel(x)
    en_percapita=dfsel.xs('Final Energy', level='VARIABLE')/dfsel.xs('Population', level='VARIABLE')
    return en_percapita.max(axis=1).groupby('ISO').max().sort_values()

def fun_shorten_region_name(r:str):
    """Shorten a region name if longer than 3 characters"""
    region=r.split('|')[-1].split()
    # If only 3 letters we reru
    if len(region)==1:
          return region[0][:3].upper()
    region= fun_first_letter_in_each_word(r.split('|')[-1])[:4]
    if len(region)<3:
        return region+r.split('|')[-1].split()[-1][1].upper()
    return region

def fun_create_missing_sectors_as_difference(df:pd.DataFrame,all_sector_vars:dict)->pd.DataFrame:
    """Creates missing sector as the difference between a main sector (keys in `all_sector_vars`) 
    and the list of other sub-sectors (values in `all_sector_vars`),  if only one sector is missing. 

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe in IAMc format
    all_sector_vars : dict
        Dictionary with key as the main sectors, and values as a list of sub-sectors.

    Returns
    -------
    pd.DataFrame
        Dataframe with missing sector as the difference between a main sector and the sum of sub-sectors
    """    
    df=df.copy(deep=True)
    all_variables=df.reset_index().VARIABLE.unique()

    for k,v in all_sector_vars.items():
        if k in all_variables:
            unit= df.xs(k, level='VARIABLE').reset_index().UNIT.unique()[0]
            missing_sectors=[x for x in v if x not in all_variables]
            # We calculate missing sector as the difference, only if there is 1 sector missing
            missing_sector= missing_sectors[0] if len(missing_sectors)==1 else None
            if missing_sector:
                weights={k:1}
                weights.update({i:-1 for i in v if i!=missing_sector})
                df=fun_create_var_as_sum(df, missing_sector,  weights, unit=unit)
            else:
                print(f'More than one sector is missing (or all sectors are availble) for {k} - unable to create new sectors')
        else:
            print(f"The `main` sector/variable {k} is not available in the dataframe")
    return df

def fun_find_most_recent_files_in_folder_including(folder, model, scen, exclude_files_with_strings=['Price_info.csv', 'Step5d', 'imports', 'non_co2','Emissions_by_sector', 'xls'],  available_scen={}
    ):
    mylist=sorted(Path(folder).iterdir(), key=os.path.getmtime) # get all files in foldr
    mylist=reversed([x for x in mylist if model in str(x)]) # sort by most recent file
    mylist=[y for x in exclude_files_with_strings for y in mylist if str(x) not in str(y)] # exclude unwanted file names 
    
    # Try to search for a `preferred` list of files=> these should include the string 'WITH_POLICY_None.csv'
    preferred=[x for x in mylist if str(x).find('WITH_POLICY_None.csv')>-1]
    
    # If we have found some preferred files, we use them. Otherwise we exclude the files with `badstring`
    if len(preferred)>0:
        mylist=preferred
    else:
        for badstring in exclude_files_with_strings:
            mylist=[x for x in mylist if str(x).find(badstring)<0]

    # Keep on searching until we find our scenario in the dataframe, starting from the most recent files
    for file in [x for x in mylist if '.csv' in str(x)]:
       if file not in available_scen:
            # We want csv files and we exclude `Explorer_` files as they have different format (D.ISO instead of ISO)
            if str(file)[-4:]=='.csv' and 'Explorer_' not in str(file).replace('5_Explorer_and_New_Variables',''):
                available_scen[file]=pd.read_csv(file)['SCENARIO'].unique().tolist()
                if scen in fun_flatten_list(available_scen.values()):
                    break
    mylist= [file for file in mylist if scen in available_scen.get(file, '')]
    return [mylist[0], available_scen] if len(mylist) else [None, available_scen]

def fun_get_most_recent_files_by_model_and_scenario(project, model, folder, verbose=True):
    res={}
    available_targets={}
    for scen in fun_get_scenarios(project):
        res[scen], available_targets[scen]=fun_find_most_recent_files_in_folder_including(folder,model, scen=scen, available_scen=available_targets)
    res=fun_invert_dictionary({k:[str(v)] for k,v in res.items() if v is not None})
    if verbose:
        print('')
        pprint.pprint({str(k).split("\\")[-1]:v for k,v in res.items()})
    return res

def fun_current_file_suffix():
    return '{date:%Y_%m_%d}'.format( date=datetime.datetime.now() )

def fun_combine_df_with_most_recent_scenarios(project:str, step:str, save_to_csv:bool=False, sub_folder:Optional[str]=None):
    """    
    Combine all dataframes for a given `project` with the latest (most recent) scenarios in a given `step` folder, 
    which maybe have been saved under different csv file names with different file_suffix.
    If step =='step3' it also combines the GDP data files and save it to csv folder.  

    Parameters
    ----------
    project : str
        project folder
    step : str
        downscaling step (e.g. 'step1')
    save_to_csv : bool, optional
        whether to save dataframe to csv, by default False
    sub_folder: Optional[str]:
        Wheter you want to search data in a sub_folder (e.g. '2023'), using today's date as `file_suffix`, by default False
    Returns
    -------
    _type_
        _description_
    """    

    res={} # step3 files
    res_gdp=pd.DataFrame() # gdp files
    folder=CONSTANTS.CURR_RES_DIR(step)
    if sub_folder:
        folder=folder/str(sub_folder)
    files_to_be_merged={}
    for model in fun_get_models(project):
        # Step1 - Find all most recent files for a given model/scenario in a given folder (saved under different file_suffix)
        # e.g. files_to_be_merged= {'MESSAGEix-GLOBIOM 1.1-M-R12_2023_08_03.csv': ['o_lowdem'], 'MESSAGEix-GLOBIOM 1.1-M-R12_2023_10_29.csv': ['d_delfrag']}
        files_to_be_merged.update(fun_get_most_recent_files_by_model_and_scenario(project, model,folder, verbose=True))
        
    # Step 2a - Combine the files above in one df and save it into the `res` dictionary (one df for each model) 
    if len(files_to_be_merged)>1:
        df=pd.DataFrame()
        for k,v in files_to_be_merged.items():
            df=pd.concat([df, fun_xs(fun_read_csv({k:k}, True, int)[k], {'SCENARIO':v})])
        res[model]=fun_drop_duplicates(df)

        # Step 2b - Combine all gdp files (f'GDP_{file_suffix}_updated_gdp_harmo.csv')
        if CONSTANTS.CURR_RES_DIR(step)==CONSTANTS.CURR_RES_DIR('step3'):
            # gdp_files=list(fun_get_most_recent_files_by_model_and_scenario(project, model,folder).keys())
            gdp_files_updated={f"{x.replace(model, 'GDP').replace('.csv','_updated_gdp_harmo.csv')}":v for x,v in files_to_be_merged.items()}
            for x,v in gdp_files_updated.items():
                gdp_temp=fun_index_names(pd.read_csv(x), True, int)
                gdp_temp=fun_xs(gdp_temp, {'SCENARIO':v})
                if model in gdp_temp.reset_index().MODEL.unique():
                    gdp_temp=gdp_temp.xs(model, drop_level=False)
                    res_gdp=pd.concat([res_gdp, gdp_temp])

        # Step 3 - Optionally save df to csv in the same folder, using current data as `file_suffix`
        if save_to_csv:
            new_filename=f"{model}_{fun_current_file_suffix()}.csv"
            res[model].to_csv(folder/new_filename)
            text0={str(k).split("\\")[-1]:v for k,v in files_to_be_merged.items()}
            text=f"\n {new_filename} was created by merging: {text0}"
            with open(folder/'README - merged files.txt', 'a') as f:
                    f.write(text)

        print(f'** This is what we merged in the `combi` dataframe: **')
        for i, (k,v) in enumerate(files_to_be_merged.items()):
            printval=k.rsplit('\\')[-1]
            print(f"{1+i}) {printval}: {v}")
        print('')

    # Step3: save all GDP files
    if CONSTANTS.CURR_RES_DIR(step)==CONSTANTS.CURR_RES_DIR('step3'):
        res_gdp=fun_drop_duplicates(np.round(fun_index_names(res_gdp),5))
        if save_to_csv:
            res_gdp.to_csv(folder/f"GDP_{fun_current_file_suffix()}_updated_gdp_harmo.csv")
    if len(res) or len(res_gdp):
        return pd.concat([pd.concat(list(res.values())), res_gdp])
    return pd.DataFrame()

def fun_create_missing_sectors_as_sum(df_blended:pd.DataFrame, mydict:dict, unit:str)->pd.DataFrame:
    """Create missing sectors (keys in mydict) as the sum of sub-sectors (values in mydict)
    Parameters
    ----------
    df_blended : pd.DataFrame
        Your dataframe
    mydict : dict
        Dictionary defining the missing sector (key) as the sum of sub-sectors (values)
    unit: str
        Unit of the new sector variable
    Returns
    -------
    pd.DataFrame
        Updated dataframe
    """    
    df_blended=df_blended.copy(deep=True)
    variables=df_blended.reset_index().VARIABLE.unique()
    for main,subs in mydict.items():
        if main not in variables:
            if all(s in variables for s in subs):
                df_blended=fun_create_var_as_sum(df_blended, main,subs, unit=unit)
    return df_blended

def fun_iamc_format_and_select_variables(
    model: str,
    target: str,
    region: str,
    df_all: pd.DataFrame,
    ra: int,
    str_to_find: str = "BLEND",
    endswith=False,
) -> Union[pd.DataFrame, str]:
    """This function selects variables, and add an unit columns to the dataframe.
       It returns the updated dataframe with the selected variables and the the csv file name (which depends based on criteria weights)

    Args:
        random_electricity_weights (bool): False if we use default electricity criteria weights, otherwise True
        model (str): model
        target (str): scenario
        region (str): region
        df_all (pd.DataFrame): pd.DataFrame
        ra (int): random criteria weight seed
        file_name_iamc (str): Name of csv file

    Returns:
        Union[pd.DataFrame, str]: Dataframe with selected downscaled results and csv file name
    """

    # Selecting variables to be included
    if endswith:
        df_long = df_all.iloc[:, df_all.columns.str.endswith(str_to_find)]
    else:
        df_long = df_all.iloc[:, df_all.columns.str.contains(str_to_find)]
    df_long = fun_pd_long_format(df_long)
    # Csv file name
    df_all = fun_pd_wide_format(df_long, region, model, target, "EJ/yr")
    df_all["UNIT"] = "EJ/yr"
    iamc_index = ["MODEL", "SCENARIO", "ISO", "VARIABLE", "UNIT"]
    setindex(df_all, iamc_index)
    return df_all

def fun_create_residential_and_commercial_as_sum_in_step2(var_list:list, df_all:pd.DataFrame, model:str, target:str, region:str)->pd.DataFrame:
    """
    Create 'Final Energy|Residential and Commercial' sector by summing up 'Final Energy|Residential' and 'Final Energy|Commercial' 
    for ENSHORT/ENLONG projections as defined in var_list.

    Parameters:
    - var_list (list): List of short/long projections (e.g. ["ENSHORT_REF", "ENLONG_RATIO"]).
    - df_all (pd.DataFrame): DataFrame containing the data in step2 format.
    - model (str): Model name.
    - target (str): Target scenario.
    - region (str): Region name.

    Returns:
    - pd.DataFrame: DataFrame with residential and commercial in step2 format.
    """
    df_all=df_all.copy(deep=True)
    res={}
    for var in var_list:
        df_blended=fun_iamc_format_from_step2(df_all)
        df_blended=fun_iamc_format_and_select_variables(model, target, region,df_all,'default',var,  endswith=True)
        vardict={x:x.replace(var, '') for x in df_blended.reset_index().VARIABLE.unique()}
        # vardict={k:v for k,v in vardict.items() if k.endswith(var)}
        # df_blended=fun_xs(df_blended, {'VARIABLE':list(vardict.keys())})
        df_blended=df_blended.rename(vardict)
        for sect in ['','|Electricity', '|Gases','|Heat','|Hydrogen','|Liquids','|Solids']:
            mydict={f'Final Energy|Residential and Commercial{sect}': 
                                                            [f'Final Energy|Commercial{sect}', f'Final Energy|Residential{sect}']}
            df_blended = fun_create_missing_sectors_as_sum(df_blended, mydict, unit='EJ/yr')
        vardict={x:f"{x}{var}" for x in df_blended.reset_index().VARIABLE.unique()}
        res[var]=df_blended.rename(vardict)
    return pd.concat([fun_step2_format_from_iamc(v) for v in res.values()] , axis=1)

def fun_check_negative_energy_variables(
    df: pd.DataFrame,
    decimals: int = 4,
    exlude_var: tuple = ("Trade", "Statistical"),
    unit: str = "EJ/yr",
    coerce_errors: bool = False,
    known_issues:Optional[dict] = None,
):
    """Checks if a `df` contains negative energy variables. If this is the case, raise Value Error.
    We exclude variables that contain strings as specified in `exlude_var`.

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe
    decimals : int, optional
        Round df using a number of decimals, by default 4
    exlude_var : tuple, optional
        Strings that should not be included in the variable name, by default ("Trade", "Statistical")
    unit: str, optional
        Unit of energy variable, by default 'EJ/yr'
    coerce_errors: bool
        Wheter you want to coerce errors - in this case will return  {iso:[final energy variables]} with negative values
    known_issues: dict, optional
        A dictionary with known issues of negative variables at the regional level: {PROJECT:MODEL:{REGION:VARIABLE}}. 
        Example => {'SHAPE_2023':{'IMAGE 3.3': {'IMAGE 3.3|SAF': 'Primary Energy|Oil|w/ CCS'}}}, by default None 
    Raises
    ------
    ValueError
        Raises error if energy values are below zero
    """
    df = df.copy(deep=True)
    check = list(df.xs(unit, level="UNIT").reset_index().VARIABLE.unique())
    for v in exlude_var:
        check = [x for x in check if v not in x]

    df = fun_index_names(fun_xs(df, {"VARIABLE": check}), True, int)
    df = np.round(df, decimals)
    df = df[df < 0].dropna(how="all")
    err_dict={}
    if len(df):
        err_dict = (
            df.reset_index()[["REGION", "VARIABLE"]]
            .set_index("REGION")
            .to_dict()["VARIABLE"]
        )
        if coerce_errors:
            return err_dict
    # Check for known issues (special cases for which we do not raise errors)
    if known_issues:
        if len(df.reset_index().MODEL.unique())==1:
            model=df.reset_index().MODEL.unique()[0]
            err_dict = fun_skip_special_cases(model, known_issues, err_dict)
    if len(err_dict):
        txt2= "If this error is present at the regional level please pass a `known_issues`. Example => known_issues={'SHAPE_2023':{'IMAGE 3.3': {'IMAGE 3.3|SAF': 'Primary Energy|Oil|w/ CCS'}}}"
        raise ValueError(f"We found negative energy variables for {err_dict}. {txt2}." )
    return err_dict

def fun_skip_special_cases(model, known_issues, err_dict):
    projects=list(known_issues)
    if len(projects)>1:
        raise ValueError(f'`known_issues` dict should contain only one project as key. You passed: {projects}')
    project=projects[0]
    known_issues=list(known_issues.values())[0]
    known_issues=fun_invert_dictionary({k:[v] for k,v in known_issues[model].items()})
    res={}
    for k,v in known_issues.items():
        countrylist=[]
        for c in v:
            countrylist=countrylist+fun_regional_country_mapping_as_dict(model, project)[f"{c.split('|')[-1]}r"] 
        res[k]=countrylist 
    known_issues =fun_invert_dictionary(res)
        # err_dict=fun_invert_dictionary({k:[v] for k,v in err_dict.items()})
    err_dict={k:[v] if not isinstance(v, list) else v for k,v in err_dict.items()}
    err_dict={k:v  for k,v in err_dict.items() for c in v if k not in known_issues or c not in known_issues[k] }
    return err_dict

def fun_check_negative_energy_variables_by_model(
    df: pd.DataFrame,
    decimals: int = 4,
    exlude_var: tuple = ("Trade", "Statistical"),
    sel_cols: range = range(2010, 2105, 5),
    unit: str = "EJ/yr",
    coerce_errors: bool = False,
    project: Optional[str]=None,
):
    """Checks if a `df` contains negative energy variables by model. If this is the case, raise Value Error.
    We exclude variables that contain strings as specified in `exlude_var`.

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe
    decimals : int, optional
        Round df using a number of decimals, by default 4
    exlude_var : tuple, optional
        Strings that should not be included in the variable name, by default ("Trade", "Statistical")
    sel_cols: range, optional
        Check dataframe just for some selected time columns, by default range(2010, 2055, 5)
    unit: str, optional
        Unit of energy variable, by default 'EJ/yr'
    coerce_errors: bool
        Wheter you want to coerce errors - in this case will return  {iso:[final energy variables]} with negative values
    project: str, optional
        Your project folder (e.g. NGFS_2023), by default None.  
        This is used to provide a more precise error message when we find negative energy variables (esepcially in step0)
        and show how to create a `known_issues` dictionary (which should contain `project` as key).
    Raises
    ------
    ValueError
        Raises error if energy values are below zero
    """

    print("\nChecking negative energy variables...")
    res = {}
    for m in df.reset_index().MODEL.unique():
        df_check = fun_index_names(df.loc[m], True, int)[sel_cols]
        if unit in df_check.reset_index().UNIT.unique():
            negative_energy = fun_check_negative_energy_variables(
                df_check,
                decimals=decimals,
                exlude_var=exlude_var,
                unit=unit,
                coerce_errors=True,
            )
            if negative_energy:
                res[m] = negative_energy
    if len(res):
        if project:
            res={project:res}
        text = f"We found negative values for:\n {res} \n"
        if coerce_errors:
            print(text)
            return res
        text2="If you want to continue, please add this as `known_issues` in the config_file.yaml (please remember to start with the project name in your dictionary)"
        example="{'SHAPE_2023':{'IMAGE 3.3': {'IMAGE 3.3|SAF': 'Primary Energy|Oil|w/ CCS'}}}"
        action = input(f"{text}  {text2}. Example: known_issues:{example} \n\n known_issues:{res} \n\n Would you like to continue?")
        if action.lower() not in ["yes", "y"]:
            raise ValueError(f"Simulation aborted by the user (user input={action})")
    else:
        print("all good with energy variables")

def fun_aggregate_non_iea_countries_all_models_scen_var(project:str, df_csv:pd.DataFrame)->pd.DataFrame:
    """Aggregates all downscaled non-iea countries for all models, variables, regions and scenarios (unless non-iea country
    is reported as native region by IAMs).

    Parameters
    ----------
    project : str
        Your project (e.g. NGFS 2023), from which we take the regional country mapping
    df_csv : pd.DataFrame
        Your dataframe in IAMC format. 
    Returns
    -------
    pd.DataFrame
        Dataframe with non-iea countries aggregated in a single region called "Non-IEA"
    """    
    df_res=pd.DataFrame()
    models=list(df_csv.reset_index().MODEL.unique())
    units_dict=fun_get_variable_unit_dictionary(df_csv)
    idx=df_csv.index.names
    idx=[x for x in idx if x!= 'UNIT']
    # The line below should exclude all price info from non-IEA aggregation
    price_var = fun_check_non_summation_variables(units_dict)
    for model in models:
        # regions_dict_iea=fun_regional_country_mapping_as_dict(model, project, iea_countries_only=True)
        regions_dict_all=fun_regional_country_mapping_as_dict(model, project, iea_countries_only=False)
        # regions_with_noniea_countries=set([k for k,v in regions_dict_all.items() for x in v  if x not in iea_countries])
        for r, iso_list in regions_dict_all.items():
            # if r in regions_with_noniea_countries and len(iso_list)>1:
            if len(iso_list)>1:
                # Aggregate downscaled non-IEA countries
                df_temp_all=fun_xs(df_csv.xs(model), {"REGION":iso_list})
                available_vars=df_temp_all.reset_index().VARIABLE.unique()
                if len(df_temp_all):
                    for var in units_dict:
                        if var in available_vars:
                            df_temp=df_temp_all.xs(var, level='VARIABLE')
                            if 'UNIT' in df_temp.index.names:
                                df_temp=df_temp.droplevel('UNIT')
                            scenarios=list(df_temp.reset_index().SCENARIO.unique())
                            price_info= var in price_var
                            # For price information we report an average across countries
                            # reg_name=f"Non-IEA|{r[:-1]}"
                            reg_name=f"Non-IEA countries of {r[:-1]} region of {model}"
                            df_temp=fun_non_iea_countries(scenarios, df_temp, iso_list, reg_name=reg_name,
                                                          price_info=price_info)
                            df_temp=df_temp.assign(MODEL=model).assign(VARIABLE=var)#.set_index(['MODEL','VARIABLE'], append=True)
                            df_temp=df_temp.reset_index().set_index(idx)
                            df_res=pd.concat([df_res, df_temp])
                print(f"{r} - non-iea aggregation done")
            else:
                # Report non-IEA countries that are native regions
                df_temp=fun_xs(df_csv, {"REGION":iso_list, "MODEL":model}).droplevel('UNIT')
                df_temp=df_temp.reset_index().set_index(idx)
                df_res=pd.concat([df_res, df_temp])
    print(f'Variables reported using average across non-iea countries: {price_var}')
    df_res=fun_add_units(df_res, None, units_dict, missing_unit="missing")
    return df_res.reset_index().set_index(df_csv.index.names)

def fun_check_non_summation_variables(units_dict:dict)->list:
    """Dictionary with units (values) for each variable (keys)

    Parameters
    ----------
    units_dict : dict
        Dictionary with {variable:unit}

    Returns
    -------
    list
        List of variables that should not be summed up across countries (e.g. price info)
    """    
    summation_var={k:v for k,v in units_dict.items() if ('/yr' in v or 'million' in v.lower()) 
                and 'price' not in k.lower() and 'index' not in v.lower() and '%' not in v
                and 'years' not in v}
    # Price and other variables that cannot be summed up (for which we use the average across countries)
    price_var=[x for x in units_dict if x not in summation_var]
    return price_var

def fun_non_iea_countries(scenarios:list, dfs:pd.DataFrame, countrylist:list, reg_name:str="Non-IEA", price_info:bool=False)->pd.DataFrame:
    """Aggregates all non-iea countries to a single region called `reg_name` (by default "Non-IEA"). It works
    for a single model, variable, region (and for a list of scenarios).

    Parameters
    ----------
    scenarios : list
        List of scenarios included in the dataframe
    dfs : pd.DataFrame
        Your dataframe with index.names = ['SCENARIO','REGION']. All countries in this datafrane should belong
        to the same IAM region. 
    countrylist : list
        List of all countries in a region (including both IEA and non-iea countries)
    reg_name:str, optional
        Name of aggregated region, by default "Non-IEA"
    price_info:bool, optional
        Wheter this is a price or another non-summation variable for which we report the average across 
        countries (instead of the sum), by default False
    Returns
    -------
    pd.DataFrame
        Dataframe with non-iea countries aggregated in a single region called "Non-IEA"
    """    
    noniea=[x for x in countrylist if x not in iea_countries and x in dfs.reset_index().REGION.unique()]
    df_noniea=fun_xs(dfs, {"REGION":noniea})
    df_noniea=df_noniea.groupby("SCENARIO").sum()
    if not (len(df_noniea)):
        df_noniea=pd.DataFrame({t:{s:0 for s in scenarios} for t in dfs.columns})
    df_noniea.index.names=["SCENARIO"]
    df_noniea=df_noniea.assign(REGION=reg_name).set_index("REGION", append=True)
    if price_info:
        # We report the average (sum divided by number of non-iea countries)
        df_noniea=df_noniea/len(noniea)
    if len(noniea):
        dfs=dfs.drop(noniea, level="REGION")
    dfs=pd.concat([dfs, df_noniea])
    return dfs

def fun_create_sample_df(models=['MESSAGEix-GLOBIOM 1.1-M-R12'], 
                         countrylist=['CAN', 'USA'], 
                         variables=['Final Energy'],    
                         unit='EJ/yr',
                        scenarios=['d_delfrag','d_strain'],
                        timerange=range(2010,2105,5),
                        val=0)->pd.DataFrame:
    """Creates a sample dataframe in IAMc format.

    Parameters
    ----------
    models : list, optional
        List of models, by default ['MESSAGEix-GLOBIOM 1.1-M-R12']
    countrylist : list, optional
        List of countries, by default ['CAN', 'USA']
    variables : list, optional
        List of variables, by default ['Final Energy']
    unit : str, optional
        Unit, by default 'EJ/yr'
    scenarios : list, optional
        List of scenarios, by default ['d_delfrag','d_strain']
    timerange : _type_, optional
        Time range of your dataframe, by default range(2010,2105,5)
    val : int, optional
        Values in your sample dataframe, by default 0

    Returns
    -------
    pd.DataFrame
        Sample dataframe in IAMC format
    """    
    df=pd.DataFrame({t:{(m, s, c, var, unit): val for s in scenarios for c in countrylist 
                        for var in variables for m in models} for t in timerange})
    df.index.names=['MODEL', 'SCENARIO', 'REGION', 'VARIABLE','UNIT']
    return df

# step1b sample file
def fun_from_iamc_to_step1b(df:pd.DataFrame, colname:str='RENAME_ME', method='mymethod'):
    """Switch from IAMC to step1b format. 
    NOTE: for the reverse just use `df.unstack('TIME')['RENAME_ME']`

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe in IAMC format
    colname : str, optional
        Naeme of the column (e.g. ENLONG_RATIO/ENSHORT_REF), by default 'RENAME_ME'

    Returns
    -------
    pd.DataFrame()
        Dataframe in step1b format
    """    
    df=df.copy(deep=True)
    if 'METHOD' not in df.reset_index().columns:
        df=df.assign(METHOD=method).set_index('METHOD', append=True)
    df=fun_rename_index_name(df,{'SCENARIO':'TARGET','VARIABLE':'SECTOR','REGION':'ISO'}).droplevel('MODEL').stack()
    df.index.names=['TIME' if x is None else x  for x in df.index.names]
    return df.droplevel('UNIT').reset_index().set_index(['TIME', 'ISO', 'TARGET', 'SECTOR', 'METHOD']).rename({0:colname}, axis=1)

def fun_aggregate_countries_general(df_merged:pd.DataFrame, reg_name:str, countrylist:list, remove_single_countries:bool=False)->pd.DataFrame:
    """Aggregate data for the specified region by summing up all countries in the list. 
    Separate out non-summation variables (e.g., price) and compute the average (instead of sum) for those variables.

    Parameters
    ----------
    df_merged : pd.DataFrame
        DataFrame containing the data to be aggregated.
    reg_name : str
        Name of the region to which the aggregated data belongs.
    countrylist : list
        List of countries to be aggregated.
    remove_single_countries : bool, optional
        Whether to remove single-country entries after aggregation, by default False.

    Returns
    -------
    pd.DataFrame
        Aggregated DataFrame.
    """    
    df_merged=df_merged.copy(deep=True)
    idxnames=df_merged.index.names
    units_dict=fun_get_variable_unit_dictionary(df_merged)
    
    # Step 1) Separate out non-summation variables (e.g., price) from aggregation (quantity) variables (e.g. Final Energy)
    # `price_var` contains all price variables (for those we calculate the average)
    # `df_aggregated` contains all summation variables (for those we sum up all the values)
    price_var = fun_check_non_summation_variables(units_dict)
    price_df = fun_xs(df_merged, {'VARIABLE': price_var, 'REGION':countrylist}).dropna(how='all')
    available_countries=price_df.reset_index().REGION.unique() 
    # NOTE we divide by `available_countries` instead of `countrylist` because some countries may be not available (also because we have dropped n/a variables).
    # This avoids possible underestimation of average calculations just because of n/a variables or missing countries
    price_df=price_df/len(available_countries)#.copy(deep=True)
    df_aggregated = fun_xs(df_merged, {'REGION':countrylist})
    df_aggregated = fun_xs(df_aggregated, {'VARIABLE': price_var}, exclude_vars=True)
    
    # Step 2) Calculate regional values for both `df_aggregated` and `df_price`, and store values in the `res` dictionary
    input_df={'quantity':df_aggregated, 'price':price_df}    
    res={}
    for k,df in input_df.items():
        res[k] = fun_aggregate_countries(
            df.reset_index().set_index(["MODEL", "REGION", "VARIABLE", "UNIT", "SCENARIO"]),
            reg_name,
            ["MODEL", "VARIABLE", "UNIT", "SCENARIO"],
            countrylist,
            remove_single_countries=True
        ).reset_index().set_index(idxnames)
    
    # Copy over results to `df_aggreagted`
    df_aggregated=pd.concat(list(res.values()))
    
    # Step3) Add single countries results, if requested
    if not remove_single_countries:
        df_aggregated = pd.concat([df_aggregated, df_merged], sort=True)
    else:
        df_aggregated = pd.concat([df_aggregated, fun_xs(df_merged,{'REGION':countrylist}, exclude_vars=True) ])
    
    return df_aggregated

def fun_check_inconsistencies(df_iam:pd.DataFrame, var_dict_demand:dict, verbose=False, coerce=True, absolute:bool=False)->dict:
    """Check inconsistencies in a dataframe (IAMC format) greater than 1%, for all variables defined in `var_dict_demand`

    Parameters
    ----------
    df_iam : pd.DataFrame
        Your dataframe in IAMC format
    var_dict_demand : dict
        A dictionary with your variable definition e.g {'Final Energy':['Final Energy|Industry', 'Final Energy|Transportation']}
    verbose: bool
        Whether you want the function to print variables not found in the dataframe, by default False.
    coerce: bool
        Whether you want to coerce raise value errors, by default True.
    absolute: bool
        Whether you want to calculate inconsistencies in absolute value raise value errors, by default False.
    Returns
    -------
    dict
        Variables with a discrepancy higher than 1% in absolute value
    """    
    df_iam=df_iam.copy(deep=True)
    if len(df_iam)==0:
        raise ValueError('Your dataframe is empty. Unable to check inconsistencies')
    df_iam.index.names=[x.replace('SCENARIO','TARGET').replace('SECTOR','VARIABLE').replace('ISO','REGION') for x in df_iam.index.names]
    variables=df_iam.reset_index().VARIABLE.unique()
    res={}
    missing_vars=[]
    threshold = 0.001 if absolute else 0.01 # NOTE 0.001 is EJ/yr if absolute, else 1%
    for k,v in var_dict_demand.items():
        if k in variables:
            num=df_iam.xs(k, level='VARIABLE')
            den=fun_xs(df_iam, {'VARIABLE':v}).groupby(['MODEL','TARGET','REGION','UNIT']).sum()
            den=den.reset_index().set_index(num.index.names)
            num=num.clip(1e-5)
            den=den.clip(1e-5)
            if absolute:
                val=abs(num-den).max().max()
            else:
                val=abs(num/den-1).max().max()
            if val>threshold:
                res[k]=val
        else:
            missing_vars=missing_vars+[k]
            if verbose:
                print(f"{k} not present in df. Var present: {variables}")
    if set(missing_vars)==set(var_dict_demand.keys()):
        if not coerce:
            raise ValueError('None of the keys in your dictionary (e.g`var_dict_demand`) are present in the dataframe ')
    return res

def fun_from_step1b_to_iamc(df, col='ENLONG_RATIO', method=None, coerce_errors=False):
    df=df.copy(deep=True)
    if 'METHOD' in df.reset_index().columns:
        if not method:
            methods=df.reset_index().METHOD.unique()
            raise ValueError(f'Please choose a method among those: {methods}, you passed None')
        if method =='wo_smooth_enlong' and col not in ['ENLONG_RATIO', 'ENSHORT_REF']:
            text=f"Method `{method}` is not available for `col`= {col}. Dataframe will just contain `nan` values"
            if coerce_errors:
                print(text)
            else:
                raise ValueError(text)
        df=df.xs(method, level='METHOD')
    df= fun_rename_index_name(df[col], {'TARGET':'SCENARIO', 'SECTOR':'VARIABLE'}).unstack('TIME')
    df=df.assign(MODEL='model', UNIT='EJ/yr').set_index(['MODEL','UNIT'], append=True)
    return df.reset_index().set_index(["MODEL", "SCENARIO", "ISO", "VARIABLE", "UNIT"])

def fun_resolve_inconsistencies_step1b( sectors:list, ec:list, obs:pd.DataFrame, col:str, df_iam:Optional[pd.DataFrame], more_sectors:bool=True, 
                                       more_ec:bool=False, solve_demand_first:bool=False, absolute:bool=True,     max_iter:int=30)->pd.DataFrame:
    """Resolves inconsistencies by solve inconsistencies adding 'Other' to the list of `sectors` or `ec` (energy carriers).

    Parameters
    ----------
    sectors : list
        List of sectors e.g. ['Industry', 'Transportation',...]
    ec : list
        List of energy carriers e.g. ['Solids', 'Liquids', ...]
    obs : pd.DataFrame
        Your dataframe with inconsistencies to be resolved (in step1b format)
    col : str
        The column of the dataframe to be resolved (e.g. 'ENLONG_RATIO')
    df_iam : Optional[pd.DataFrame]
        Dataframe with Regional IAMs results. If available, will harmonize country level results to match regional IAMs values.
    more_sectors : bool, optional
        Whether to add `Other` to the list of `sectors`, by default True
    more_ec : bool, optional
        Whether to add `Other` to the list of energy carriers (`ec`), by default True
    solve_demand_first : bool, optional
        Wheter you want to solve energy demand (`sectors`) first, by default False (to prevent 
        impossible energy transitions, for details see https://github.com/iiasa/downscaler_repo/issues/186)
    absolute: bool
        Whether you want to calculate inconsistencies in absolute value raise value errors, by default True.
        # NOTE `absolute` defaults to True only if we want to resolve inconsistencies (we care more about large sectors/values.)
    max_iter: int
        Number of maximum interations, by default 30
    Returns
    -------
    pd.DataFrame
        Dataframe with resolved inconsistencies
    """    
    obs=obs.copy(deep=True)
    # Create list of new sectors
    if more_sectors and 'Other sector (downscaling)' not in sectors:
        sectors=sectors+['Other sector (downscaling)']
    if more_ec and 'Other energy carrier (downscaling)' not in ec:
        ec=ec+['Other energy carrier (downscaling)']
    var_dict_demand_new = fun_make_var_dict(sectors, ec, demand_dict=True)[0]
    var_dict_supply_new = fun_make_var_dict(ec, sectors, demand_dict=False)

    # Remove sub-sectors not present in `df_iam`, except for the `(downscaling)` sectors/energy carriers.
    if df_iam is not None:
        iam_vars=df_iam.reset_index().VARIABLE.unique()
        var_dict_demand_new={k:[vv for vv in v if 'downscaling' in vv or vv in iam_vars]
                                for k,v in var_dict_demand_new.items()}
        var_dict_supply_new={k:[vv for vv in v if 'downscaling' in vv or vv in iam_vars]
                                for k,v in var_dict_supply_new.items()}

    # Slice dataframe to make calculations faster
    available_methods=obs.reset_index().METHOD.unique()
    if col in ['ENSHORT_REF', 'ENLONG_RATIO']:
        allowed_methods=['wo_smooth_enlong', 'mymethod']
        obs=pd.concat([obs.xs(k, level='METHOD', drop_level=False) for k in allowed_methods if k in available_methods])
    else:
        if 'wo_smooth_enlong' in obs.reset_index().METHOD.unique():
            df_remove=obs.xs('wo_smooth_enlong', level='METHOD', drop_level=False)
            obs=obs.loc[~obs.index.isin(df_remove.index)]
        
    # Convert to IAMC format and create missing sector
    res={}
    for method in obs.dropna(how='all').reset_index().METHOD.unique():
    # for method in obs.reset_index().METHOD.unique():
        if method =='wo_smooth_enlong' and col not in ['ENLONG_RATIO', 'ENSHORT_REF']:
            # This method is not available for this `col`
            # res[method]=obs.xs(method, level='METHOD', drop_level=False)[[col]]
            pass
        else:
            # Convert to IAMC for each method
            res[method]=fun_from_step1b_to_iamc(obs, col=col, method=method)
            
            # Create missing sectors
            for _ in range(2):
                # NOTE: By doing this two times we create 'Final Energy|Other sector(downscaling)|Other energy carrier (downscaling)', otherwise will be missing.
                res[method]=fun_create_missing_sectors_as_difference(res[method], var_dict_demand_new)
                res[method]=fun_create_missing_sectors_as_difference(res[method], var_dict_supply_new)
            expected_vars=fun_flatten_list(var_dict_supply_new.values())+fun_flatten_list(var_dict_demand_new.values())
            obs_vars=res[method].reset_index().VARIABLE.unique()
            missing_vars=[x for x in expected_vars if x not in obs_vars]
            if df_iam is not None:
                for x in set(missing_vars):
                    res[method]=fun_create_var_as_sum(res[method], x, {'Final Energy':1e-10})
            
            # Clip negative energy data and convert to step1b
            res[method]=fun_from_iamc_to_step1b(res[method], method=method).rename({'RENAME_ME':col}, axis=1).fillna(0).clip(1e-10)
    

    # Create missing sectors in df_iam
    # NOTE: the `for _ in range(2)` CAN BE REMOVED WITH OUR ALTERNATIVE APPROACH.
    if df_iam is not None:
        for _ in range(2):
            # NOTE: By doing this two times we create 'Final Energy|Other sector(downscaling)|Other energy carrier (downscaling)', otherwise will be missing.
            df_iam = fun_create_missing_sectors_as_difference(df_iam, var_dict_demand_new)
            df_iam = fun_create_missing_sectors_as_difference(df_iam, var_dict_supply_new)

    # obs=fun_from_step1b_to_iamc(obs, col=col, method=method)
    # obs=fun_rename_index_name(obs, {'SCENARIO':'TARGET', 'ISO':'REGION', 'VARIABLE':'SECTOR'})
    obs=pd.concat(list(res.values()))
    # Re-harmonize including the new 'Other' sector
    res={'Start':10}#'with a non empty dict'}
    count=0
    error_dict={}
    # error_dict_abs={}
    action_dict={}
    gradient=0.1 # 1 means no improvement compared to previous iteration. 0.9 means 10% improvement.
    average_error_variation=1

    if absolute:
        # NOTE Units of Threshold below are in EJ/yr
        # average_threshold=0.003  # Average error across sectors should be below this threshold
        # max_threshold=0.005 # Max error across sectors should be below this threshold
        average_threshold=0.001 # Average error across sectors should be below this threshold
        max_threshold=0.03 # Max error across sectors should be below this threshold
    else:
        # NOTE Units of Threshold below are in %
        average_threshold=0.03 # Average error across sectors should be below this threshold
        max_threshold=0.05 # Max error across sectors should be below this threshold
    # NOTE We will check inconsitencies for one method. Below is how we choose the method:
    if col not in ['ENSHORT_REF', 'ENLONG_RATIO']:
    # if col  in ['ENSHORT_REF', 'ENLONG_RATIO']:
        # method='wo_smooth_enlong'
    # else:
        # Just pick one method that is not 'wo_smooth_enlong'
        method=[x for x in obs.reset_index().METHOD.unique() if x not in 'wo_smooth_enlong'][0]
    # while len(res)>0: 
    if solve_demand_first:
        dict_list0=[var_dict_demand_new, var_dict_supply_new]
    else:
        dict_list0=[var_dict_supply_new, var_dict_demand_new]

    # while len(res)>0 and gradient <0.90: # 0.9 means that we want at least a 10% improvement (otherwise exit loop)
    # while sum(list(res.values())) >0.2:
    dict_list=dict_list0
    d1, d2 = dict_list0[0], dict_list0[1]
    while sum(list(res.values()))/len(res)>average_threshold: # While average error is more than 3%
    # while sum(list(res.values()))>average_threshold: # While average error is more than 3%
    # while sum(list(res.values())) >0.2:
        action_dict[count]='continue'
        mysector=None # Large sector
        # res_large_errors={k:v for k,v in res.items() if v>2*np.median(list(res.values()))}
        # We take the 3 largest sectors
        # res_large_errors={k:res[k] for k in list(res.keys())[:3]}
        res_large_errors=res
        # if len(res_large_errors)>0 and df_iam is not None and gradient<0.999:
        myres= res_large_errors if len(res_large_errors)>0 else res
        # if gradient >1.01:
        #     # NOTE 'gradient worsened - go back to original order and to the full list'
        #     d1=dict_list0[0]
        #     d2=dict_list0[1]
        #     dict_list=[d1, d2]
        #     action_dict[count]='back to original order and list (gradient worsened)'

        if gradient <0.5:
            pass
        elif gradient>0.5 and gradient<0.8:
            # `Final Energy` should be always included?? no, does not seem to be an issue. issue is that sectors not included get worse 
            if len ({k:v for k,v in dict_list[0].items() if k in myres.keys()}):
                d1={k:v for k,v in dict_list[0].items() if k in myres.keys()}
            else:
                d1=dict_list[0]
            if len ({k:v for k,v in dict_list[1].items() if k in myres.keys()}):
                d2={k:v for k,v in dict_list[1].items() if k in myres.keys()}
            else:
                d2=dict_list[1]
            dict_list=[d1,d2]
            action_dict[count]='update d1,d2 (smaller dictionaries)'
            pass

        # elif len([x for x in list(res.keys())[:2] if x in list(d1.keys())])==2:
        #     # Inverting dictionaries if worst errors (first two keys in res) are all present `d1` (first dictionary)
        #     dict_list=[dict_list[1], dict_list[0]]
        #     action_dict[count]='invert dictionaries'
        else:
            # NOTE 'gradient worsened - go back to original order and to the full list'
            d1=dict_list0[0]
            d2=dict_list0[1]
            dict_list=[d1, d2]
            action_dict[count]='back to original order and list (gradient worsened)'

        
        # NOTE: After `max_iter` (e.g. 30) iterations we break the loop. 
        # After `max_iter-5` iterations (e.g. 25) we have less stringent rules. 
        # Therefore from `max_iter`-6 loops (e.g.24 ) we want to keep original dictionary 
        if count>=max_iter-6: 
            dict_list=[dict_list0[0], dict_list0[1]]
            action_dict[count]='back to original dictionaries (count>24)'
                        
            # NOTE: If we use smaller dictionaries this could lead to inconsistent sum of country level results
        for d in dict_list:
            # obs=obs.clip(1e-7) # This one worsen the convergence (but makes it a bit faster)
            # if not(len(res)==1 or df_iam is None):
            if mysector is not None and mysector in d:
                # if mysector in d:
                dsel=SliceableDict(d).slice(mysector)
            else:
                dsel=d
            # else:
            #     dsel=d
            #     mysector=None
            obs =run_sector_harmo_enhanced(
                    obs, # downscaled results in step1b format
                    dsel,
                    col, # ENSHLORT_REF/ENLONG_RATIO
                    df_iam, # regional iam results
                    w=0.8,
                ).clip(0)
        
        # NOTE: Goal is to minimize worse inconsistencies across all methods
            
        methods=obs.reset_index().METHOD.unique()
        res_all_methods={method:fun_inconsistencies(obs, [var_dict_demand_new, var_dict_supply_new] ,col=col,method=method, absolute=absolute) for method in methods}
        res_all_methods_sum=fun_sort_dict({k:sum(list(v.values())) for k,v in res_all_methods.items()}, by='values', reverse=True)
        worse_method=list(res_all_methods_sum.keys())[0]
        
        res=res_all_methods[worse_method] # we consider error for the worse method
        ## TODO USE THIS FUNCTION INSTEAD get_worst_method_inconsistencies

        # Below we consider sum of errors across all methods
        # res=sum({k:sum(list(v.values())) for k,v in res_all_methods.items()}.values())
        # mysector=list(res.keys())[0]if len(res) else None  # sector/ec with largest inconsistencies
        # for error_dict we take the sum of errors across all methods
        error_dict[count]=sum({k:sum(list(v.values())) for k,v in res_all_methods.items()}.values())

        if count>=1:
            gradient=error_dict[count]/error_dict[count-1]
            average_error_variation=np.std(list(error_dict.values())[-5:])/np.mean(list(error_dict.values())[-5:])
        if len(res)==0:
            break
        if count>=max_iter-5 and gradient>0.9: # e.g. After 25 iterations we break loop if poor improvements.
            break
        if count>=max_iter: # e.g. After 30 iterations we break the loop anyway.
            break
        # if sum(list(res.values())) <0.2: # If sum of all errors (across sectors) is less than 20% break the loop
        #     break
        if max(list(res.values())) <max_threshold :
            if gradient >0.9 : # 0.9 means 10% improvement
                # If maximum error is less than 5% in each sector, and improvements are small, break the loop
                break
        if count>=5 and average_error_variation<0.05 and mysector is None and gradient >0.999 and dict_list0==dict_list : # 0.9 means that we want at least a 10% improvement (otherwise exit loop)
            # if np.round(list(error_dict.values())[-1],4)== np.round(min(list(error_dict.values())),4): # this is maybe tricky 
            break
        count+=1
    if len(res)>0:
        error=sum({k:v for k,v in res.items() if 'downscaling' not in k }.values())
        if (sum(list(res.values()))/len(res)<average_threshold and gradient>0.8) or (max(list(res.values())) <max_threshold) :
            print(f'Inconsistencies mostly resolved. Sum of % errors: {error}')
        else:
            print(f'Unable to resolve inconsistencies in step1b. Sum of % errors: {error}')
        # Dataframe with actions taken based on error
        df_error_dict=pd.DataFrame({'error':error_dict, 'action':action_dict})
        df_error_dict['action']=df_error_dict['action'].shift(-1)
        
        #TODO Maybe save df below to csv
        print(np.round(show_inconsistencies(obs, [var_dict_demand_new, var_dict_supply_new], priority=None),4))
    
    else:
        print('Inconsistencies in step1b resolved!')

    return obs

def fun_most_recent_iea_data()->pd.DataFrame:
    """Reads most recent energy data from IEA. Final Energy from IEA 2019, and Secondary/Primary data from  IEA Energy Statistics (r2022).
    Emissions data are not included.

    Returns
    -------
    pd.DataFrame
        Dataframe with energy data from IEA 
    """ 
    fname=CONSTANTS.INPUT_DATA_DIR/'Most_recent_iea_data.csv'   
    if os.path.exists(fname):
        return fun_read_csv({'aa':fname}, True, int)['aa'].rename({'Reference':'IEA'})
    # Final Energy from IEA 2019
    iea=fun_read_energy_iea_data_in_iamc_format (iea_flow_dict)
    fenvar=[x for x in iea.reset_index().VARIABLE.unique() if 'Final Energy' in x]
    iea=fun_xs(iea, {'VARIABLE':fenvar})
    
    # Primary and Secondary energy from IEA 2022
    penvar=['Primary Energy|Coal|w/ CCS', 
            'Primary Energy|Gas|w/ CCS', 
            'Primary Energy|Oil|w/ CCS', 
            'Primary Energy|Biomass', 
            'Primary Energy|Coal|w/o CCS', 
            'Primary Energy|Gas|w/o CCS', 
            'Primary Energy|Oil|w/o CCS', 
            'Primary Energy|Geothermal', 
            'Primary Energy|Hydro', 
            'Primary Energy|Nuclear', 
            'Primary Energy|Solar', 
            'Primary Energy|Wind']
    iea2=fun_read_iea_data_from_iamc_format(2019, penvar)
    iea2=iea2.rename({'IEA Energy Statistics (r2022)':'Historic data'})
    # Save dataframe to CSV
    pd.concat([iea,iea2]).rename({'Reference':'IEA'}).to_csv(fname)
    return fun_index_names(pd.concat([iea,iea2]).rename({'Reference':'IEA'}), True, int)


def fun_inconsistencies(df:pd.DataFrame, list_of_dicts:List[dict], by_sector:bool=True, by_country:bool=False, method:Optional[str]=None, col:Optional[str]=None, absolute:bool=False )->dict:
    """
    Check inconsistencies by country or sector in a dataframe `df`, either in step1b or IAMC format.
    If `df` is in step1b format you need to select a `method` and column `col`.
    NOTE: Please check https://github.com/iiasa/downscaler_repo/issues/181 for alternative/simpler versions

    Parameters:
    df (pd.DataFrame): DataFrame with inconsistencies to be checked (in IAMC or step1b format).
    list_of_dicts (Union[List[dict], dict]): List of dictionaries or a single dictionary containing variables and their sub-sectors.
    by_sector (bool, optional): Whether to calculate inconsistencies by sector. Defaults to True.
    by_country (bool, optional): Whether to calculate inconsistencies by country. Defaults to False.
    method (str, Optional): Method to check inconsistencies, if your dataframe is in step1b format.
    col (str, Optional): The column of the dataframe to be checked (e.g., 'ENLONG_RATIO'), if your dataframe is in step1b format.
    absolute (bool): Whether you want to calculate inconsistencies in absolute value raise value errors. Defaults to False.
    Returns:
    Union[dict]: Inconsistencies by country or sector.
    """
    df=df.copy(deep=True)
    if 'TIME' in df.index.names:
        if not method and 'METHOD' in df.index.names:
            raise ValueError('Your dataframe seems to be in step1b format (it contains a METHOD index). Please pass a `method`e .g. `wo_smooth_enlong`')
        if not col:
            raise ValueError('Your dataframe seems to be in step1b format (it contains a METHOD index). Please pass a `col` e.g. `ENLONG_RATIO` ')
        df=fun_from_step1b_to_iamc(df, col=col, method=method)
    else:
        df=fun_rename_index_name(df, {'REGION':'ISO'})
    
    # Check if list_of_dicts is a single dictionary, if so, convert it to a list
    if isinstance(list_of_dicts, dict):
        list_of_dicts = [list_of_dicts]

    # Validate input arguments
    if not (by_sector or by_country):
        raise ValueError("Either `by_sector` or `by_country` needs to be True")

    iso_unique = df.reset_index().ISO.unique()
    inconsistencies={}
    sum_inconsistencies={}    
    
    if by_sector==True and by_country==False:
        for x in enumerate(list_of_dicts):
            if x[0]==0:
                d=fun_check_inconsistencies(df,x[1], absolute=absolute)
            else:
                # Sum up current and previous dictionary
                d=sum_two_dictionaries(d,fun_check_inconsistencies(df,x[1],absolute=absolute))
        # This is better than `d.update()` to avoid sectors present in both dictionaries being overwritten 
        return fun_sort_dict(d, by='values', reverse=True)

    for c in iso_unique:
        df_bycountry=df.xs(c, level='ISO', drop_level=False)
        for x in enumerate(list_of_dicts):
            if x[0]==0:
                d=fun_check_inconsistencies(df_bycountry,x[1],absolute=absolute)
            else:
                # Sum up current and previous dictionary
                d=sum_two_dictionaries(d,fun_check_inconsistencies(df_bycountry,x[1],absolute=absolute))
        inconsistencies[c] =fun_sort_dict(d,by='values', reverse=True)
        # Sum of inconsistencies across sectors, by country
        sum_inconsistencies[c] = sum(inconsistencies[c].values())
    # Order by country with largest inconsistencies
    sum_inconsistencies=fun_sort_dict(sum_inconsistencies, by='values', reverse=True)
    inconsistencies={k:inconsistencies[k] for k in sum_inconsistencies.keys()}
    if by_country and not by_sector:
        return sum_inconsistencies 
    if by_country and by_sector: 
        return inconsistencies

def get_worst_method_inconsistencies(obs: pd.DataFrame, var_dicts: List[Dict[str, List[str]]], col: str, absolute:bool=False) -> Dict[str, float]:
    """
    Identify the worst-performing method and show inconsistencies.

    Parameters:
    obs (pd.DataFrame): DataFrame with inconsistencies.
    var_dicts (List[Dict[str, List[str]]]): List of dictionaries with variable mappings.
    col (str): Column name.

    Returns:
    Dict[str, float]: Dictionary containing inconsistencies for the worst-performing method.
    """
    # Select relevant methods below
    available_methods=obs.reset_index().METHOD.unique()
    if col in ['ENSHORT_REF', 'ENLONG_RATIO']:
        methods=[x for x in available_methods if x in ['wo_smooth_enlong', 'mymethod']] 
    else:
        methods=[x for x in available_methods if x not in ['wo_smooth_enlong']]

    # Calculate inconsistencies for each method below
    res_all_methods = {
        method: fun_inconsistencies(obs, var_dicts, col=col, method=method, absolute=absolute) for method in methods
    }
    res_all_methods_sum = fun_sort_dict(
        {k: sum(list(v.values())) for k, v in res_all_methods.items()}, by='values', reverse=True
    )
    worst_method = list(res_all_methods_sum.keys())[0]
    return {worst_method:res_all_methods[worst_method]}

def get_worst_scenario_inconsistencies(df:pd.DataFrame, my_dicts:List[Dict[str, List[str]]], absolute:bool=False)->Dict[str, float]:
    """Identify scenario with largest inconsistencies (and show them in a dictionary)

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe to be checked
    my_dicts : List[Dict[str, List[str]]]
        Dictionaries with scenario mapping

    Returns
    -------
    Dict[str, float]
        Dictionary containing inconsistencies for the worst-performing scenario.
    """    
    res_all_scen={scen:fun_inconsistencies(df.xs(scen, level='SCENARIO', drop_level=False), my_dicts, absolute=absolute) for scen in df.reset_index().SCENARIO.unique()}
    return fun_sort_dict(
        {k: sum(list(v.values())) for k, v in res_all_scen.items()}, by='values', reverse=True
    )

def get_worst_column(df:pd.DataFrame, my_dicts:List[Dict[str, List[str]]], absolute:bool=False)->Dict[str, float]:
    """Identify column with largest inconsistencies (and show them in a dictionary)

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe to be checked
    my_dicts : List[Dict[str, List[str]]]
        Dictionaries with scenario mapping

    Returns
    -------
    Dict[str, float]
        Dictionary containing inconsistencies for the worst column found.
    """
    res_all_t={t:fun_inconsistencies(df[[t]], my_dicts, absolute=absolute) for t in df.columns}
    return fun_sort_dict({k: sum(list(v.values())) for k, v in res_all_t.items()}, by='values', reverse=True)
    

def show_inconsistencies(df:pd.DataFrame, # var_dict_demand:dict, var_dict_supply:dict      
                         my_dicts:List[dict],
                         method=None,
                         scen=None,
                         c=None,
                         var:Union[int, str, List[str]]=0,  # 0 Means the worse variable. 1 Means the second worse
                         t=None,
                         priority:str='Final Energy',
                         absolute:bool=False
                         )->pd.DataFrame:
    
    """Show inconsistencies in dataframe (in step1b or IAMC format)

    Parameters:
    df (pd.DataFrame): DataFrame to be checked.
    var_dicts (List[Dict[str, List[str]]]): List of dictionaries with variable mappings.
    method (Optional str): Wheter you want to check a specific method (e.g. 'wo_smooth_enlong'). If None will search for the worst performing method, by default None.
    scen (Optional str): Wheter you want to check a specific scenario. If None will search for the worst performing scenario,  by default None.
    c (Optional str): Wheter you want to check a specific country. If None will search for the worst performing country,  by default None.
    var (Union[int, str, List[str]], defaults to 0): Wheter you want to check a specific variable. 0 means you want to check for the worst performing variable, 1 means the second worst etc., by default 0.
    t (Optional str): Wheter you want to check a specific time. If None will search for the worst performing time period,  by default None.
    priority (Optional str): Whether you want to prioritize a variable. Show this variable if consistencies are found (otherwise shows other variables with inconsistencies), by default `Final Energy`.
    absolute (bool): Whether you want to calculate inconsistencies in absolute value raise value errors. Defaults to False.

    Returns:
    Dict[str, float]: Dataframe showing largest inconsistencies found, for a given `method`, `scen`, `c`, `var`, and `t`.
    """

    # TODO automatically detect worse column

    # Step 0) Detect worse method in the `df` (if step1b format) and convert to IAMC format
    res_method_col=None
    if 'METHOD' in df.index.names:
        res_method_col = get_worst_column_and_method_inconsistencies(df, my_dicts, method, absolute=absolute)
        # worse_column_method, col = get_worst_column_method_inconsistencies(df, my_dicts, method)

        col=list(res_method_col.keys())[0]
        # Convert df to IAMc format for the worst method
        df=fun_from_step1b_to_iamc(df, col=col, method=res_method_col[col]) 

    df=fun_rename_index_name(df, {"REGION":"ISO"})
    
    # Select time, scenario, country, variable if provided
    if t is not None:
        df=df[[t]]
    if scen is not None:
        df=fun_xs(df, {'SCENARIO':scen})
    if c is not None:
        df=fun_xs(df, {'ISO':c})
    if not isinstance(var, int):
        df=fun_xs(df, {'VARIABLE':var})

    # Step1 detect worse scenario
    if scen is None:
        res=get_worst_scenario_inconsistencies(df, my_dicts, absolute=absolute)
        scen=list(res.keys())[0]
    df=fun_xs(df, {'SCENARIO':scen})
    
    # Step 2) Select worse country
    if c is None:
        res=fun_inconsistencies(df, my_dicts, by_country=True, absolute=absolute)
        c=list(res.keys())[0]
    
    
    df=fun_xs(df, {'ISO':c})

    # Step 3) Select worse sector (or energy carriers)
    if len(list(res.values())[0].keys())>0:
        all_vars=list(res.values())[0].keys()
    else:
        print('All sectors are consistent!')
        return pd.DataFrame()
    if priority in all_vars:
        var=priority
    else:
        var=list(list(res.values())[0].keys())[var]
    
    # var=list(res.keys())[0]
    
    # Select sub sectors associated to `var`. NOTE: my_dicts[0] is `var_dict_demand`,   my_dicts[1] is  `var_dict_supply` (or viceversa)
    subs=[]
    for d in my_dicts:
        if var in d:
           subs=subs+d[var]  
    # Select all relevant variables (`var` + subsectors)
    all_vars=[var]+subs


    # Step4 get worse time period:
    if t is None:
        t=list(get_worst_column(fun_xs(df, {'VARIABLE':all_vars}), my_dicts, absolute=absolute).keys())[0]
    df=df[[t]]

    if res_method_col is not None:
        df=df.assign(METHOD=res_method_col[col], COLUMN=col).set_index(['METHOD', 'COLUMN'], append=True)
    
    # NOTE Block below (optional) calculates sum according to dictionaries (with variable mapping) and concat to `df`
    data=pd.concat([fun_xs(df, {'VARIABLE':[var]}), fun_xs(df, {'VARIABLE':subs})])
    sum_dict={}
    for k,v in enumerate(my_dicts):
        if var in v:
            sum_dict[k]=fun_xs(df, {'VARIABLE':v[var]}).groupby([x for x in df.index.names if x!='VARIABLE']).sum().rename({t:f'd{k}'}, axis=1)    # Step4 Return Slice dataframe by variables and iso
    sector_sum=pd.concat(list(sum_dict.values()), axis=1).assign(VARIABLE=var).reset_index().set_index(data.index.names)    
    df=pd.concat([data, sector_sum], axis=1).droplevel(['UNIT'])
    
    return pd.concat([fun_xs(df, {'VARIABLE':[var]}), fun_xs(df, {'VARIABLE':subs})])

def get_worst_column_and_method_inconsistencies(df, my_dicts, method, absolute:bool=False):
    res_method_col={}
    df=df.copy(deep=True)
    selcols=[x for x in df.columns if ('ENSHORT_REF' in x or 'ENLONG_RATIO' in x) and 'EI_' not in x]
    if method is not None:
        df=fun_xs(df, {'METHOD':method})

    for col in selcols:
        res_worse_method=get_worst_method_inconsistencies(df, my_dicts, col, absolute=absolute)
            # Inconsistencies by country
        # tmp_method=list(res_worse_method.keys())[0]
        # res=res_worse_method[tmp_method]
        temp={sum(list(v.values())):k for k,v in res_worse_method.items()}
        temp=fun_sort_dict(temp, by='keys', reverse=True)
        val=list(temp.keys())[0]
        res_method_col[val]={col:temp[val]}
    
    res_method_col=fun_sort_dict(res_method_col, by='keys', reverse=True)
        
    res_method_col=res_method_col[list(res_method_col.keys())[0]]
    return res_method_col



def fun_min_across_datasets(df_list:List[pd.DataFrame], min=True):
    if min==True:
        return pd.concat(df_list, axis=0).min() 
    else:
        return pd.concat(df_list, axis=0).max() 
    

def resolve_subsector_inconsistencies(obs: pd.DataFrame, my_dicts: List[Dict[str, List[str]]]) -> pd.DataFrame:
    """
    Resolve inconsistencies where subsector values are larger than the main sector. 
    NOTE: We just do this for a single time/iso/target/sector/method/column, where we found the largest inconsistencies. 
    We don't do this throughout all the sub-sectors in the whole dataframe

    Parameters:
    obs (pd.DataFrame): DataFrame with inconsistencies in step1b format.
    my_dicts (List[Dict[str, List[str]]]): List of dictionaries containing variable mappings.

    Returns:
    pd.DataFrame: DataFrame with resolved inconsistencies.
    """
    # Calculate inconsistencies and clip values
    inconsistencies = np.round(show_inconsistencies(obs, my_dicts, absolute=False, priority=False), 4)
    inconsistencies = inconsistencies.clip(0, inconsistencies.iloc[0, :].to_dict(), axis=1)
    col=inconsistencies.reset_index().COLUMN.unique()[0]

    # Convert inconsistencies to step1b format and rename column
    step1b_inconsistencies = fun_from_iamc_to_step1b(inconsistencies).rename({'RENAME_ME':col}, axis=1)
    step1b_inconsistencies =step1b_inconsistencies.drop('COLUMN', axis=1)
    
    # Update original DataFrame with resolved inconsistencies
    obs.loc[step1b_inconsistencies.index, step1b_inconsistencies.columns]=step1b_inconsistencies
    
    return obs

def replace_element_in_list_with_another_list(mylist:list, mydict:Dict[str,List[str]])->list:
    """Replace_element in `mylist` with another list as defined in  `mydict`
    Example: `replace_element_in_list_with_another_list(['EU', 'JPN'], {'EU': ['AUT', 'BEL','DEU...']})`
    Returns -> ['AUT', 'BEL', 'DEU...', 'JPN']

    Parameters
    ----------
    mylist : list
        List to be updated.
    mydict : dict
        Dictionary with element to be replaced as key, and replacement list as values

    Returns
    -------
    list
        Updated list
    """    
    for element_to_be_replaced,replacement_list in mydict.items():
        if element_to_be_replaced in mylist:
            index = mylist.index(element_to_be_replaced)
            mylist.pop(index)
            mylist[index:index] = replacement_list

    return mylist

def fun_main_sector_simple(var:str, invert=False)->str:
    """Returns a `main sector` variable associated to `var` based on the pipe "|" separator. 

    Parameters
    ----------
    var : str
        Your variable

    Returns
    -------
    str
        Main variable
    """   

    main= '|'.join(var.split('|')[:-1])
    if not invert:
        return main
    last_one= var.split('|')[-1]
    main= '|'.join(main.split('|')[:-1])
    return f"{main}|{last_one}"

def fun_slope_over_time_iamc_format(df:pd.DataFrame)-> Dict[str, Union[float, Dict[str, Dict[str, float]]]]:
    """
    Calculate the slope over time (df.columns) for IAMC formatted data.

    Parameters:
    - df (pd.DataFrame): Input DataFrame with IAMC formatted data.

    Returns:
    - Dict[str, Union[float, Dict[str, Dict[str, float]]]]:
      Dictionary containing the slopes for each combination of MODEL, SCENARIO, and REGION/ISO.
      If the DataFrame has a single index, returns a dictionary with index keys and slope values (over df.columns).

    Raises:
    - ValueError: If the DataFrame index contains a multindex and does not have the required IAMC format.

    Example:
    ```python
    import pandas as pd
    from scipy.stats import linregress

    df = pd.DataFrame({
        'MODEL': ['Model1', 'Model1', 'Model2', 'Model2'],
        'SCENARIO': ['Scenario1', 'Scenario2', 'Scenario1', 'Scenario2'],
        'REGION': ['Region1', 'Region1', 'Region2', 'Region2'],
        '2000': [1, 2, 3, 4],
        '2005': [2, 3, 4, 5],
        '2010': [3, 4, 5, 6]
    }).set_index(['MODEL', 'SCENARIO', 'REGION'])

    result = fun_slope_over_time_iamc_format(df)
    ```

    This will calculate the slope of the values over the years 2000, 2005, and 2010 for each combination of MODEL, SCENARIO, and REGION.
    """
    if len(df.index.names)==1:
        return {c:linregress(list(df.columns), df.loc[c]).slope for c in df.index}
    fun_check_iamc_index(df)
    dfreset=df.reset_index()
    models=dfreset.MODEL.unique()
    scen_col='SCENARIO' if 'SCENARIO' in dfreset.columns else 'TARGET'
    iso_col='REGION' if 'REGION' in dfreset.columns else 'ISO'
    scenarios=dfreset[scen_col].unique() 
    countrylist=dfreset.REGION.unique() if 'REGION' in dfreset.columns else dfreset.ISO.unique()
    res = {}    
    for m in models:
        res_by_scen={}
        for scen in scenarios:
            res_by_country={}
            for country in countrylist:
                data = df.xs((m,scen,country), level=('MODEL',scen_col,iso_col))
                if len(data):
                    x_values = data.columns.astype(int)
                    y_values = data.values.flatten()
                    slope, _, _, _, _ = linregress(x_values, y_values)
                    res_by_country[country]=slope
            res_by_scen[scen]=res_by_country
        res[m] = res_by_scen
    return res

def find_historic_pattern(df_iea: pd.DataFrame, df: pd.DataFrame, c: str, model:str, scenario:str, sector_mapping:Optional[dict] = None, 
                          time_mapping: Optional[dict]=None) -> dict:
    """
    Find historic pattern based on squared difference between `df` and historical `df_iea` data for selected time
    periods defined in `time_mapping` and a given `main_sector`

    Parameters:
    -----------
    df_iea : pd.DataFrame
        DataFrame containing historical IEA data.
    df : pd.DataFrame
        DataFrame containing downscaled results (with a single scenario/model) in IAMC format.
    c : str
        Country for which the historic pattern is to be analyzed.
    sector_mapping : dict, optional
        Dictionary for normalization, with main sector as key and list of sub-sectors as value. Dictionary should contain a single key , by default None.
    time_mapping : dict, optional
        Mapping of years between the two dataframes, by default {2020:2000,2035:2015}.

    Raises
    ------
    ValueError
        If multiple models/scenarios are found in the `df`
    ValueError
        If inconsistent or negative time intervals are found in `time_mapping`
    ValueError
        If len(set(sector_mapping.keys())) !=1. 
        Reason: we derive a `main_sector` from keys in `sector_mapping` for normalization. We do not allow for multiple main sectors.
    Returns:
    --------
    dict
        Historic pattern of difference between the two dataframes for the specified country.
    """
    
    # Check IAMC index
    fun_check_iamc_index(df)
    # Convert colum strings to integer e.g '2000'->2000
    df_iea=fun_index_names(df_iea, True, int).dropna(how='all', axis=1)
    df2=fun_index_names(df.copy(deep=True), True, int)
    
    # Get default `time_mapping` if None
    time_mapping = fun_get_time_mapping(df_iea, time_mapping)


    # Define `sector_mapping` if None, and get main_sector from `sector_mapping.keys()`
    if sector_mapping is None:
        sector_mapping={"Primary Energy": [
        "Primary Energy|Biomass",
        "Primary Energy|Coal",
        "Primary Energy|Gas",
        "Primary Energy|Geothermal",
        "Primary Energy|Hydro",
        "Primary Energy|Nuclear",
        "Primary Energy|Oil",
        "Primary Energy|Solar",
        "Primary Energy|Wind",]}
    if len(set(sector_mapping.keys()))!=1:
        text1="`len(set(sector_mapping.keys()))` should be equal to one, because we need a single main sector (keys of sector_mapping))"
        text2="NOTE: You can pass a dictionary with {'Primary energy':[list1], 'Primary Energy':[list2]} as long as the keys are always the same (e.g. `Primary Energy`)"
        raise ValueError(f'{text1}. {text2}. You passed a dictionary with these keys: {set(sector_mapping.keys())}')
    
    
    main_sector=list(sector_mapping.keys())[0]

    # Slice, for model, scenario and selected variables
    df2=fun_xs(df2, {'MODEL':model, 'SCENARIO': scenario})
    
    # # Check MODEL and SCENARIO uniqueness
    for x in ['MODEL', 'SCENARIO']:
        check = df2.reset_index()[x].unique()
        if len(check) != 1:
            raise ValueError(f"You should select a single {x}. You have selected: {check}")
    df2= fun_xs(df2, {'VARIABLE':[main_sector]+sector_mapping[main_sector]})
    
    # Drop FILE level if present
    if 'FILE' in df2.index:
        df2 = df2.droplevel('FILE')

    # Create an empty DataFrame to store results and calculate historic pattern for each time mapping
    res_df = pd.DataFrame()
    for k, v in time_mapping.items():
        # Normalize dataframes by main_sector
        dfa=(df_iea/df_iea.xs(main_sector, level='VARIABLE')).dropna(how='all', axis=1)[v].droplevel(['MODEL','SCENARIO','UNIT'])
        dfb=(df2/df2.xs(main_sector, level='VARIABLE'))[k].droplevel(['MODEL','SCENARIO','UNIT'])

        # Calculate difference and sum of squares
        temp = pd.DataFrame({v: (dfa - dfb.xs(c, level='REGION')) ** 2}).dropna(how='all').groupby('REGION').sum()

        # Concatenate results
        res_df = pd.concat([res_df, temp], axis=1, sort=True)

    # Calculate sum of squared differences for each country and sort by values
    return res_df.dropna().sum(axis=1).sort_values().to_dict()

def fun_get_time_mapping(df_iea, time_mapping):
    if time_mapping is None:
        if 2030 in df_iea.dropna(how='all', axis=1).columns:
            # This means we are using a current policy projection to compare feasible transitions (same time period)
            time_mapping={2020:2020, 2035:2035}
            iea_scen=df_iea.reset_index().SCENARIO.unique()
            if len (iea_scen)!=1:
                raise ValueError(F'`df_iea` should contain only one scenario. It contains {iea_scen}')
        else:
            # This means we are using historical data to compare feasible transitions (shifting time periods to 20 years earlier).
            # The time interval considered is 15 years: 2035-2020 = 15  (and 2015-2020=15)
            time_mapping={2020:2000,2035:2015}

    # Check differences in time mapping (we need to use the same time interval (e.g. 20 years between
    # changes in historical `df_iea` (e.g. 2015-2015) and future `df` data (e.g. 2020-2035)):
    check_time_intervals={int(k)-int(v) for k,v in time_mapping.items()}
    if len(check_time_intervals)!=1:
        txt1='We found inconsistent time intervals in `time_mapping`'
        txt2='Please make sure that key-value in `time_mapping` yields always the same time interval'
        raise ValueError(f'{txt1}:{check_time_intervals}. {txt2}')
    check_order={x for x in check_time_intervals if x<0}
    if len(check_order):
        raise ValueError(f'Keys in `time_mapping` should be always greater than values e.g. `2020:2000`. you passed {time_mapping}')
    return time_mapping

def find_and_show_patterns(df:pd.DataFrame, scen:str, model:str, c:str, sector_mapping:dict, ref_scen:str='Historic data', max_countries:int=3)->plt:
    """Finds and show plot of similar patterns across countries.

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe with downscaled results. NOTE: if `ref_scen=='Historical data'` and `ref_scen` is not present in the `df`, it  will read and append historical iea data to the df)
    scen : str
        Your selected scenario for country `c` e.g. 'o_1p5c'
    model : str
        Selected model for country `c`, e.g. 'MESSAGE'
    c : str
        Selected country, e.g. 'AUT'
    sector_mapping : dict
        Dictionary with main sector as key and a list of sub-sectors as values e.g. {'Primary Energy':['Primary Energy|Oil', 'Primary Energy|Gas', ...]}
    ref_scen : str, optional
        Reference scenario to search for similar patterns observed in other countries e.g. (`h_cpol`), by default 'Historic data'
    max_countries : int, optional
        Max number of countries with similar patterns considered, by default 3

    Raises
    ------
    ValueError
        If cannot find selected `scen` or `ref_scen` in the `df`.
    
    Returns:
    --------
    plt
        Plot showing similar patterns
    """    
    df=df.copy()
    df=fun_xs(df, {'MODEL':model})
    if ref_scen =='Historic data':
        df = add_most_recent_iea_data_if_missing(df)
        df_ref=fun_xs(df, {'SCENARIO':'Historic data'})
        group_by_time=False
    else:
        df_ref=fun_xs(df, {'SCENARIO':ref_scen})
        group_by_time=True
    for i in [ref_scen, scen]:
        if i not in df.reset_index().SCENARIO.unique():
            raise ValueError(f'Cannot find {i} scenario in `df`')
    # res_single=find_historic_pattern(fun_xs(df_ref, {'MODEL':'IEA'}), fun_xs(df, {'MODEL':model, 'SCENARIO':scen}), c, model, scen,sector_mapping=sector_mapping)
    time_mapping = fun_get_time_mapping(fun_xs(df, {'SCENARIO':ref_scen}), None) 
    # NOTE: all keys/values should have the same time interval, e.g. 15 yeas: example (2020,2035) for keys and (2000,2015) for values
    res_single=find_historic_pattern(df_ref, fun_xs(df, {'MODEL':model, 'SCENARIO':scen}), c, model, 
                                     scen,sector_mapping=sector_mapping, time_mapping=time_mapping)
    c_list=list(res_single.keys())[:max_countries]
    # for i in [True, False]:
    fig = fun_show_energy_transition(model, df, {ref_scen:c_list, scen:c}, sector_mapping, group_by_time=group_by_time,
                                     default_time_hist=list(time_mapping.values()),
                                     default_time_future=list(time_mapping.keys()),
                                     )
    return fig
# scen_dict={
#     'Historic data':sel_countries, 
#    'h_cpol':c}
def fun_show_energy_transition(model:str, df:pd.DataFrame, scen_dict:dict, sector_mapping:dict, group_by_time:bool=False,     
    default_time_hist:list=[2000,2015],
    default_time_future:list=[2020,2035],
    title:Optional[str]=None,
    )->plt:
    """Plot energy transition across countries and scenario
    , pd.concat([df_country,df_iea]), , step2_primary_energy, group_by_time=True
    Parameters
    ----------
    model : str
        Your model e.g. "MESSAGEix-GLOBIOM 1.1-M-R12" 
    df : pd.DataFrame
        Your dataframe
    scen_dict : dict
        Dictionary with scenarios as keys, and list of countries as values e.g. {'Historic data':['ITA', 'DEU','FRA'], 'h_cpol':'myISO'}
    sector_mapping : dict
        Dictionary with main sector as key, and list of sub-sectors as values g  e.g. {'Primary Energy':['Primary Energy|Coal',...]}
    group_by_time : bool, optional
        Whether your plot should be grouped by TIME (otherwise will group by REGION), by default False
    default_time_hist : list, optional
        Selected time to show transition in the 'Historic data', by default [2000,2015]
    default_time_future : list, optional
        Selected time to show transition all the other (future) scenarios (e.g. `h_cpol`), by default [2020,2035]

    Returns
    -------
    plt
        Plot with energy transition
    """    

    # TODO: refine legends and order of countries
    if 'Historic data' in list(scen_dict.keys()):
        df = add_most_recent_iea_data_if_missing(df)
    
    if 'FILE' in df.index.names:
        df=df.droplevel('FILE')

    res={}
    for k,v in scen_dict.items():
        res[k]=fun_xs(df, {'SCENARIO':v, 'MODEL':model}).dropna(how='all', axis=1)
    
    main_sector=list(sector_mapping.keys())[0]
   
    time_dict={x:default_time_hist if x=='Historic data' else default_time_future for x in scen_dict }
    drop_list=['MODEL','SCENARIO','UNIT']
    res={}
    for scen,country in scen_dict.items():
        temp=fun_xs(df, {'REGION':v, 'SCENARIO':scen,'REGION':country})
        temp=(temp/temp.xs(main_sector, level='VARIABLE')).droplevel(drop_list)[time_dict[scen]]
        temp=fun_xs(temp,{'VARIABLE':list(sector_mapping.values())[0]}).dropna(how='all', axis=1)
        res[scen]=temp.rename({x:f"{scen} - {x}" for x in temp.reset_index().REGION.unique()}) # rename country c 

    dfs3=pd.concat(list(res.values()))
    dfs3 = dfs3.rename({x:x.split('|')[-1] for x in dfs3.reset_index().VARIABLE.unique()})

    # Create graphs by grouping by TIME (otherwise group by REGION)
    what='TIME'
    if not group_by_time:
        what='REGION'
    else:
        # Below groups by time periods
        dfs3=dfs3.stack().unstack('REGION')
        dfs3.index.names=['VARIABLE','TIME']
    no_sub_plots=len(dfs3.reset_index()[what].unique())
    # fig, axes = plt.subplots(1, no_sub_plots,figsize=(12,4))
    fig, axes = plt.subplots(1, no_sub_plots,figsize=(12,6))
    fig.subplots_adjust(top=0.4)
    count=0
    for (k,d), ax in zip(dfs3.groupby(what), axes.flat):
        d.droplevel(what).dropna(how='all', axis=1).T.plot.bar(stacked=True, ax=ax, title=k, sharey=True, ylim=[0,1], color=d.reset_index()['VARIABLE'].map(colors))
        if count==0:
            ax.get_legend().remove()
        else:
            plt.legend(bbox_to_anchor=(1, 0.5))
        ax.set_ylabel(f'Share in {main_sector}')
        ax.set_xlabel('')
        count+=1
    if title is None:
        title=model
    fig.suptitle(title, y=1.0)
    fig.tight_layout()

    return plt

def add_most_recent_iea_data_if_missing(df:pd.DataFrame)->pd.DataFrame:
    """Add IEA data if missing in the `df`.
    Parameters
    ----------
    df : pd.DataFrame
        Dataframe with downscaled results in IAMc format

    Returns
    -------
    pd.DataFrame
        Dataframe with IEA data.
    """    
    if 'IEA' not in df.reset_index().MODEL.unique():
        df_iea=fun_index_names(fun_most_recent_iea_data(), True, int).rename({'Reference':'IEA'})
        if 'FILE' in df.index.names:
            df=df.droplevel('FILE')
        df=pd.concat([df_iea, df])
    return df
    # plt.tight_layout()


def group_keys_based_on_similar_values(mydict:dict)->dict:
    """Group keys in a dictionary based on their values

    Parameters
    ----------
    mydict : dict
        Your dictionary

    Returns
    -------
    dict
        Your dictionary (with grouped keys bases on values they have in common)
    """    
    res={}
    mydict={k:v if isinstance(v, list) else [v] for k,v in mydict.items()}
    d=find_best_key_match_by_common_values(mydict)
    # already_taken=[]
    # count=1
    for k,v in d.items():
        # if k not in already_taken:
        if len(v)>0:
            res[k]=list(set(list(set(fun_flatten_list([mydict[v[0]]]+[v])))+[k]))
        else:
            print('do something')
    return res




def from_IAMC_long_to_wide_format(df:pd.DataFrame)-> pd.DataFrame:
    """Convert IAMC dataframe from long (all time periods in a single column) to wide format (all time periods in different columns).

    Parameters
    ----------
    df : pd.DataFrame
        Your original dataframe in long IAMC format. 
        It should contain these columns: ['model', 'scenario', 'country', 'variable', 'unit', 'year', 'value']

    Returns
    -------
    pd.DataFrame
        Dataframe in wide IAMC format
    """    
    if len(df.index.names)>1 and df.index.names[0]==None:
        df=df.reset_index()
    if 'index' in df.index.names:
        df=df.drop('index')
    df.columns=[x.upper() for x in df.columns]
    idx=['MODEL','SCENARIO','ISO','VARIABLE','UNIT','TIME']
    return df.rename({'COUNTRY':'ISO', 'YEAR':'TIME'}, axis=1).set_index(idx).unstack('TIME')['VALUE']


def check_missing_iea_countries(df:pd.DataFrame, 
                                small_iea_countries_excluded:list=["ANT","GIB"],
                                variables:Optional[list]=[["GDP|PPP", "Population"], "GDP|PPP", "Population"]):
    """Check if IEA countries are missing in the dataframe `df` for a list of `variables` and returns dictionary with missing data

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe in IAMC format
    small_iea_countries_excluded : list, optional
        List of small IEA countries that should be excluded from check, by default ["ANT","GIB"]
    variables: Optional[list]
        List of variables to be checked, by default None (in this case we first we check if either
        pop or gdp is missing. Then we chek individually for GDP/pop)
    Returns
    ------
    dict
        Dictionary with missing data
    """    
    if variables is None:
        variables=["GDP|PPP", "Population", ["GDP|PPP", "Population"]]
        # NOTE: We check if GDP or pop are missing (or both)
    res={}
    for v in variables:
        av_countries=fun_xs(df, {"VARIABLE":v}).reset_index().ISO.unique()
        missing=[x for x in iea_countries if x not in av_countries]
        missing=[x for x in missing if x not in small_iea_countries_excluded]
        if len(missing)>0:
            for c in missing:
                res[c]=v
            # return {c:[x for x in v] for c in missing}
            # raise ValueError(f"{v} data are missing for {missing} in `df`")
        return res


def fun_check_missing_data(df:pd.DataFrame, level1:str='REGION', level2:str='SCENARIO')->dict:
    """Check missing data in your `df`. Typically `df` should contain only one variable, 
    e.g. `Final Energy`, especially for downscaled results. For IAMs results, it can contain all variables.
    Returns a dictionary (e.g. if `h_ndc` is present for some regions but not for others).

    Parameters
    ----------
    df : pd.DataFrame
        Your dataframe in IAMC format
    level1 : str, optional
        First index level to be checked in your dataframe, by default 'REGION'
    level2 : str, optional
        Second index level to be checked in your dataframe, by default 'SCENARIO'

    Returns
    -------
    dict
        Dictioanary with missing data
    """    
    res=fun_dict_level1_vs_list_level2(df, level1, level2)
    available=df.reset_index()[level2].unique()
    res2={k:set(v)^set(available) for k,v in res.items()}

    res3={k:v for k,v in res2.items() if len(v)>0}
    res4=fun_invert_dictionary(res3)
    if len(res4):
        print(f"Missing {level2}: {res4}")
    return res4


def fun_divide_variable_by_another(df: pd.DataFrame,var_name, numerator: str, denominator: str, operator='/', level='VARIABLE', unit:str=None, concatenate:bool=False) -> pd.DataFrame:
    """Perform an arithmetic operation (addition, subtraction, multiplication, division, power, etc.) 
    between two variables in a DataFrame.

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame in IAMC format.
    var_name: str
        Name for the resulting variable after performing the operation.
    numerator : str
        The variable to be used as the numerator (or the `main` variable) in the operation.
    denominator : str
        The variable to be used as the denominator (or the `other` variable) in the operation.
    operator:str
        The arithmetic operation to be performed.
        Allowed operators are: '+', '-', '*', '**', '/', '//', '%'. By default '/'.
    level : str, optional
        The level in the DataFrame's index where the variables are located, by default 'VARIABLE'
    unit: str, optional
        The unit or your calculated variable. If None, will try to provide it automatically, by default None
    concatenate: bool
        Wheter your want to concatenate the results of your operation with the existing `df`
    Returns
    -------
    pd.DataFrame
        DataFrame with the results of the operation.
    """
    # Check if chosen `operator` is allowed
    ALLOWED_OPERATORS = {
        "+": "add",
        "-": "sub",
        "*": "mul",
        "**": "pow",
        "/": "div",
        "//": "floordiv",
        "%": "mod",
    }
    if operator not in ALLOWED_OPERATORS:
        raise ValueError(
            f"Only Arithmetic Operators are allowed: {ALLOWED_OPERATORS}. You provided: {operator} "
        )

    # Check for a valid `var_name` if results will be concatenated to the existing `df`
    if concatenate:
        if var_name in df.reset_index()[level]:
            txt="Please select another variable name or select `concatenate=False` to suppress this error."
            raise ValueError(f"Your selected `var_name`{var_name} is already present in the `df`. {txt}")
    
    # Extract the numerator and denominator data
    df_numerator = df.xs(numerator, level=level)
    df_denominator = df.xs(denominator, level=level)

    # Perform the operation
    if 'UNIT' in df.index.names:
        res_unit={}
        for x in ['df_numerator', 'df_denominator']:
            res_unit[x]=eval(x).reset_index().UNIT.unique()
            if len(res_unit[x])!=1:
                raise ValueError(f'We fuond multiple units in {x}: {res_unit[x]}. Please check your dataframe')
        df_result =getattr(df_numerator.droplevel('UNIT'), ALLOWED_OPERATORS[operator]
                           )(df_denominator.droplevel('UNIT'))
    else:
        df_result =getattr(df_numerator, ALLOWED_OPERATORS[operator])(df_denominator)
    
    # Rename Variable and set_index
    df_result=df_result.assign(my_level=var_name)
    df_result=fun_index_names(df_result.reset_index())
    df_result=fun_rename_index_name(df_result, {'my_level':level})

    # Add the unit
    if 'UNIT' in df.index.names:
        df_result = fun_add_unit_when_performing_operations(operator, unit, res_unit, df_result)

    df_result=df_result.reset_index().set_index(df.index.names)
    return pd.concat([df, df_result]) if concatenate else df_result

def fun_add_unit_when_performing_operations(operator:str, unit:Optional[str], res_unit:dict, df_result:pd.DataFrame)->pd.DataFrame:
    """Adds or determines the unit of measurement when performing arithmetic operations on a DataFrame.

    Parameters
    ----------
    operator : str
        Your selected operation
    unit : Optional[str]
        Your unit. If None will try to automatically detect it based on the `res_unit`
    res_unit : dict
        Dictionary containing the units for the `df_numerator` (the main variable for your selected operation) and `df_denominator` (the other variable for your selected operations).
    df_result : pd.DataFrame
        The DataFrame with the result of the arithmetic operation 

    Returns
    -------
    pd.DataFrame
        Dataframe with the unit

    Raises
    ------
    ValueError
        When `unit` is `None`, if the `operator` is not recognized.
    ValueError
        When `unit` is `None`, For the '+' and '-' operators, if the units in `res_unit.values()` are different 
        (This means that you are trying to sum or subtract variables with different units)
    """    
    if unit is None:
        txt='please pass a `unit` to suppress this error (you passed `None`) '
        if operator not in  ['+','-','*','/']:
            raise ValueError(f'Unable to detect unit for the {operator} operator. {txt}')
        else:
            if operator in ['+','-']:
                unit=set(fun_flatten_list(list(res_unit.values())))
                if len(unit)!=1:
                    txt0='We found different units in your numerator and denominator'
                    txt1=f"Unable to perform the {operator} operation"
                        
                    raise ValueError(f'{txt0}: {res_unit}. {txt1}. Or {txt} ')
            else:
                space=' ' if operator == '*' else ' / '
                unit=f"{res_unit['df_numerator'][0]}{space}{res_unit['df_denominator'][0]}"

    df_result=df_result.assign(UNIT=unit)
    return df_result

def fun_harmonize_hist_data_by_preserving_sum_across_countries(df:pd.DataFrame, df_iea:pd.DataFrame, var:str, baseyear:int=2020, tc:int=2100)->pd.DataFrame:
    """
    Align `df` with the historical data `df_iea`, while preserving the same sum across countries as before.
    Hence, this is a `soft` harmonization (discrepancy with historical data are possible). For a `hard` harmonization please refer to `step5e`.

    This function adjusts your `df` to replicate historical data until the `baseyear` (by shifting the data upward/downward), 
    and ensures that the sum of country-level results remains the one as before (e.g. consistent with the regional IAMs results).
    For trade variable (with positive or negative values), this function preserves the importing/exporting status of countries, at the base year.
    Finally it  blends the results using a time of convergence `tc`: from the updated results at the base year (largely driven by the historical data shift),  
    to the original (previous) results (usually largely driven by IAM results). By varying `tc` we get a range of results.
    With a fast convergence (e.g tc=2030) importing countries could potentially become exporter soon (and viceversa)

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame containing data (e.g. trade) to be harmonized. 
        It should contain a data from a single model and countries from a single region.
        It can contain multiple scenarios.
    df_iea : pd.DataFrame
        DataFrame containing historical IEA data.
    var : str
        Variable to be harmonized.
    baseyear: int, optional
        The base year up to which historical trade data is replicated (default is 2020).
    tc: int, optional:
        Time of convergence to blend the results, (default is 2100)
    Returns
    -------
    pd.DataFrame
        Harmonized DataFrame with `var`, adjusted to match historical 2020 data while preserving the same sum across countries as before.

    Notes
    -----
    The function performs the following steps:
    1. Adjusts data to replicate 2020 historical trade data (by shifting the data upward/downward).
    2. Adjusts data to preserve the same sum across countries as before, attempting to maintain the export/import status of countries 
       (only importing or exporting countries will adjsust to match IAM results).
    3. Blends the updated data (shifted to match historical data) and the previous results using a time of convergence `tc`.

    Backup (simpler) or alternative functions  can be found here: https://github.com/iiasa/downscaler_repo/issues/181
    """
    
    # Check dataframes
    hist_harmo_checks(df, df_iea)

    # Prepare the data
    df_iea=df_iea.copy()
    df=df.copy()
    df_iea.columns=pd.to_numeric(df_iea.columns)
    df.columns= pd.to_numeric(df.columns)
    trade=fun_xs(df, {'VARIABLE':var})
    iea_adj=df_iea.xs(var,level="VARIABLE").loc[:,:baseyear]
    iea_adj=iea_adj.droplevel(["SCENARIO","MODEL","UNIT"])
    
    # 1 Match hist data. However this creates a mismatch in the sum across countries
    trade_match_hist = fun_match_hist(baseyear, trade, iea_adj)
    
    # 2 Restore sum of country level results (we want the same as we had before)
    trade_all = fun_restore_sum_of_country_level_data_as_before(trade, trade_match_hist)
    if len(trade_match_hist.dropna(how="all"))==0:
        # If we cannot match historical data (df is empty) then we return previous `trade` results
        return trade
    # 3. Use weighted average (with convergence over time in `tc`)
    w=fun_discount_rate(list(range(2010,2505,5)), list(range(2005,2025,5)),tc,1,0)

    return (trade_all.fillna(0)*w+trade.fillna(0)*(1-w)).dropna(how='all', axis=1)

def fun_restore_sum_of_country_level_data_as_before(df_before:pd.DataFrame, df_after:pd.DataFrame)->pd.DataFrame:
    """
    Restore the sum of country-level data as it was before any changes 
    (by preserving signs - e.g. importring/exporting countries). It can be also used to harmonize
    `df_after` with regional IAMs results (if `df_before` contains regional IAMs results (from a single model/region) )

    Parameters
    ----------
    df_before : pd.DataFrame
        DataFrame representing data before any changes (or regional IAMs results from a single model/region)
    df_after : pd.DataFrame
        DataFrame representing data after changes (to be harmonized in line with `df_before`).

    Returns
    -------
    pd.DataFrame
        DataFrame with country-level data restored to its previous sum (in line with regional IAMs results).

    Notes
    -----
    This function calculates the difference between dataframes `df_after` and `df_before`,
    then restores the sum of country-level data as it was before any changes, ensuring
    that the sum across countries remains consistent.

    It first groups `df_before` by all levels except 'ISO' and 'REGION', then updates
    the dataframe based on the calculated delta and the trade status. Finally, it asserts
    that the sum across countries is restored before returning the results.

    This function can be used to harmonize data with regional IAMs results, like the example below:
    ```
    # Example 3 - This one is not working (test that we do not get division by zero error):
    iam=fun_clip_df_by_dict(fun_create_sample_df(countrylist=['MY BIG REGION']), {'MY BIG REGION':1}, 'REGION', True, True)[range(2010,2030,5)]
    mydf=fun_clip_df_by_dict(fun_create_sample_df(countrylist=["country1","country2"]), {'country1':1, 'country2':-1}, 'REGION', True, True)[range(2010,2030,5)]
    fun_restore_sum_of_country_level_data_as_before(iam, mydf)
    ```
    """
    if len(df_after.dropna(how="all"))==0:
        return df_before
    df_before=df_before.copy(deep=True)
    df_after=df_after.copy(deep=True)

    # Fill missing data with zeroes
    df_before=df_before.fillna(0)
    df_after=df_after.fillna(0)

    # Get correct sum of country level results (from `df_before`)
    group_levels=[x for x in df_before.index.names if x not in ['ISO', 'REGION']]
    iam_res=df_before.groupby(group_levels).sum().copy(deep=True)

    regions=None
    for x in ["ISO", "REGION"]:
        if x in df_before.index.names:
            regions=df_before.reset_index()[x].unique()
            break
    if regions is None:
        txt="`df_before` should contain `ISO` or `REGION` in its index"
        raise ValueError(f"{txt}. It contains:{df_before.index.names} ")
    
    # if df_before contains one region, we create allocate the sum across countries, proportionallt
    if len(regions)==1:
        diff=(iam_res-df_after.groupby(group_levels).sum())
        to_be_allo=(np.abs(df_after)/np.abs(df_after).groupby(group_levels).sum())*diff
        to_be_allo=to_be_allo.reset_index().set_index(df_after.index.names)
        df_before=df_after+to_be_allo
    
    # Calculate delta (after-before)
    delta=df_after-df_before
    
    # Update dataframe (by keeping import/export status if trade variables are present)
    res = fun_harmo_by_keeping_trade_status(iam_res, delta, df_after)
    
    # Check if we have successfully restored the sum across countries before returning the results:
    common_cols=list(set(df_before.columns)&(set(res.columns)))
    try:
        assert_frame_equal(iam_res[common_cols], res.groupby(group_levels).sum()[common_cols], 
                        check_like=True,
                        check_dtype=False
                        )
    except:
        # Block below is useful when running in debug-mode
        fun_harmo_by_keeping_trade_status(iam_res, delta.fillna(0), df_after)
        assert_frame_equal(iam_res[common_cols], res.groupby(group_levels).sum()[common_cols], 
                        check_like=True,
                        check_dtype=False
                        )

    return res

def fun_harmo_by_keeping_trade_status(iam_res:pd.DataFrame, delta:pd.DataFrame, df:pd.DataFrame):
    """
    Harmonize `df` data to match `iam_res` by maintaining the trade status of countries.

    This function harmonizes (e.g. trade) data to match IAM results while preserving the import/export status of countries.
    It ensures the adjustments preserve the overall trade balance and do not result in extreme discrepancies.

    Parameters
    ----------
    iam_res : pd.DataFrame
        IAM results dataframe, containing aggregated data at a regional level.
    delta : pd.DataFrame
        Dataframe containing the differences (deltas) to be applied to the trade data for harmonization.
    df : pd.DataFrame
        The original trade data at the country level.

    Returns
    -------
    pd.DataFrame
        Harmonized trade data, preserving the sum of country-level results and maintaining trade status.

    Notes
    -----
    This function performs the following steps:
    1. Calculates the regional mismatch to be allocated, to restore the sum across countries.
    2. Calculates country-level delta to maintain the export/import status of countries, depending on the regional mismatch.
    3. Adjusts trade data to preserve the sum of country-level results, aligning with IAM results.
    """
    # Determine the grouping levels, excluding 'ISO' and 'REGION'
    group_levels=[x for x in df.index.names if x not in ['ISO', 'REGION']]

    # Calculate regional mismatch (to be allocated, to restore sum across countries)
    reg_mismatch=iam_res-df.groupby(group_levels).sum()
    scenarios=list(df.reset_index().SCENARIO.unique())
    # Calculates country-level delta to maintain the export/import status of countries (depending on sign of `res_mismatch_2020`) if the imbalance (`check`) is not too extreme
    delta_all = pd.concat([calc_delta_all(fun_xs(delta, {"SCENARIO":s}), 
                                          fun_xs(df, {"SCENARIO":s}), 
                                          fun_xs(reg_mismatch, {"SCENARIO":s}))
                           for s in scenarios])
    
    # Adjust trade data to preserve sum of country-level results (same as in `iam_res`)
    trade_all = fun_keep_sum_across_countries(iam_res, df, delta_all)
    return trade_all

def hist_harmo_checks(df, df_iea):
    """
    Check if the data `df` and `df_iea` are in IAMC format and contain only one model.
    `df` should contain coutries that belong only to one IAM region and asks for confirmation if more that 100 countries are found.

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame containing trade data at the country level.
    df_iea : pd.DataFrame
        DataFrame containing the IEA historical data for comparison.

    Raises
    ------
    ValueError
        If `df` contains more than one model or if the user does not confirm that all countries belong to a single IAM region.
    """
    fun_check_iamc_index(df_iea)
    fun_check_iamc_index(df)
    models=df.reset_index().MODEL.unique()
    regions=df.reset_index().REGION.unique()
    if len(models)>1:
        raise ValueError(f"`df` should contain only one model. It contains {len(models)} models: {models}")
    if len(regions)>100:
        txt=f"`df` should contain countries only from 1 region. It contains {len(regions)} countries."
        txt2="Please confirm all these countries belong to a single IAM region y/n "
        print(txt)
        action=input(txt2)
        if action.lower() not in ["yes", "y"]:
            raise ValueError("Simulation aborted by the user")

def fun_keep_sum_across_countries(iam_res:pd.DataFrame, df:pd.DataFrame, delta_all:pd.DataFrame)->pd.DataFrame:
    """
    Adjust `df` data to preserve the overall sum across countries in line with IAMs model results `iam_res`.

    This function harmonizes the data by ensuring that the adjustments maintain the overall sum of country-level results
    consistent with the Integrated Assessment Model (IAM) results. It does so by distributing the regional mismatch proportionally
    across countries based on their shares.

    Parameters
    ----------
    iam_res : pd.DataFrame
        DataFrame containing the IAM results with regional sums.
    df : pd.DataFrame
        Input DataFrame containing trade data at the country level.
    delta_all : pd.DataFrame
        DataFrame containing the delta adjustments for each country and time period, 
        by keeping imbalances between positive and negative values (e.g. trade data), and
        ensuring consistency with historical data. This is usually calculated using the function `calc_delta_all`

    Returns
    -------
    pd.DataFrame
        DataFrame with adjusted trade data that preserves the overall sum across countries.
    """
    # Harmonize data
    delta_all=delta_all.fillna(0)
    reg_mismatch=iam_res-df.groupby("SCENARIO").sum()
    # try:
    shares=(delta_all/delta_all.groupby("SCENARIO").sum())
    # except:
    #     raise ValueError("shares not working")
    if  len(shares.index.names)==1:
        delta_reg=fun_add_multiply_dfmultindex_by_dfsingleindex(shares, reg_mismatch, operator='*')
    else:
        delta_reg=reg_mismatch*shares
        delta_reg=delta_reg.reset_index().set_index(df.index.names)
    return delta_reg.fillna(0) + df

def calc_delta_all(delta:pd.DataFrame, trade_2020:pd.DataFrame, reg_mismatch_2020:pd.DataFrame, extreme_perc:float=0.05, base_year=2020)->pd.DataFrame:
    """
    Calculate adjusted delta values to handle imbalances in trade data, ensuring consistency with historical data.

    Parameters
    ----------
    delta : pd.DataFrame
        DataFrame containing the initial delta values calculated to adjust the trade data.
    trade_2020 : pd.DataFrame
        DataFrame containing the trade data adjusted to replicate historical trade data up to the base year (e.g. 2020).
    reg_mismatch_2020 : pd.DataFrame
        DataFrame containing the regional mismatches calculated from the adjusted trade data.
    extreme_perc : float, optional
        The threshold percentage for determining extreme imbalances between importing and exporting countries, by default to 0.05.
        If extreme imbalances are found, we will not try to keep the importing/exporting status (e.g., we don't want a small country 
        to suddenly become a large importer/exporter due to unexpected status changes in larger countries)
    base_year: int, optional
        Base year  in your dataframe, by default 2020

    Returns
    -------
    pd.DataFrame
        Adjusted delta values to handle imbalances in trade data across time periods.

    Notes
    -----
    The function performs the following steps:
    1. Iterates over each time period (column) in the delta DataFrame.
    2. Checks if the trade data contains both positive and negative values, indicating the presence of both importing and exporting countries.
    3. Calculates the imbalance between importing and exporting countries and adjusts the delta values accordingly.
       (e.g. if `reg_mismatch_2020` is negative, only exporting countries (the ones with negative trade values) will adjust to match regional data).
    4. Ensures that the adjustments maintain the overall (regional) trade balance and prevent extreme discrepancies between importing and exporting countries
    5. Concatenates the adjusted delta values across all time periods to form the final adjusted delta DataFrame.
    """
    if len(delta.dropna(how="all"))==0:
        return delta
    delta_all=pd.DataFrame()
    for t in delta.columns:
        is_hist_year= t in [int(x) for x in delta.columns if int(x)<=base_year]
        if len(trade_2020[t].dropna(how="all"))>0:
            # This means we have data in `trade_2020` (this is not an empty dataframe)
            try:
                if is_hist_year:
                    # We check the `flag` only for historical years. Then we continue to use the same 
                    # `flag` in subsequent years to avoid possible structural breaks (unexpected drops/spikes)
                    # in the trade results
                    flag=np.sign(trade_2020[t].min()*trade_2020[t].max())==-1

                if flag:
                    # This mean we have data that can be positive or negative (e.g Trade data)
                    delta_sel=delta[[t]].dropna(0)
                    
                    # Check for extreme imbalance between importing/exporting countries
                    exp=np.abs(delta_sel[delta_sel>=0].sum()[t])
                    imp=np.abs(delta_sel[delta_sel<=0].sum()[t])
                    if exp+imp==0:
                        check=0.5
                    else:
                        check=exp/(exp+imp) 
                    
                    if extreme_perc < check < (1-extreme_perc):
                        if reg_mismatch_2020[t].median()<0:
                            delta_sel=delta_sel[delta_sel<=0][[t]].fillna(0)
                        else:
                            delta_sel=delta_sel[delta_sel>=0][[t]].fillna(0)
                        if not len(delta_sel):
                            delta_sel=delta[[t]]
                    else:
                        # NOTE: Too much discrepancy between importing/exporting:
                        # we will not try to keep the importing/exporting status
                        # (e.g., we don't want a small country to suddenly become a large importer/exporter due to unexpected status changes in larger countries)
                        delta_sel=delta[[t]]
                    delta_all=pd.concat([delta_all, delta_sel], axis=1)          
                else:
                    delta_all=pd.concat([delta_all, delta[[t]]], axis=1)
            except:
                raise ValueError(f"delta not workig for {trade_2020[t]}")
        else:
            # We concat a pd.Dataseries() with 0 values (in time [t])
            delta_sel=trade_2020[t].clip(0,0)
            delta_all=pd.concat([delta_all, delta_sel], axis=1)
            # print("Na values")
    return delta_all

def fun_match_hist(baseyear:int, trade:pd.DataFrame, iea_adj:pd.DataFrame)->pd.DataFrame:
    """
    Adjust `trade` data to replicate historical `iea_adj` data up to a specified `baseyear` 
    and extend the base-year adjustment to future years.

    Parameters
    ----------
    baseyear : int
        The base year up to which historical data is to be replicated (e.g. 2020).
    trade : pd.DataFrame
        DataFrame containing the data to be adjusted (e.g. trade data).
    iea_adj : pd.DataFrame
        DataFrame containing the historical IEA data used for adjustment.

    Returns
    -------
    pd.DataFrame
        Adjusted DataFrame with trade data that replicates historical data up to the base year and extends adjustments to future years.

    Notes
    -----
    The function performs the following steps:
    1. Calculates the delta needed to replicate historical trade data up to the base year.
    2. Extends the calculated delta to future years (up to 2105 in increments of 5 years).
    3. Adjusts the original trade data using the calculated delta.
    """
    # Calculate delta to replicate historical trade data
    delta=fun_add_multiply_dfmultindex_by_dfsingleindex(-trade.loc[:,:baseyear], iea_adj, operator='+')
    delta.columns=pd.to_numeric(delta.columns)

    # Calculate historical trade data until base year (e.g 2020)
    add=pd.concat([delta[[baseyear]].rename({baseyear:x}, axis=1) for x in range(2025,2105,5)], axis=1)
    delta=pd.concat([delta, add], axis=1)
    
    # Adjust trade data
    trade_2020=trade+delta
    return trade_2020



def search(df:pd.DataFrame, val:Union[str, float, int], search_all:bool=False)->list:
    """
    Search for a value in a DataFrame and return matching rows or a summary.

    Parameters
    ----------
    df : pd.DataFrame
        The DataFrame to search in.
    val : str or number
        The value to search for in the DataFrame.
    search_all : bool, optional
        If True, search all columns; otherwise, search index columns, by default False.

    Returns
    -------
    list
        A DataFrame with matching values or a summary dictionary (which can be used as input in `fun_xs` function).
    """
    d=summarize_dataframe(df,search_all)
    mylist=fun_flatten_list(list(d.values()))
    val=str(val)
    # Check if the the wildcard `*` is present in `val`. This means we want to find any string that contain `val`
    flag=False
    if '*' in val:
        val=val.replace('*','')
        flag=True 

    # Try to search for exact `val`, unless we don't find it, or wildcard `*` present in `val`-> Flag =True
    res=[i for i in mylist if str(val).upper() == str(i).upper()]
    if len(res)==0 or flag:
        res= [i for i in mylist if str(val).upper() in str(i).upper()]
    # if len(res)>10:
    #     return res
    if set(res)==set(mylist):
        return df
    
    reslist=[df.eq(x).replace(False, np.nan).dropna(how='all').dropna(axis=1).replace(1,x) for x in res]
    # reslist=[df.where(df.eq(1)).dropna(axis=0) for x in res]
    resdf=pd.DataFrame()
    if len(reslist):
        #raise ValueError(f'Cannot find {val} in the dataframe')
        resdf=pd.concat(reslist, sort=True)
    if len(resdf)>0:
        return resdf
    mynewlist=[SliceableDict(fun_invert_dictionary(d)).slice(x) for x in res]
    mynewlist=[{k:str(v[0]) for k,v in d.items() } for d in mynewlist]
    # try:
    #     res = sum_multiple_dictionaries(mynewlist, only_common_keys=False)
    # except:
    res={k: [v] for d in mynewlist for k, v in d.items()}
    return fun_invert_dictionary(res)

def fun_xs_fuzzy(df:pd.DataFrame, val_to_search:Union[str, float, int], _and:bool=True)->pd.DataFrame:
    """
    Search for fuzzy matches of values in a DataFrame and return the matching rows/cols.
    It also converts all columns into integeres, if df is in IAMc format

    Parameters
    ----------
    df : pd.DataFrame
        The DataFrame to search in.
    val_to_search : str or list of str
        A single value or a list of values to search for in the DataFrame.

    Returns
    -------
    pd.DataFrame
        A DataFrame with rows that match the fuzzy search criteria.

    Raises
    ------
    ValueError
        If `val_to_search` is empty.
    """
    df=df.copy(deep=True)
    if not val_to_search or len(str(val_to_search))==0:
        raise ValueError(f'`val_to_search` is empty. Please provide a string/number to search in the `df`')
    if not isinstance(val_to_search, list):
        val_to_search=[val_to_search]
    
    # We just search for index.names
    d=summarize_dataframe(df,False) 
    
    # Step1  Search for exact match (elements in `val_to_search` that matches whole words in the dataframe) -> Exact results x== string
    exact_dict={x:v for x,v in fun_invert_dictionary(d).items() if  x.lower() in [i.lower() for i in val_to_search if isinstance(i, str)]}
    if len(exact_dict)>0:
        df=fun_xs(df, fun_invert_dictionary(exact_dict))
        # We search again for index.names now that the dataset is smaller
        d=summarize_dataframe(df,False)
        # We exclude from the search all the elements that we have already found with an exact match:
        val_to_search=[x  for x in val_to_search if x.lower() not in  [i.lower() for i in exact_dict]]

    # Step2  Search for approximate match (elements in `val_to_search` that are contained in larger strings in the dataframe) e.g. `GDP` is contained in `GDP|PPP`
    mydict={}
    for i in val_to_search:
        temp_dict= createmydict([i], d, exact=False) # Approximate result x in string
        temp_dict=fun_sort_dict_by_value_length(temp_dict, reverse=True)
        mykey=0
        if len(temp_dict):
            temp_dict2=SliceableDict(temp_dict).slice(list(temp_dict.keys())[mykey])
            # We take the df.index with largest amount of matches, unless this is index (e.g. 'VARIABLE') is already present in mydict. Otherwise, we take the next index, e.g. 'UNIT'
            while list(temp_dict2.keys())[0] in mydict.keys():
                if mykey==(len(temp_dict)-1):
                    break
                mykey+=1
                temp_dict2=SliceableDict(temp_dict).slice(list(temp_dict.keys())[mykey])

            mydict.update(temp_dict2)

    mydict={k:v for k,v in mydict.items() if set(v)!=set(d[k])}
    if len(mydict)>0:
        if _and:
            df_all=pd.DataFrame()
            count=0
            for k,v in mydict.items():
                if count==0:
                    df_all=fun_xs(df, {k:v})
                else:
                    df_all=fun_xs(df_all, {k:v})
                count+=1
            df=df_all
        else:  
            df=fun_xs(df, mydict)
    
    # Step 3 Now we check for numbers:
    val_to_search= [x  for x in val_to_search if fun_check_if_all_characters_are_numbers(x)]
    if len(val_to_search)>1:
        raise ValueError(f'This function can only search for 1 number, you passed {len(val_to_search)} numbers: {val_to_search}')
    mylist=[]
    count=0
    while len(mylist)==0:
        if count==0:
            # Try to find exact value
            mylist=[df[np.isclose(df,x)] for x in val_to_search ]
        else:
            # Try to find approx value
            mylist=[df[np.isclose(df,x, atol=1/np.power(10, 5-count))] for x in val_to_search ]
        mylist=[x for x in mylist if len(x)>0]
        count+=1
        if count==5:
            break
    
    if len(mylist):
        df=pd.concat(mylist)
        # Below we get exact column with specified value
        df= pd.concat([pd.concat([df[[x]] for x in df.columns if np.isclose(df[x],y, atol=1/np.power(10, 5-count))[0] ]) for y in val_to_search] )

    if 'FILE' in df.index.names:
        df=df.droplevel('FILE')
    if 'FILE' in df.columns:
        df=df.drop('FILE', axis=1)
    return convert_time(df, _type=int)

def createmydict(val_to_search:list, d:dict, exact=False):
    """
    Create a dictionary of keys and values based on a search criterion.

    This function searches for values in the input dictionary `d` that match (exactly or approximately) 
    any of the values in `val_to_search`. The keys of the resulting dictionary are the matched values 
    (as strings), and the values are lists of the corresponding keys from `d`.

    Parameters
    ----------
    val_to_search : list
        A list of values to search for in the dictionary `d`.
    d : dict
        The dictionary to search within. Values of `d` are expected to be lists of strings.
    exact : bool, optional
        If True, perform an exact match; otherwise, perform a case-insensitive substring match, by default False.

    Returns
    -------
    dict
        A dictionary where the keys are the matched values from `val_to_search` and the values 
        are lists of keys from `d` that contain these matched values.


    Notes
    -----
    - The search is case-insensitive.
    - If `exact` is False, the function performs a substring match.
    - Only keys from `d` that contain exactly one matched value in `val_to_search` are included in the result.
    """
    if exact:
        mydict={str(myval):[k] for k,v in d.items()  for myval in v for jj in val_to_search if str(jj).upper() == str(myval).upper() } 
    else:
        mydict={str(myval):[k] for k,v in d.items()  for myval in v for jj in val_to_search if str(jj).upper() in str(myval).upper() }      
    # We just want the index names with only one item found
    mylist=list(set(fun_flatten_list(list(mydict.values()))))
    mylist=list({x:mylist.count(x) for x in mylist if mylist.count(x)==1}.keys())
    mydict=SliceableDict(fun_invert_dictionary(mydict)).slice(*mylist)
    return mydict

def fun_xs_fuzzy_single(df:pd.DataFrame, val_to_search:Union[str, float, int])->pd.DataFrame:
    """
    Search for a single fuzzy match value in a DataFrame and return the matching rows.

    Parameters
    ----------
    df : pd.DataFrame
        The DataFrame to search in.
    val_to_search : str
        The value to search for in the DataFrame.

    Returns
    -------
    pd.DataFrame
        A DataFrame with rows that match the fuzzy search criteria.
    """
    try:
        res =fun_xs(df, search(df, val_to_search, False))
        if len(res)>0:
            return res
        else:
            return search(df, val_to_search, True)
    except:
        return search(df, val_to_search, True)

def convert_time(df: pd.DataFrame, _type: type = int) -> pd.DataFrame:
    """
    Convert the time-related columns in the DataFrame index to a specified type.

    This function converts the columns in the DataFrame's index to a specified type (e.g., int, float).
    It preserves the original index names and returns the DataFrame with the converted index.

    Parameters
    ----------
    df : pd.DataFrame
        The input DataFrame with the index to be converted.
    _type : type, optional
        The type to which the index columns should be converted (default is int).

    Returns
    -------
    pd.DataFrame
        The DataFrame with the index columns converted to the specified type.

    Examples
    --------
    >>> df = pd.DataFrame({'A': [1, 2]}, index=pd.Index(['2020', '2021'], name='Year')).T
    >>> df.index.names=['MODEL']
    >>> convert_time(df, int)
    MODEL   2020    2021   
    A          1       2

    Notes
    -----
    - The function relies on `fun_index_names` to convert the index columns to the specified type.
    - The original index names are preserved in the resulting DataFrame.
    """
    df=df.copy(deep=True)
    myidx = df.index.names
    # mycols = df.columns
    df = fun_index_names(df, True, _type)
    # mycols=[_type(x) for x in mycols]
    df2=df.reset_index().set_index(myidx)
    if _type==int or _type==float:
        try:
            df2.columns=pd.to_numeric(df2.columns)
        except:
            pass
    return df2

def get_native_countries(model:str, project:str, native:bool=True)->list:
    """
    Retrieve a list of countries based on their native or non-native status within a model and project.

    This function uses a regional-country mapping to identify countries that are either native (region with a single country) 
    or non-native (region with multiple countries) for a given model and project.

    Parameters
    ----------
    model : str
        The model name to be used for the regional-country mapping.
    project : str
        The project name to be used for the regional-country mapping.
    native : bool, optional
        If True, the function returns countries that are native (IAM region consists of a single country). 
        If False, it returns countries that are non-native (belonging to regions with multiple countries) (default is True).

    Returns
    -------
    list
        A list of countries based on their native or non-native status.
    Examples
    --------
    >>>  get_native_countries('REMIND-MAgPIE 3.2-4.6', 'NGFS_2023', True)
    ['IND', 'JPN', 'USA']

    Notes
    -----
    - The function relies on `fun_regional_country_mapping_as_dict` to obtain the regional-country mapping.
    - The `native` parameter controls whether to return native or non-native countries.
    """
    f=fun_regional_country_mapping_as_dict
    if native:
        mydict={k:v for k,v in f(model, project).items() if len(v)==1}
    else:
        mydict={k:v for k,v in f(model, project).items() if len(v)>1}
    return fun_flatten_list(mydict.values())

    
def fun_country_to_region(project:str, c:str, models:Optional[list]=None)->dict:
    """
    Map a country to its respective regions across different models in a project.

    This function retrieves all models associated with a project and maps a specified country 
    to its corresponding region for each model. The mapping is inverted to find the region(s) 
    for the given country.

    Parameters
    ----------
    project : str
        The project name for which the country-to-region mapping is to be performed.
    c : str,
        The country ISO3-code to be mapped to its respective regions.
    models: Optional[list],
        Your list of IAMs models, by default None
    Returns
    -------
    dict
        A dictionary where the keys are model names and the values are the regions associated 
        with the specified country for each model.

    Examples
    --------
    >>> fun_country_to_region('NGFS_2024', 'AUT')
    {'GCAM 6.0 NGFS': 'EU-15r', 'REMIND-MAgPIE 3.3-4.7': 'EU 28r', 'MESSAGEix-GLOBIOM 2.0-M-R12-NGFS': 'Western Europer'}

    Notes
    -----
    - The function uses `fun_get_models` to obtain all models associated with the project.
    - It inverts the regional-country mapping using `fun_invert_dictionary` to find the region(s) for the specified country.
    - If the country belongs to multiple regions in different models, the result will reflect that mapping.
    """
    if models is None:
        models= fun_get_models(project)
    mydict={model:fun_invert_dictionary({k:v for k,v in fun_regional_country_mapping_as_dict(model, project).items()})[c][0]
            for model in models}
    return mydict

def check_variable_coverage_issues(df: pd.DataFrame, model: str, mininum_no_of_var: int = 100, exceptions:Optional[list]=["ANT","GIB"]) -> dict:
    """
    Identify countries that do not meet the minimum variable requirements, indicating issues with the downscaling outcome.

    This function takes a `df` and a `model` name, and identifies countries within each scenario
    that have fewer variables than the specified minimum number `mininum_no_of_var`. 
    It returns a dictionary with scenarios as keys and dictionaries of countries with inadequate variables as values.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the data to be analyzed.
    model : str
        The model name to be used in the analysis.
    mininum_no_of_var : int, optional
        The minimum number of variables required (default is 100).
    exceptions : List, optional
        Lits of countries (ISO3 codes) to be exluded from checks that will raise Value Errors, by default ["ANT","GIB"]
        
    Returns
    -------
    dict
        A dictionary where each key is a scenario, and each value is a dictionary of countries that do not meet the minimum variable requirements.

    Examples
    --------
    >>> d_all = flag_inadequate_downscaling(df, 'GCAM 6.0 NGFS')
    >>> print(d_all)
    {'scenario1': {'country1': 95, 'country2': 88}, 'scenario2': {'country3': 70}}
    """
    df=df.copy(deep=True)

    # Slice by `df` by `model` and get available scenarios and regions
    df=fun_xs(df, {"MODEL": model})
    av_scenarios=df.reset_index().SCENARIO.unique()
    av_regions=df.reset_index().REGION.unique()

    # Check for missing IEA countries
    missing_regions=[x for x in iea_countries if x not in av_regions]
    missing_regions=[x for x in missing_regions if x not in exceptions]
    if len(missing_regions):
        raise ValueError(f"These IEA countries are missing: {missing_regions}")
    
    # Count how many variables are available by country and scenario
    d_all = {}
    for scen in av_scenarios:
        df_sel=fun_xs(df, { "SCENARIO": scen})
        d = {c: len(summarize_dataframe(fun_xs(df_sel, {"REGION": c}))['VARIABLE']) 
             for c in av_regions}
        d = {k: v for k, v in d.items() if v < mininum_no_of_var and k in iea_countries}
        d_all[scen] = d

    return d_all


def fun_check_missing_variables(df:pd.DataFrame,  var_to_check:list, check_countries:Optional[list]=None, coerce_errors:bool=False)->None:
    """
    Check for missing variables in a given DataFrame for specific models and countries.

    This function checks if specific variables are present in the DataFrame for each model 
    and specified countries. If certain variables are missing, it raise a value error.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the data to be checked.
    var_to_check : list
        List of variables to check for presence in the DataFrame.
    check_countries : list, optional
        List of countries (regions) to check for missing variables. If not provided, the 
        default `check_IEA_countries` list will be used.
    coerce_errors: bool, optional
        Whether you want to coerce errors, by default False

    Returns
    -------
    None
        This function prints the models and missing variables for specified countries.
    
    Raises
    ------
    ValueError
        If any of the variables are missing.

    Examples
    --------
    >>> df = pd.DataFrame(...)  # DataFrame with appropriate data and structure
    >>> var_to_check = ['variable1', 'variable2']
    >>> fun_check_missing_variables(df, var_to_check, check_countries=['USA', 'CAN'])
    
    Notes
    -----
    The function assumes that the DataFrame's index contains 'MODEL', 'VARIABLE', and 'REGION' 
    (or 'ISO') levels. It uses the `fun_rename_index_name`, `fun_xs`, and 
    `fun_check_missing_elements_in_dataframe` helper functions.
    """
    df=df.copy(deep=True)
    df=fun_rename_index_name(df, {"ISO":"REGION"})
    if check_countries is None:
        check_countries=check_IEA_countries
    for model in df.reset_index().MODEL.unique():
        print(f"{model}")
        # Check if specific variables are missing:
        for i in var_to_check:
            df_sel=fun_xs(df, {"VARIABLE":i, "MODEL":model})
            [fun_check_missing_elements_in_dataframe({"REGION":c,"VARIABLE":i, "MODEL":model}, df_sel, coerce_errors=coerce_errors) 
                                for c in check_countries]


def step1_process_region_countries(r:str, df_countries:pd.DataFrame):
    """
    Process region and create a list of countries for the given region.

    Parameters:
    r (str): The region string to process.
    df_countries (pd.DataFrame): DataFrame containing country information with regions and ISO codes.

    Returns:
    str: The processed region name.
    list: List of country ISO codes in the region.
    """
    
    # Separate model name from region (using '|' as delimiter)
    r_sep = r.rsplit("|")[0]
    if isinstance(r_sep, list):
        r_sep = r_sep[1]
    else:
        r_sep = r.rsplit("|")[0]

    region = r
    
    # Create a list of country ISO codes in the region
    countrylist = df_countries[df_countries.REGION == region].ISO.unique()

    # In case the region is not found, try with 'r' appended at the end
    if len(countrylist) == 0:
        countrylist = df_countries[df_countries.REGION == region + "r"].ISO.unique()
    
    return region, countrylist.tolist()

def step1_process_region_and_dataframe(
    model: str, 
    region: str, 
    df_iam_all: pd.DataFrame, 
    df_iea_all: pd.DataFrame, 
    df_iea_h: pd.DataFrame, 
    sectors: List[str], 
    iea_flow_dict: Dict[str, Tuple[str, str]], 
    gdp: str, 
    pop_iea: str
) -> Tuple[str, pd.DataFrame, pd.DataFrame, List]:
    """
    Process the region, select and transform the necessary dataframes.
    
    Parameters:
    model (str): The model name.
    region (str): The region name.
    df_iam_all (pd.DataFrame): DataFrame containing IAM data for all models.
    df_iea_all (pd.DataFrame): DataFrame containing IEA data.
    df_iea_h (pd.DataFrame): DataFrame to hold historical data.
    sectors (list): List of sectors.
    iea_flow_dict (dict): Dictionary mapping sectors to IEA flows and products.
    gdp (str): GDP column name.
    pop_iea (str): Population column name.
    
    Returns:
    tuple: A tuple containing the updated region name and transformed DataFrames (df_iea_melt, df_iea_h).
    """
    
    # Look for the region, if you don't find it, look for Model|Region
    if len(df_iam_all[df_iam_all.REGION == region]) == 0:
        if len(df_iam_all[df_iam_all.REGION == model + "|" + region]) != 0:
            r = model + "|" + region
        else:
            r = model + "|R11_" + region
            r_sep = r.rsplit("_")
            if isinstance(r_sep, list):
                r_sep = r_sep[1]
            else:
                r_sep = r.rsplit("_")[0]  # separates model name from region (using '|' as delimiter)

        region = r

    # IEA: select and melt database (also used for 2010 energy mix criteria)
    max_year = int(pd.to_numeric(df_iea_all.columns, errors='coerce').max())
    range_list = [str(i).zfill(2) for i in range(1960, max_year, 1)]

    flow_list = [iea_flow_dict[s][0] for s in sectors]
    flow_list = str(flow_list).replace("[", "").replace("]]", "")
    flow_list = flow_list.split("'")

    df_iea_sel = df_iea_all[
        (df_iea_all.FLOW == "Electricity output (GWh)") |
        (df_iea_all.FLOW.isin(flow_list)) |
        (df_iea_all.FLOW == "Imports") |
        (df_iea_all.FLOW == "Exports") |
        (df_iea_all.FLOW == "Total final consumption")
    ]

    df_iea_melt = df_iea_sel.melt(
        id_vars=["COUNTRY", "FLOW", "PRODUCT", "IAM_FUEL", "ISO"],
        value_vars=range_list,
        var_name="TIME",
        value_name="VALUE"
    )
    df_iea_melt["VALUE"] = pd.to_numeric(df_iea_melt["VALUE"], errors="coerce")
    df_iea_melt["TIME"] = pd.to_numeric(df_iea_melt["TIME"], errors="coerce")

    # Adding TWN ISO code to df_iea_melt
    df_iea_melt.set_index("COUNTRY", inplace=True)
    if "Chinese Taipei" in df_iea_melt.index.unique():
        df_iea_melt.loc["Chinese Taipei", "ISO"] = "TWN"

    if 'ISO' in df_iea_h.columns:
        df_iea_h=df_iea_h.set_index('ISO')
    selcol = ["REGION", "COUNTRY", gdp, pop_iea, "TIME", "SCENARIO", "MODEL", "UNIT", "Y_NUM", "Y_DEN", "X_NUM", "X_DEN"] + sectors
    df_iea_h = df_iea_h[df_iea_h.columns[df_iea_h.columns.isin(selcol)]]

    return region, df_iea_melt, df_iea_h,

def step1_process_model_data(df_iam_all_models, model, df_countries, pyam_mapping_file, region_patterns, target_patterns):
    """
    Process model data by filtering and mapping countries and regions, and identifying target scenarios.

    Parameters:
    df_iam_all_models (pd.DataFrame): DataFrame containing all models data.
    model (str): The model to filter by.
    df_countries (pd.DataFrame): DataFrame containing country information.
    pyam_mapping_file (str): File path to the pyam mapping file.
    region_patterns (list): List of region patterns to match.
    target_patterns (list): List of target patterns to match.

    Returns:
    pd.DataFrame: Filtered DataFrame for the given model.
    list: List of filtered regions.
    list: List of filtered target scenarios.
    """
    
    # Filter DataFrame for the given model
    df_iam_all = df_iam_all_models[df_iam_all_models.MODEL == model]
    
    # Load model mapping for countries and regions
    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)
    
    # Filter regions based on input patterns
    regions = [
        r for r in regions
        if any(
            r in r_df and match_any_with_wildcard(r_df, region_patterns)
            for r_df in df_iam_all.REGION.unique()
        )
    ]

    # Identify target scenarios
    targets = list(df_iam_all_models[df_iam_all_models.MODEL == model].SCENARIO.unique())
    targets = [t for t in targets if match_any_with_wildcard(t, target_patterns)]

    # Remove 'nanr' from regions if present
    if "nanr" in regions:
        regions.remove("nanr")
    
    return df_iam_all, regions, targets


def step1_update_targets_with_ref(
    ref_target: Union[str, List[str]], 
    targets: List[str]
) -> List[str]:
    """
    Update the targets list to prioritize the reference target.
    
    Parameters:
    ref_target (Union[str, List[str]]): The reference target, which can be a string or a list of strings.
    targets (List[str]): The list of target scenarios.
    
    Returns:
    List[str]: The updated list of target scenarios with the reference target prioritized.
    
    Raises:
    Exception: If ref_target is not a string or a list.
    """
    if ref_target in targets:  # ADDED ON 2021_08_13 to make it work with Jarmo's engage scenario (in case there is no ref_target)
        if isinstance(ref_target, str):
            targets.remove(ref_target)  # This works if ref_target is a string
            targets = [ref_target] + targets  # Put ref_target at the beginning of the targets list
        elif isinstance(ref_target, list):
            targets = [tar for tar in targets if tar not in ref_target]
            targets = ref_target + targets  # Put ref_target at the beginning of the targets list
        else:  # 2021_03_10
            raise Exception(
                "ref_target should be either a string or a list, not a " + str(type(ref_target))
            )
    else:
        ref_target = targets[0]
        print(
            ref_target,
            "ref_target not found in targets. We re-define the ref_target as the first target in the target list.",
        )

    return targets

def step1_process_ssp_scenario(
    scen_dict: Optional[Dict[str, str]],
    target: str,
    ssp_scenario: str,
    df_ssp: pd.DataFrame,
    change_ssp_model: bool,
    change_ssp_scenario: bool,
    model: str,
    ssp_model: str
) -> pd.DataFrame:
    """
    Process SSP data and create the melted DataFrame for a given scenario.

    Parameters:
    scen_dict (Optional[Dict[str, str]]): Scenario dictionary, if any.
    target (str): Target scenario.
    ssp_scenario (str): SSP scenario.
    df_ssp (pd.DataFrame): DataFrame containing SSP data.
    change_ssp_model (bool): Flag to change SSP model.
    change_ssp_scenario (bool): Flag to change SSP scenario.
    model (str): Model name.
    ssp_model (str): SSP model name.

    Returns:
    pd.DataFrame: Melted DataFrame of SSP data filtered by scenario and model.
    
    Raises:
    ValueError: If the model or scenario is not found in the data.
    """

    if scen_dict is not None and target in scen_dict:
        def_ssp_scenario = None
    else:
        def_ssp_scenario = ssp_scenario

    ssp_scenario = fun_get_ssp(target, default_ssp=def_ssp_scenario, _scen_dict=scen_dict)

    # Melting the SSP df
    df_ssp_melt = df_ssp.melt(
        id_vars=["SCENARIO", "VARIABLE", "MODEL", "UNIT", "ISO"]
    )
    df_ssp_melt.rename(
        columns={"variable": "TIME", "value": "VALUE"}, inplace=True
    )
    df_ssp_melt["TIME"] = pd.to_numeric(df_ssp_melt["TIME"])

    # SSP data selection (MODEL AND SCENARIO)
    txt = "You can pass `ssp_model=None` and/or `ssp_scenario=None` if `df_ssp` contains projections for each IAM model/scenario at the country level"
    if change_ssp_model:
        if model in df_ssp_melt.MODEL.unique():
            ssp_model = model
        else:
            raise ValueError(
                f"cannot find {ssp_model} nor {model} in `df_ssp`. We found {df_ssp_melt.MODEL.unique()}. {txt}"
            )
    if change_ssp_scenario:
        if target in df_ssp_melt.SCENARIO.unique():
            ssp_scenario = target
        else:
            raise ValueError(
                f"cannot find {ssp_scenario} nor {target} in `df_ssp`. We found {df_ssp_melt.SCENARIO.unique()}. {txt}"
            )

    df_ssp_melt = df_ssp_melt[
        (df_ssp_melt.MODEL == ssp_model)
        & (df_ssp_melt.TIME.isin(range(2010, 2105, 5)))
    ]

    if target in df_ssp_melt.SCENARIO.unique():
        df_ssp_melt = df_ssp_melt[df_ssp_melt.SCENARIO == target]
    elif ssp_scenario in df_ssp_melt.SCENARIO.unique():
        df_ssp_melt = df_ssp_melt[df_ssp_melt.SCENARIO == ssp_scenario]
    else:
        df_ssp_melt = df_ssp_melt[
            df_ssp_melt["SCENARIO"].str.contains(ssp_scenario)
        ]

    return df_ssp_melt


def step1_initialize_main_dataframe(
    df_ssp_melt: pd.DataFrame,
    countrylist: List[str],
    gdp: str,
    pop: str,
    m: str
) -> pd.DataFrame:
    """
    Process the main DataFrame by adding country-level GDP and population from the SSP database.

    Parameters:
    df_ssp_melt (pd.DataFrame): Melted DataFrame containing SSP data.
    countrylist (List[str]): List of countries.
    gdp (str): GDP column name.
    pop (str): Population column name.
    m (str): Index name for the DataFrame.

    Returns:
    pd.DataFrame: Main DataFrame with country-level GDP and population added.
    
    Raises:
    ValueError: If GDP|PPP data coincide with Population data.
    """

    df_main = pd.DataFrame()
    
    # Populate GDP data in df_main
    gdp_data = fun_pd_sel(df_ssp_melt, range(2010, 2101, 1), countrylist, gdp)
    df_main.loc[:, gdp] = gdp_data.VALUE
    
    # Set index and populate Population data in df_main
    setindex(df_main, m)
    pop_data = fun_pd_sel(df_ssp_melt, range(2010, 2101, 1), countrylist, pop)
    df_main.loc[:, pop] = pop_data.VALUE
    
    # Check if GDP|PPP == Population data
    if (df_main['GDP|PPP'] / df_main['Population']).eq(True).all():
        raise ValueError('GDP|PPP data coincide with Population in the `df_main`. Please check your socioeconomic data')

    return df_main

def step1_print_progress(
        target: str, 
        models: List[str], 
        targets: List[str], 
        sectors: List[str], 
        _s: int, 
        last_s: int, 
        region: str, 
        regions: List[str], 
        count_loop: int, 
        seconds0: float, 
        ) -> None:
    """
    Print progress information including scenario details, sector details, progress percentage,
    elapsed time, and estimated completion time.

    Parameters:
    - target (str): The current scenario target.
    - models (List[str]): List of all models.
    - targets (List[str]): List of all scenario targets.
    - sectors (List[str]): List of sectors.
    - _s (int): Index of the current sector.
    - last_s (int): Total number of sectors.
    - region (str): The current region.
    - regions (List[str]): List of all regions.
    - count_loop (int): Current progress count in the loop.
    - seconds0 (float): Starting time in seconds.

    Returns:
    - None
    """
    # how big is our loop. (how many sectors, targets, regions we have)
    len_loop = (len(models) * len(targets) * len(regions) * last_s) 
    seconds = time.time()  ## time elapsed
    estimated_seconds = (seconds - seconds0) / (0.5 / len_loop + (count_loop) / len_loop) + 30
    
    print("================================================================")
    print(f"SCENARIO: {target} ({targets.index(target) + 1} / {len(targets)})")
    
    try:
        print(f"SECTOR: {sectors[_s]} ({_s + 1} / {last_s}) | {region} ({regions.index(region) + 1} / {len(regions)})")
    except ValueError:
        print(f"SECTOR: {sectors[_s]} ({_s + 1} / {last_s}) | {region} ({regions.index(region.rsplit('|')[1]) + 1} / {len(regions)})")
    
    print(f"PROGRESS: {round((count_loop) / (len_loop) * 100)}% time elapsed: {round(seconds - seconds0)} seconds")
    print(f"ESTIMATED TIME: {time.ctime(estimated_seconds + seconds0)}")
    print("================================================================")

def step1_add_2100_gdp_pop_to_hist_data(
        df_ssp_melt: pd.DataFrame, 
        df_iea_h: pd.DataFrame, 
        t: int, 
        countrylist: List[str], 
        gdp: str, 
        pop: str, 
        pop_iea: str) -> pd.DataFrame:
    """
    Add GDP/POP data from `df_ssp_melt` to `df_iea_h` for a given year (t) and country (c).

    Parameters:
    - df_ssp_melt (pd.DataFrame): DataFrame containing SSP data.
    - df_iea_h (pd.DataFrame): DataFrame where values will be assigned.
    - t (int): Year of interest.
    - countrylist List[str]: Country of interest.
    - gdp (str): Column name for GDP in df_ssp_melt.
    - pop (str): Column name for population in df_ssp_melt.
    - pop_iea (str): Column name for population in df_iea_h.

    Returns:
    - pd.DataFrame: Updated df_iea_h DataFrame.
    """
    a=fun_xs(df_ssp_melt, {'ISO':countrylist,'TIME':[t]})[['VALUE','VARIABLE']].set_index('VARIABLE', append=True)['VALUE'].unstack('VARIABLE')
    return pd.concat([df_iea_h, a.rename({x:x.upper() for x in a.columns}, axis=1)])

    # for c in countrylist:
    #     gdp_val = fun_pd_sel(df_ssp_melt, t, c, gdp)["VALUE"][0]
    #     pop_val = fun_pd_sel(df_ssp_melt, t, c, pop)["VALUE"][0]

    #     df_iea_h = fun_pd_assign(df_iea_h, t, c, gdp, gdp_val)
    #     df_iea_h = fun_pd_assign(df_iea_h, t, c, pop_iea, pop_val)

    # return df_iea_h


# NOTE: redefined from enlong_calc_summer
def enlong_calc_summer(
    _func, _s, _region, x_num, x_den, _countrylist, _df_fut, df_iam, region, sectors, df_main
):
    """
    Calculates enlong based on regression. It returns a series for all countryies.
    """
    
    df_iam=df_iam.copy(deep=True)
    # _df_fut=eval(_df_fut_str)
    setindex(_df_fut, "ISO")
    _df_iam = pd.DataFrame()

    setindex(df_iam, "TIME")
    _df_iam = pd.DataFrame()

    ## making sure variables have the same time lenght
    t1 = df_iam[(df_iam.VARIABLE == x_den) & (df_iam.REGION == region)]["VALUE"].dropna().index
    t2 = df_iam[(df_iam.VARIABLE == x_num) & (df_iam.REGION == region)]["VALUE"].dropna().index
    
    df_iam = df_iam[df_iam.REGION==region]
    df_iam = df_iam[df_iam.index.isin(t1) & df_iam.index.isin(t2)]

    _df_iam.loc[:, "X_DEN"] = df_iam[(df_iam.VARIABLE == x_den)]["VALUE"]
    _df_iam["X_NUM"] = df_iam[(df_iam.VARIABLE == x_num)]["VALUE"]
    _df_iam["Y_DEN"] = df_iam[(df_iam.VARIABLE == dict_y_den[sectors[_s]])]["VALUE"]
    _df_iam["Y_NUM"] = df_iam[(df_iam.VARIABLE == sectors[_s])].VALUE

    _df_fut["X_NUM"] = _df_fut[x_num]
    _df_fut["X_DEN"] = _df_fut[x_den]
    try:
        _df_fut["Y_DEN"] = _df_fut[dict_y_den[sectors[_s]]]
    except:  ## IF THE ABOVE DOES NOT WORK IT MEANS THE DENOMINATOR IS THE (PREVIOUSLY CALCULATED) ENLONG.
        _df_fut["Y_DEN"] = _df_fut[
            dict_y_den[sectors[_s]] + "_ENLONG_RATIO"
        ]  ## CAREFUL HERE WE NEED TO DISTINGUIS FOR ALL SECTORS

    # Clipping all variables shoudl be positive >= threshold 1e-10
    _df_iam["Y_DEN"] = _df_iam["Y_DEN"].clip(1e-10, np.inf)
    _df_iam["Y_NUM"] = _df_iam["Y_NUM"].clip(1e-10, np.inf)

    # setindex(_df_fut, "TIME")
    # setindex(df_main, "TIME")  
    setindex(_df_fut, "ISO")  
    setindex(df_main, "ISO")

    _r_squared = 0
    _beta = 0
    _alpha = 0

    ## Controlling for functional form:
    if _func == "log-log":
        _r_squared, _beta, _alpha = reg_stat(
            np.log(_df_iam["X_NUM"] / _df_iam["X_DEN"]),
            np.log((_df_iam["Y_NUM"] / _df_iam["Y_DEN"])),
        )
        _df_fut["ENLONG"] = (
            flexi_fun_lin(_alpha, _beta, np.log(_df_fut[x_num] / _df_fut[x_den]))
            * _df_fut["Y_DEN"]
        )  # applies for all countries (same alpha and beta)
        print(_alpha, _beta, _r_squared)

    elif _func == "s-curve":
        print("s-curve function")
        _r_squared, _beta, _alpha = reg_stat(
            np.log(_df_iam["X_NUM"] / _df_iam["X_DEN"]),
            np.log(s_shape_lin(_df_iam["Y_NUM"] / _df_iam["Y_DEN"])),
        )
        _df_fut["ENLONG"] = (
            s_shape_lin(
                flexi_fun_lin(
                    _alpha, _beta, np.log(_df_fut["X_NUM"] / _df_fut["X_DEN"])
                ),
                sign=-1,
            )
            * _df_fut["Y_DEN"]
        )  # applies for all countries (same alpha and beta)
        print(_alpha, _beta, _r_squared)
        if (
            np.log(s_shape_lin(_df_iam["Y_NUM"] / _df_iam["Y_DEN"])).unique()[0]
            == np.inf
        ):
            print("#############!!!!!!!!!!!!!!!###############")
            print("Wrong functional form, Try log-log instead")
            print("#############!!!!!!!!!!!!!!!###############")

    else:
        _r_squared, _beta, _alpha = reg_stat(
            (_df_iam["X_NUM"] / _df_iam["X_DEN"]),
            ((_df_iam["Y_NUM"] / _df_iam["Y_DEN"])),
        )
        _df_fut["ENLONG"] = (
            flexi_fun_lin(_alpha, _beta, (_df_fut[x_num] / _df_fut[x_den]), _exp=False)
            * _df_fut["Y_DEN"]
        )  # applies for all countries (same alpha and beta)
        print(_alpha, _beta, _r_squared)
        # _y = np.nan

    #     return _df_fut['ENLONG']
    return _df_fut["ENLONG"], _beta, _alpha

def enlong_calc_2021(
    _func,
    _s,
    _region,
    x_num,
    x_den,
    _countrylist,
    _df_fut,
    df_iam,
    region,
    sectors, df_main, target,
    _alpha_exogenous=False,
    _y_min_threshold=1e-3,
    ):
    """
    Calculates enlong based on regression. It returns a series for all countries.

    It also drops values below a certain threshold (_y_min_threshold) for the estimation of paramters in regression

    ## 2021_01_11 Compared to enlong_calc_summer, This function returns also the paramaters alpha and beta based on enlong projections
    """
    setindex(_df_fut, "ISO")
    setindex(df_iam, "TIME")
    _df_iam=pd.DataFrame()
    df_iam=df_iam.copy(deep=True)

    ## making sure variables have the same time lenght (xnum and xden) NOTE this is global variable (df_iam, instead of _df_iam)
    t1 = df_iam[(df_iam.VARIABLE == x_den) & (df_iam.REGION == region)]["VALUE"].dropna().index
    t2 = df_iam[(df_iam.VARIABLE == x_num) & (df_iam.REGION == region)]["VALUE"].dropna().index

    # Slice `df_iam` for region and time
    df_iam = df_iam[df_iam.REGION==region]
    df_iam = df_iam[df_iam.index.isin(t1) & df_iam.index.isin(t2)]

    # Reading regional level data
    _df_iam.loc[:, "X_DEN"] = df_iam[(df_iam.VARIABLE == x_den)]["VALUE"]
    _df_iam["X_NUM"] = df_iam[(df_iam.VARIABLE == x_num)]["VALUE"]
    _df_iam["Y_DEN"] = df_iam[(df_iam.VARIABLE == dict_y_den[sectors[_s]])]["VALUE"]
    _df_iam["Y_NUM"] = df_iam[(df_iam.VARIABLE == sectors[_s])].VALUE

    # Reading country level data
    _df_fut["X_NUM"] = _df_fut[x_num]
    _df_fut["X_DEN"] = _df_fut[x_den]

    if _s == 0: 
        _df_fut["Y_DEN"] = _df_fut[x_num]  # This will be the GDP
    else:
        try:
            _df_fut["Y_DEN"] = _df_fut[dict_y_den[sectors[_s]]]
        except:  ## IF THE ABOVE DOES NOT WORK IT MEANS THE DENOMINATOR IS THE (PREVIOUSLY CALCULATED) ENLONG.
            # CAREFUL HERE WE NEED TO DISTINGUIS FOR ALL SECTORS
            _df_fut["Y_DEN"] = _df_fut[dict_y_den[sectors[_s]] + "_ENLONG_RATIO"]  

    # Clipping all variables shoudl be positive >= threshold 1e-10
    _df_iam["Y_DEN"] = _df_iam["Y_DEN"].clip(1e-10, np.inf)
    _df_iam["Y_NUM"] = _df_iam["Y_NUM"].clip(1e-10, np.inf)

    setindex(_df_fut, "ISO")  
    setindex(df_main, "ISO")

    _beta = 0
    _alpha = 0

    # We drop values below a given threshold
    _y = []
    _count = 0
    while len(_y) == 0:  ## we increase the min threshold if the len of _y is
        _y_num = df_iam[
            (df_iam.VARIABLE == sectors[_s])
            & (df_iam.SCENARIO == target)]["VALUE"]

        _y_den = df_iam[
            (df_iam.VARIABLE == dict_y_den[sectors[_s]])
            & (df_iam.SCENARIO == target)
        ]["VALUE"]
        _y = _y_num / _y_den
        _y = _y[_y >= _y_min_threshold]  ## Threshold value
        _x = (
            _df_iam.loc[_y.index, "X_NUM"] / _df_iam.loc[_y.index, "X_DEN"]
        )  ## same index as _y
        _y_min_threshold = (
            _y_min_threshold / 10
        )  ## this one should have been /10

        _count = _count + 1
        #             print(_count,len(_y),_y_min_threshold)
        if len(_y) != 0:
            break

        if _y_min_threshold == 1e-6:
            break

        if _count == 3:
            break
    
    
    # # Without harmonization:
    # pd.concat([pd.DataFrame(fun_projections(f, fun_regression(f, _x, _y),_x)).rename({0:f},axis=1) for f in ['log-log','linear','logistic']], axis=1)
    # # With harmonization
    # pd.concat([pd.DataFrame(fun_projections(f, fun_harmonize_alpha(2010, f, _x, _y, fun_regression(f, _x, _y), func_dict),_x)).rename({0:f},axis=1) for f in ['log-log','linear','logistic']], axis=1)
    
    info_dict=fun_regression(_func, _x, _y)
    info_dict=fun_harmonize_alpha(None, _func, _x, _y, info_dict, func_dict)

    # Graph to compare:
    # import matplotlib
    # matplotlib.use('TkAgg') # not working even with default 'agg'
    # from matplotlib import pyplot as plt
    # plt.scatter(_x, _y)
    # plt.scatter(_x, fun_regression(_func, _x, _y, True))
    # plt.show()

    _beta=info_dict['beta']
    _alpha=info_dict['alpha'] if _func in ['linear','lin','log-log'] else np.nan
    if _alpha_exogenous != False:  # exogenous input
        _alpha = _alpha_exogenous
    ei=fun_projections(_func, info_dict,(_df_fut[x_num] / _df_fut[x_den])).clip(0,1)
    _df_fut["ENLONG"] = (ei * _df_fut["Y_DEN"] )  

    if _s >= 1:
        _df_fut["ENLONG"] = np.minimum(_df_fut["ENLONG"], _df_fut["Y_DEN"])

    return _df_fut["ENLONG"], _beta, _alpha

def step1_calculate_enlong(
        df_main: pd.DataFrame,
        func_type: str,
        _s: int,
        region: str,
        gdp: str,
        pop: str,
        countrylist: List[str],
        df_iam: pd.DataFrame,
        sectors,
        target
        ) -> None:
    """
    Calculate 'ENLONG', 'BETA_ENLONG', and 'ALPHA_ENLONG' values for df_main DataFrame based on given parameters.

    Parameters:
    - df_main (pd.DataFrame): DataFrame where 'ENLONG', 'BETA_ENLONG', and 'ALPHA_ENLONG' values will be calculated.
    - func_type (str): Type of function to use for calculation ('enlong_calc_2021' or 'enlong_calc_summer').
    - _s (int): Index of the sector.
    - region (str): Region name.
    - gdp (str): GDP column name.
    - pop (str): Population column name.
    - countrylist (List[str]): List of countries.
    - df_iam (pd.DataFrame): DataFrame containing IAM data.

    Returns:
    - None: The function modifies df_main in place.
    """
    try:
        df_main["ENLONG"], df_main["BETA_ENLONG"], df_main["ALPHA_ENLONG"] = enlong_calc_2021(
            func_type, _s, region, gdp, pop, countrylist, df_main, df_iam, region, sectors, df_main, target
        )  # s-curve
        
        if len(df_main["ENLONG"].unique()) == 1:
            df_main["ENLONG"], df_main["BETA_ENLONG"], df_main["ALPHA_ENLONG"] = enlong_calc_summer(
                func_type, _s, region, gdp, pop, countrylist, df_main, df_iam, region, sectors, df_main
            )  # s-curve
            print("** ENLONG CALC SUMMER")
        else:
            print("** ENLONG CALC 2021")
        return df_main
    
    except Exception as e:
        print(f"Error occurred: {e}")
        print("** ENLONG CALC SUMMER")
        df_main["ENLONG"], df_main["BETA_ENLONG"], df_main["ALPHA_ENLONG"] = enlong_calc_summer(
            func_type, _s, region, gdp, pop, countrylist, df_main, df_iam, region, sectors, df_main
        )  # s-curve
        return df_main


def step1_fun_hist_ind(
    _s: int,
    _df_iea_h: pd.DataFrame,
    df_iea_melt: pd.DataFrame,
    countrylist: List[str],
    ref_countries: List[str],
    sectors: List[str],
    iea_flow_dict: dict,
    m: str,
    gdp: str,
    pop_iea: str,
    dict_y_den: dict,
    df_main: pd.DataFrame,
) -> pd.DataFrame:
    """
    Creates historical indicators for use in regression analysis.

    Parameters:
    _s (int): Sector index.
    _df_iea_h (pd.DataFrame): DataFrame to hold historical data.
    df_iea_melt (pd.DataFrame): Melted DataFrame containing IEA data.
    countrylist (List[str]): List of countries.
    ref_countries (List[str]): List of reference countries.
    sectors (List[str]): List of sectors.
    iea_flow_dict (dict): Dictionary mapping sectors to IEA flow data.
    m (str): Index name for the DataFrame.
    gdp (str): GDP column name.
    pop_iea (str): Population column name.
    dict_y_den (dict): Dictionary mapping sectors to their Y_DEN values.
    df_main (pd.DataFrame): Main DataFrame containing downscaled results.
    fun_pd_sel (function): Function to select data from DataFrame.
    fun_pd_assign (function): Function to assign data to DataFrame.

    Returns:
    pd.DataFrame: Updated DataFrame with historical data.
    """

    # Select data for the given sector and countries
    _df_iea_melt = fun_pd_sel(df_iea_melt, "", countrylist + ref_countries)

    sector_flow, sector_product = iea_flow_dict[sectors[_s]][0], iea_flow_dict[sectors[_s]][1]

    if isinstance(sector_flow, str):  # Single flow
        if isinstance(sector_product, str):  # Single product
            _df_iea_h[sectors[_s]] = (
                _df_iea_melt[
                    (_df_iea_melt.FLOW == sector_flow) & (_df_iea_melt.PRODUCT == sector_product)
                ].VALUE * 0.041868 / 1e3  # Conversion from ktoe to EJ
            )
        else:  # Multiple products
            _df_iea_h[sectors[_s]] = (
                _df_iea_melt[
                    (_df_iea_melt.FLOW == sector_flow) & (_df_iea_melt.PRODUCT.isin(sector_product))
                ].VALUE * 0.041868 / 1e3
            )
    else:  # Multiple flows
        if isinstance(sector_product, str):  # Single product
            _df_iea_h[sectors[_s]] = (
                _df_iea_melt[
                    (_df_iea_melt.FLOW.isin(sector_flow)) & (_df_iea_melt.PRODUCT == sector_product)
                ].VALUE.groupby(m).sum() * 0.041868 / 1e3
            )
        else:  # Multiple products
            _df_iea_h[sectors[_s]] = (
                _df_iea_melt[
                    (_df_iea_melt.FLOW.isin(sector_flow)) & (_df_iea_melt.PRODUCT.isin(sector_product))
                ].VALUE.groupby(m).sum() * 0.041868 / 1e3
            )
        
    # Slice for countrylist
    _df_iea_h=_df_iea_h[_df_iea_h.index.get_level_values('ISO').isin(countrylist)]
    
    # Adjust 2010 GDP Data (so that historical data is consistent with the projected GDP)
    deflator=(df_main[df_main.TIME==2010][gdp]/_df_iea_h.loc[2010, gdp]).dropna()
    _df_iea_h[gdp]=_df_iea_h[gdp]*deflator

    # Adjust 2010 Population Data (so that historical data is consistent with the projected Population)
    deflator_pop=(df_main[df_main.TIME==2010]['Population']/_df_iea_h.loc[2010, pop_iea]).dropna()
    _df_iea_h[pop_iea]=_df_iea_h[pop_iea]*deflator_pop


    # Setting X_NUM and X_DEN for GDP and population
    _df_iea_h["X_NUM"] = _df_iea_h[gdp]
    _df_iea_h["X_DEN"] = _df_iea_h[pop_iea]

    # Setting Y_NUM for the sector
    _df_iea_h["Y_NUM"] = _df_iea_h[sectors[_s]]

    # Setting Y_DEN for GDP (denominator for sectors other than 0)
    if _s >= 1:
        _df_iea_h["Y_DEN"] = _df_iea_h[dict_y_den[sectors[_s]]]
    else:
        _df_iea_h["Y_DEN"] = _df_iea_h[gdp]


    _df_iea_h["Y_NUM"] = _df_iea_h[sectors[_s]]

    return _df_iea_h

def step1_enshort_calc_summer(
    _start: int,
    _end: int,
    _func: str,
    _s: int,
    _c: str,
    countrylist: List[str],
    sectors: List[str],
    dict_y_den: dict,
    df_main: pd.DataFrame,
    df_iea_h: pd.DataFrame,
    gdp: str,
    pop_iea: str,
    _base_year: int = 2010,
    _print: bool = True,
) -> Union[pd.Series, float, float, float]:
    """
    Calculates enshort based on regression. It returns a series for one country.
    It harmonizes intercept to match the base year data.

    Parameters:
    - Various parameters for dataframes, columns, and functional forms.

    Returns:
    - Series of calculated values, R-squared, beta, and alpha.
    """


    # Prepare historical data
    _df_fut = df_main
    _df_hist = df_iea_h[
        [sectors[_s], "Y_NUM", "Y_DEN", gdp, pop_iea, "X_NUM", "X_DEN"]
    ].copy(deep=True)
    _df_hist = fun_pd_sel(df_iea_h, "", _c)[
        [sectors[_s], "Y_NUM", "Y_DEN", gdp, pop_iea, "X_NUM", "X_DEN"]
    ].dropna()

    setindex(_df_hist, "TIME")
    setindex(_df_fut, "TIME")

    # Calculate X and Y
    _x = (_df_hist.loc[(_df_hist.ISO == _c) & (_df_hist.index >= _start) & (_df_hist.index <= _end), "X_NUM"] /
          _df_hist.loc[(_df_hist.ISO == _c) & (_df_hist.index >= _start) & (_df_hist.index <= _end), "X_DEN"]).dropna()
    _y = (_df_hist.loc[(_df_hist.ISO == _c) & (_df_hist.index >= _start) & (_df_hist.index <= _end), "Y_NUM"] /
          _df_hist.loc[(_df_hist.ISO == _c) & (_df_hist.index >= _start) & (_df_hist.index <= _end), "Y_DEN"]).dropna()
    _y.replace(0, 1e-7, inplace=True)

    ## HERE THERE IS A PROBLEM WITH SUB-SECTORS >1: WHEN ADDING 2100
    ## WE GET DIFFERENT x values for 2100
    ## FOR Y values we get the same in 2100, but there is a mismatch with x values:
    # (the lenght of x and y is different) => len(_x)=43, len(_y)=42

    # Print debug info if necessary
    if _c == countrylist[0] and _print:
        try:
            for i in [2010,2100]:
                if i in _y:
                    print(f"this is _y in {i}: ", _y.loc[i])
        except Exception as e:
            print("Error printing _y values: ", str(e))

    # Align _x and _y
    setindex(df_main, "ISO")

    # The below should fix the problem of duplicated x, y values, and aligns the index. (can replace code above)
    mydf=pd.concat([_y.drop_duplicates(), _x.drop_duplicates()], axis=1)
    mydf=mydf.drop(2017) if 2017 in mydf.index else mydf
    mydf=mydf.dropna()
    _y=mydf.iloc[:,0]
    _x=mydf.iloc[:,1]


    # Initialize regression results
    alpha = 0
    beta = 0
    r_squared = np.nan

    
    # Better scaling to find fit
    scale=10 if _func == 's-curve' else 1
    # scale=1
    _y=_y / scale
    
    if len(_x)+len(_y)==0:
        # Return long term projections if there is no historical data
        df_main=df_main.reset_index().set_index('ISO')
        df_main.loc[_c, "ALPHA"] = 0
        df_main.loc[_c, "R_SQUARED"] = 0
        df_main.loc[_c, "BETA"] = 0
        return df_main, df_main.reset_index().set_index(['ISO','TIME']).loc[_c, "ENLONG"]  , r_squared, beta, alpha
    try:
        info_dict=fun_regression(_func, _x, _y)
    except:
        txt2=f" fun_regression(_func, _x, _y) with _func:{_func} _x: {_x.to_dict()} and _y: {_y.to_dict()}"
        raise ValueError(f'{_c}, sector {sectors[_s]} not working for regression, {txt2}')
    # Validate projections
    # pd.concat([pd.Series(fun_regression(_func, _x, _y, True), index=_y.index), pd.Series(_y)], axis=1)
    info_dict=fun_harmonize_alpha(None, _func, _x, _y, info_dict, func_dict)
    beta=info_dict['beta']
    alpha=info_dict['alpha'] if _func in ['linear','lin','log-log'] else np.nan
    r_squared=info_dict['r_squared']
    ei= scale * fun_projections(_func, info_dict,_x).clip(0,1)

    ## Plot to compare
    # import matplotlib
    # matplotlib.use('TkAgg') # not working even with default 'agg'
    # from matplotlib import pyplot as plt
    # # pd.concat([pd.Series(fun_regression(_func, _x, _y, True), index=_y.index), pd.Series(_y)], axis=1).plot()
    # pd.concat([pd.Series(fun_projections(_func, info_dict,_x), index=_y.index), pd.Series(_y)], axis=1).plot()
    # plt.show()

    _y = (ei * _df_hist["Y_DEN"] )

    if _s >= 1:
        df_main["Y_DEN"] = df_main[dict_y_den[sectors[_s]] + "_ENSHORT_REF"]

    # Make calculations for country `_c` in `df_main_country`
    if _func == "lin":
        # Keeps energy intensity above zero
        df_main.loc[_c, "ALPHA"] = alpha
        df_main.loc[_c, "R_SQUARED"] = r_squared
        df_main.loc[_c, "BETA"] = beta
        ei_future = adjust_energy_intensity_for_linear_function(_c, countrylist, df_main).clip(0,1)
    else:
        ei_future=scale*fun_projections(_func, info_dict, df_main["X_NUM"] / df_main["X_DEN"]).clip(0,1)


    # df_main['fit_func']=np.nan
    
    # Create dataframe with country specific info - to be copied to `df_main`
    df_main_country=df_main.copy(deep=True)
    df_main_country['fit_func']=info_dict['fit_func']
    if 'ENSHORT' not in df_main_country.columns:
        df_main_country['ENSHORT']=np.nan
    df_main_country[ "ENSHORT"]=ei_future * df_main["Y_DEN"]
    
    timelist=df_main.TIME.unique()
    df_main.set_index("TIME", append=True, inplace=True)
    df_main_country.set_index("TIME", append=True, inplace=True)
    for t in timelist:
        df_main.loc[(_c, t), "ENSHORT"] = df_main_country.loc[(_c, t), "ENSHORT"]
        df_main.loc[(_c, t), "fit_func"] = df_main_country.loc[(_c, t), "fit_func"]
    
    df_main=df_main.reset_index("TIME")

    # Update df_main
    df_main.loc[_c, "ALPHA"] = alpha
    df_main.loc[_c, "R_SQUARED"] = r_squared
    df_main.loc[_c, "BETA"] = beta

    return df_main, _y, r_squared, beta, alpha

def adjust_energy_intensity_for_linear_function(_c: str, countrylist: List[str], df_main: pd.DataFrame, adjust_slope: bool = True) -> np.ndarray:
    """
    Adjusts the energy intensity projections based on GDP per capita and ensures they remain positive.
    NOTE This function is intented to be used only when the functional form is `linear`.

    Parameters:
    - _c (str): Current country being processed.
    - countrylist (List[str]): List of all countries within a region.
    - df_main (pd.DataFrame): Main dataframe containing the data.
    - adjust_slope (bool, optional): Whether to adjust the slope (or just clip the data above minimum threshold) 
        to ensure positive energy intensity.  Default is True.

    Returns:
    - np.ndarray: Adjusted energy intensity values.

    Note: This function assumes the existence of the following helper functions:
    - `flexi_fun_lin(alpha, beta, x, _exp)`: Computes a linear function with flexibility for exponential form.
    - `ensure_positive_energy_intensity(df, ei, gdpcap, countries, current_country, gdp_markup)`: Ensures energy intensity remains positive.
    - `smooth_max_quadratic(ei, min_val, threshold)`: Smoothly clips the energy intensity to avoid zero values.

    Example:
    ```python
    adjusted_intensity = adjust_energy_intensity_for_linear_function('USA', ['USA', 'CAN'], df_main)
    print(adjusted_intensity)
    ```
    """
    df_main=df_main.copy(deep=True)
    gdpcap=df_main["X_NUM"] / df_main["X_DEN"]
    ei_future=np.minimum(1, flexi_fun_lin(df_main["ALPHA"], df_main["BETA"], gdpcap, _exp=False))
        
    # Keeps energy intensity above zero.
    if adjust_slope:
        ei_future, df_main =ensure_positive_energy_intensity(df_main, ei_future, gdpcap, countrylist, _c, gdp_markup=1.05)
        # We recalculate the `intensity` with the updated `df_main["BETA"]` (and `df_main["ALPHA"]`)
        ei_future=np.minimum(1, flexi_fun_lin(df_main["ALPHA"], df_main["BETA"], gdpcap, _exp=False))

    # To avoid possible zero data in 2100, we clip the energy intensity using a smooth_max_quadratic() approximation
    # Lowest energy intensity ever reached across al countries is 3.31848065e-05 (BRN in 1979). Calculations can be found here:https://github.com/iiasa/downscaler_repo/issues/201
    ei_future=smooth_max_quadratic(ei_future, 1e-10,3.31848065e-05 ) # lowest EI ever reached

    return ei_future


def step1_harmo_summer(
    _df: pd.DataFrame,
    _var: str,
    _s: int,
    sectors: List[str],
    df_iam: pd.DataFrame,
    region: str,
    _baseyear: Optional[Union[int, str]] = ""
) -> pd.Series:
    """
    Harmonises variable proportionally to match IAM results (keep relative % share across countries unchanged)

    Parameters:
    _df (pd.DataFrame): DataFrame to harmonize.
    _var (str): Variable name to harmonize.
    _s (int): Sector index.
    sectors (List[str]): List of sectors.
    df_iam (pd.DataFrame): DataFrame containing IAM results.
    region (str): Region identifier.
    fun_pd_sel (callable): Function to select data from DataFrame.
    setindex (callable): Function to set index of DataFrame.
    _baseyear (Optional[Union[int, str]]): Base year for harmonization. Default is "".

    Returns:
    pd.Series: Harmonized variable series with "_RATIO" suffix.
    """
    ## new 2020_10_22
    if (
        len(
            fun_pd_sel(
                df_iam[df_iam.VARIABLE == sectors[_s]], "", region, sectors[_s]
            )["VALUE"]
        )
        != 0
    ):
        _ratio = fun_pd_sel(_df, "", "")[_var].groupby("TIME").sum() / (
            fun_pd_sel(
                df_iam[df_iam.VARIABLE == sectors[_s]], "", region, sectors[_s]
            )["VALUE"]
        )

        setindex(_df, "TIME")
        _ratio.index = [
            _ratio.index[i][0] for i in range(0, len(_ratio.index), 1)
        ]  ## fixing index in _ratio

        ## Excluding base year from harmonization (ratio=1)
        if _baseyear != "":
            _ratio[_baseyear] = 1
        try:
            _df.loc[:, _var + "_RATIO"] = _df.loc[:, _var] / _ratio.interpolate(
                method="linear", limit_direction="forward"
            )
        except Exception as e:  ## new 2020_10_22
            print("_ratio:", _ratio)
            print("Error:", str(e))
            _df.loc[:, _var + "_RATIO"] = _df.loc[:, _var] / _ratio

    return _df[_var + "_RATIO"]
    
def step1_bounds_harmo(
    _df: pd.DataFrame,
    _var: str,
    _lower_var: Union[str, int, float],
    _upper_var: Union[str, int, float],
    _s: int,
    df_iam: pd.DataFrame,
    region: str,
    sectors: List[str]
) -> pd.Series:
    """
    This function sets lower/upper bounds on a variable (_var) of a dataframe (_df).
    Then it re-harmonizes the data to match IAM results (for sector _s).

    Parameters:
    _df (pd.DataFrame): DataFrame containing the variable to be bounded and harmonized.
    _var (str): Variable name to be bounded and harmonized.
    _lower_var (Union[str, int, float]): Lower bound value or column name.
    _upper_var (Union[str, int, float]): Upper bound value or column name.
    _s (int): Sector index.
    fun_pd_sel (callable): Function to select data from DataFrame.
    df_iam (pd.DataFrame): DataFrame containing IAM results.
    region (str): Region identifier.
    sectors (List[str]): List of sectors.

    Returns:
    pd.Series: Harmonized variable series grouped by "TIME".
    """
    # =============================================
    # INTERPRETING UPPER/LOWER DATA
    # =============================================
    if _lower_var == "":
        _lower_var = 0  # WE ASSUME LOWER BOUND IS ZERO (UNLESS OTHERWISE INDICATED)
    else:
        if not isinstance(_lower_var, (int, float)):
            _lower_var = _df[_lower_var]

    if _upper_var == "":
        _upper_var = 1e90  # THIS IS JUST A HIGH NUMBER
    else:
        if not isinstance(_upper_var, (int, float)):
            _upper_var = _df[_upper_var] * 0.95  # we lower the upper bound a bit (e.g. by 5%)

    # =============================================
    # IMPOSING UPPER / LOWER BOUND
    # =============================================
    _df[_var] = _df[_var].clip(_lower_var, _upper_var)

    # =============================================
    # HARMONISATION
    # =============================================
    _country_ratios = (
        fun_pd_sel(_df, "", "")[_var]
        / fun_pd_sel(_df, "", "")[_var].groupby("TIME").sum()
    )  # country share

    _df_enshort_value = (
        fun_pd_sel(df_iam, "", region, sectors[_s])["VALUE"] * _country_ratios
    ).reset_index()  # calculating values + resetting index

    _df_enshort_value = _df_enshort_value[["TIME", "ISO", 0]]  # selecting columns

    _df[_var] = fun_pd_sel(_df_enshort_value, "", "")[0]  # copy new values in the original df

    return _df[_var].groupby("TIME").sum()

def smooth_max_quadratic(x, epsilon=1e-10, lower_bound=1e-5):
    """
    Smoothly approximates the max function using a quadratic approximation.

    Parameters:
    x (array-like): Input array.
    epsilon (float): A small positive constant to ensure smoothness.
    lower_bound (float): The lower bound value.

    Returns:
    numpy.ndarray: Smooth approximation of max(lower_bound, x).
    """
    return (x + lower_bound + np.sqrt((x - lower_bound)**2 + epsilon)) / 2

def fun_drop_columns(df, col_names:Union[list, str] = ["Unnamed"], case_sensitive: bool = False):
    """
    Drops all columns from the DataFrame that contain strings as specified in the column names.
    Example, if `df` contains an `Unnamed 0` column and `col_names=["Unnamed"]`, that column wil be dropped. 

    Parameters:
        df (pd.DataFrame): Input DataFrame.
        col_names (str or list of str): Names of the columns to be dropped. Default is "Unnamed".
        case_sensitive (bool): Flag to indicate whether the search for column names should be case-sensitive.
                               Default is False.

    Returns:
        pd.DataFrame: DataFrame with the specified columns dropped.
    """
    df = df.copy(deep=True)
    if isinstance(col_names, str):
        col_names = [col_names]
    my_list_all = []
    for col_name in col_names:
        if case_sensitive:
            mylist = [x for x in df.columns if col_name in x]
        else:
            mylist = [x for x in df.columns if col_name.lower() in x.lower()]
        my_list_all += mylist
    df = df.drop(columns=my_list_all)

    if len(col_names)!=len(my_list_all) and len(my_list_all)>0:
        print(f'We have dropped: {my_list_all}')
    return df

def ensure_positive_energy_intensity(
    df_main: pd.DataFrame,
    intensity: pd.Series,
    gdpcap: pd.Series,
    countrylist: List[str],
    _c: str,
    gdp_markup:float =1,
) -> pd.Series:
    """
    Update the BETA and ALPHA values in `df_main` for a given country `_c` to ensure energy intensity remains non-negative.
    BETA is calculated so that the energy intensity develops linearly (against GDP per capita), until reaching 
    zero when the GDP per capita is highest.

    This function ensures that the energy intensity for a given country does not become negative by adjusting 
    the BETA and ALPHA values in the main DataFrame. The BETA is set based on the relationship between 
    energy intensity and GDP per capita, ensuring it leads to zero energy intensity at the maximum GDP per capita, 
    multiplied by a `gdp_markup`.

    Parameters:
    - df_main (pd.DataFrame): Main DataFrame containing energy data.
    - intensity (pd.Series): Series of energy intensity values.
    - gdpcap (pd.Series): Series of GDP per capita values.
    - countrylist (List[str]): List of country codes.
    - _c (str): Country code to update.
    - gdp_markup (float): GDP markup (e.g., 1 means energy intensity reaches zero when GDP per capita 
      is equal to its maximum value multiplied by 1), by default 1.

    Returns:
    - pd.Series: Updated intensity values.
    - pd.DataFrame: Updated main DataFrame with adjusted BETA and ALPHA values.

    The rationale behind this function is to ensure energy intensity develops linearly with GDP per capita,
    avoiding any negative energy consumption values. The BETA is calculated to keep the energy intensity
    always positive, with the lower possible value being zero.
    """
    # Calculate min_beta for each country
    min_beta = {c: -intensity.xs(c).max() / (gdpcap.xs(c).max()*gdp_markup - gdpcap.xs(c).min()) for c in countrylist}
    
    # Update BETA for the specific country
    df_main.loc[_c, 'BETA'] = max(pd.Series(min_beta).loc[_c], df_main.loc[_c, 'BETA'].unique()[0])

    # Check if min_beta is being used and update ALPHA if necessary
    if pd.Series(min_beta).loc[_c] == df_main.loc[_c, 'BETA'].unique()[0]:
        gdpcap_2010 = df_main[df_main.TIME == 2010]["X_NUM"] / df_main[df_main.TIME == 2010]["X_DEN"]
        df_main.loc[_c, 'ALPHA'] = intensity.loc[_c].iloc[0] - gdpcap.xs(_c).iloc[0] * min_beta[_c]

    # Recalculate the intensity with updated BETA and ALPHA
    intensity = np.minimum(1, flexi_fun_lin(df_main["ALPHA"], df_main["BETA"], gdpcap, _exp=False))
    
    return intensity, df_main

def evaluate_local_variables(items, local_vars):
    """
    Evaluate the items in the given list using the provided local variables and print the results for defined items only.
    
    Parameters:
    - items (list): A list of variable names as strings to be evaluated.
    - local_vars (dict): Dictionary of local variables to use for evaluation.
    
    Returns:
    - None
    """
    result = {}
    
    for x in items:
        try:
            result[x] = eval(x, {}, local_vars)
        except NameError:
            continue
    
    # Filter out None or False values and print the results
    return ({k: v for k, v in result.items() if v})
    # print('\n')

def correct_gdp_and_ei(df_graph:pd.DataFrame, country_gdpcap:pd.Series, country_ei:pd.Series, sector:str)->tuple:
    """
    Perform GDP correction to match 2010 data and adjust GDP per capita.
    If sector=="Final Energy" will also adjust the energy Intensity (`country_ei`) (because it depends on GDP).

    Parameters:
    - df_graph (pd.DataFrame): DataFrame containing GDP data.
    - country_gdpcap (pd.Series): Series of country GDP per capita.
    - country_ei (pd.Series): Series of country Energy Intensity.
    - sector (str): Sector name, e.g., 'Final Energy'.

    Returns:
    - tuple: Corrected country_gdpcap and country_ei.
    """
    # GDP correction to match 2010 data
    if 'GDPCAP' in df_graph.index:
        deflator = (df_graph[df_graph.TIME == 2010]['GDPCAP'] / country_gdpcap.loc[2010]).dropna()
    else:
        gdpcap=df_graph[df_graph.TIME == 2010]['GDP|PPP']/df_graph[df_graph.TIME == 2010]['Population']
        deflator = (gdpcap / country_gdpcap.loc[2010]).dropna()
    # Drop 'TIME' level if present
    if 'TIME' in country_gdpcap.index.names:
        country_gdpcap = country_gdpcap.droplevel('TIME')
    if 'TIME' in country_ei.index.names:
        country_ei = country_ei.droplevel('TIME')

    # GDP Correction affects  GDP per capita
    country_gdpcap = country_gdpcap * deflator
    # Enegy Intensity adjustment (if sector=="Final Energy")
    if sector == 'Final Energy':
        country_ei = country_ei / deflator

    return country_gdpcap, country_ei

def find_intersection_among_list_of_lists(sets_list:Union[List[list], List[set]])->set:
    """
    Finds the intersection of all sets in the given list.

    Parameters:
    sets_list (Union[List[list], List[set]]: A list containing sets to be intersected.

    Returns:
    set: A set containing elements that are common to all sets in the input list.
    """
    error_message='This function only works for a List[list] and for List[set]'
    if not isinstance(sets_list, list):
        raise ValueError(f'{error_message}. You passed a: {type(sets_list)}')  
    
    # Turns list of lists into a list of sets e.g. [[a,b], [b,c]] => [{a,b}, {b,c}]
    max_nests=find_max_nesting(sets_list)
    if max_nests!=2:
        raise ValueError('This function only works for a List[list] and for List[set]')
    sets_list2=[set(x) for x in sets_list]
    return set.intersection(*sets_list2)

def find_max_nesting(lst):
    """
    Finds the maximum nesting level of a list.

    Parameters:
    lst (list): A list which may contain nested lists.

    Returns:
    int: The maximum nesting level required to flatten the list.
    """
    def get_nesting_level(item):
        if isinstance(item, list):
            return 1 + max(get_nesting_level(subitem) for subitem in item)
        return 0

    return get_nesting_level(lst)

def fix_scenarios(df:pd.DataFrame, df_iam:pd.DataFrame, myscen:str, refscen:str='h_cpol', dt=5 )->pd.DataFrame:
    """
    Fixes scenario `myscen` to a reference scenario `refscen` by matching selected time periods 
    (until two scenarios match) and updates the DataFrame accordingly.

    Parameters:
    df(pd.DataFrame): DataFrame containing the downscaled data.
    df_iam (pd.DataFrame): DataFrame containing the IAM data.
    myscen (str): Scenario to be fixed (e.g.'o_1p5c').
    refscen (str, optional): Reference scenario to be used for fixing. Default is 'h_cpol'.
    dt (int, optional): Time steps present in the `df`, by default 5.
    
    Returns:
    pd.DataFrame: Updated DataFrame with fixed scenarios.
    """
    
    # Check if selected scenarios are available:
    checkdict={'df':df, 'df_iam':df_iam}
    for k,v in checkdict.items():
        avscen=v.reset_index().SCENARIO.unique()
        for scen in myscen, refscen:
            if scen not in avscen:
                raise ValueError(f'{scen} scenario is not available in {k}')
    
    # Check if the two scenarios coincide (if so we return the original scenario)
    if myscen==refscen:
        return fun_xs(df, {'SCENARIO': myscen})     
    
    # Define variables and parameters
    myvars = df.reset_index().VARIABLE.unique()
    
    # Copy the `df` dataframe:
    df = df.copy(deep=True)

    # Copy and preprocess the IAM DataFrame
    df_iam = df_iam.copy(deep=True)
    df_iam = fun_index_names(df_iam, True, int)
    df_iam = fun_xs(df_iam, {'VARIABLE': list(myvars)})

    # Determine the selected time periods where the scenarios match
    seltime = []
    for t in range(df.columns.min(), df.columns.max() + dt, dt):
        try:
            assert_frame_equal(df_iam.xs(refscen, level='SCENARIO')[[t]], 
                               df_iam.xs(myscen, level='SCENARIO')[[t]])
            
            seltime.append(t)
        except:
            break
    
    # Return original scenario if data from IAMs differ for all time periods 
    if len(seltime)==0:
        print(f'{myscen} never matches {refscen}')
        return fun_xs(df, {'SCENARIO': myscen})
    print(f'Fixing {myscen} for {seltime}')

    # Determine the common indices between the two scenarios in the `df`
    df1=fun_xs(df, {'SCENARIO': myscen})
    df2=fun_xs(df, {'SCENARIO': refscen}).rename({refscen: myscen})
    myindex=find_common_indices_in_a_list_of_dataframes([df1,df2 ])


    # Fix the DataFrame with the matched scenarios for the selected time periods `seltime`
    df_fixed = fun_xs(df, {'SCENARIO': refscen}).rename({refscen: myscen}).loc[myindex, seltime]
    df.loc[df_fixed.index, df_fixed.columns] = df_fixed

    return df.loc[df_fixed.index]



def find_common_indices_in_a_list_of_dataframes(dfs):
    """
    Determines the common indices among multiple DataFrames.

    Parameters:
    dfs (list of pd.DataFrame): List of DataFrames to find common indices.

    Returns:
    list: List of common indices among all provided DataFrames.
    """
    if not dfs:
        return []

    # Check if dataframes have the same index names:
    list_of_index_names=[]
    for d in dfs:
        list_of_index_names.append(list(d.index.names))
    not_in_common=find_difference_among_list_of_lists(list_of_index_names)
    if len(not_in_common)>0:
        txt='Dataframe do not have the same df.index.names.'
        raise ValueError(f'{txt} {not_in_common} not present in a at least dataframe')
    
    # Initialize the common indices set with the indices of the first DataFrame
    common_indices = set(dfs[0].index)

    # Iterate through the rest of the DataFrames and find common indices
    for df in dfs[1:]:
        common_indices &= set(df.index)

    return list(common_indices)


def find_difference_among_list_of_lists(sets_list: Union[List[list], List[set]]) -> set:
    """
    Finds the elements that are not common to all sets in the given list.

    Parameters:
    sets_list (Union[List[list], List[set]]): A list containing sets to find the difference.

    Returns:
    set: A set containing elements that are not common to all sets in the input list.
    """
    error_message = 'This function only works for a List[list] and for List[set]'
    if not isinstance(sets_list, list):
        raise ValueError(f'{error_message}. You passed a: {type(sets_list)}')  
    
    # Turns list of lists into a list of sets e.g. [[a,b], [b,c]] => [{a,b}, {b,c}]
    max_nests = find_max_nesting(sets_list)
    if max_nests != 2:
        raise ValueError('This function only works for a List[list] and for List[set]')
    sets_list2 = [set(x) for x in sets_list]
    
    # Find the intersection
    intersection = set.intersection(*sets_list2)
    
    # Find the union
    union = set.union(*sets_list2)
    
    # The elements that are not in the intersection
    difference = union - intersection
    
    return difference

def step1_fun_enshort_ei(_alpha, _beta, _x_num, _x_den, _func="log-log"):
    """
    Calculates short-term trends or indicators based on previously estimated parameters.

    Parameters:
    _alpha (pd.Series): Constant term.
    _beta (pd.Series): Slope term.
    _x_num (pd.Series): Numerator, usually GDP.
    _x_den (pd.Series): Denominator, usually population.
    _func (str): Functional form, either "log-log" or "lin".

    Returns:
    pd.Series: Calculated indicator values.
    """
    try:
        zero_alpha_countries = _alpha[_alpha == 0].loc[2010].index
        zero_beta_countries = _beta[_beta == 0].loc[2010].index
        zero_alpha_beta_countries = set(zero_alpha_countries).intersection(zero_beta_countries)
    except:
        zero_alpha_beta_countries = []

    if _func == "log-log":
        _ei = np.exp(_alpha + _beta * np.log(_x_num / _x_den)).clip(0, 1)
    elif _func == "lin":
        _ei = np.exp(_alpha + _beta * (_x_num / _x_den)).clip(0, 1)
    else:
        raise ValueError("This function only works with _func='log-log' or 'lin'")

    _ei = pd.DataFrame(_ei)
    _ei.loc[_ei.index.get_level_values(1).isin(zero_alpha_beta_countries), 0] = 0
    return _ei[0]

def step1_init_right_list_2100(df:pd.DataFrame, gdp:str, pop:str, max_n:int=6):
    """
    Creates a list of the largest countries based on cumulative GDP, population, 
    and base-year energy consumption. It selects up to max_n countries for each criterion.
    The final list is created by combining these criteria and removing duplicates.

    Parameters:
    df (pd.DataFrame): The input dataframe containing country data.
    gdp (str): Column name for GDP data.
    pop (str): Column name for population data.
    max_n (int): Maximum number of countries to select for each criterion. Default is 6.

    Returns:
    list: A list of country ISO codes representing the largest countries by the given criteria.
    """
    df.copy(deep=True)
    setindex(df, "ISO")
    # We do not consider `problematic_countries` with BETA==0 and R_SQUARED ==0 as this hints at problemst with the regression estimates
    # E.g. BETA ==0 Implies high energy consumption (constant energy instensity)
    problematic_countries=df[(df.BETA==0)&((df.R_SQUARED==0))&((df.ALPHA==0))].index
    
    df["GDP"] = df[gdp]
    df["POP"] = df[pop]
    
    # Calculate cumulative GDP and population
    df["GDPCUM"] = df.groupby(df.index)["GDP"].transform('sum')
    df["POPCUM"] = df.groupby(df.index)["POP"].transform('sum')

    # Create lists sorted by cumulative GDP and population
    gdp_list = df.dropna().sort_values("GDPCUM", ascending=True).index.unique().tolist()
    pop_list = df.dropna().sort_values("POPCUM", ascending=True).index.unique().tolist()

    # Add energy consumption data
    df["ENSHORT_INIT"] = df["ENSHORT"]
    en_list = df[df["TIME"] == 2010].dropna().sort_values("ENSHORT_INIT", ascending=True).index.unique().tolist()
    en_list_2100 = df[df["TIME"] == 2100]["ENSHORT"].dropna().sort_values()[-max_n:].index.unique().tolist()

    # Combine lists and remove duplicates
    right_list = list(set(gdp_list[-max_n:] + en_list[-max_n:] + en_list_2100 + pop_list[-max_n:]))

    # Remove problematic countries:
    right_list=[x for x in right_list if x not in problematic_countries]

    # Remove countries with no data in 2010
    for country in right_list.copy():
        if df.loc[(df.index == country) & (df["TIME"] == 2010), ["GDPCUM", pop, "POPCUM", "ENSHORT"]].min().min() == 0:
            right_list.remove(country)

    setindex(df, "ISO")

    return right_list



def step1_reg_length_slope(
    df: pd.DataFrame, 
    num: str, 
    den: Union[str, bool], 
    start: int, 
    end: int, 
    sect: int = 0, 
    n_min: int = 4, 
    func_type: str = 'log-log'
) -> Union[int, float]:
    """
    Finds the 'optimal' starting time in a regression to maximize the R-squared value multiplied 
    by the number of observations. Returns the starting time associated with this optimal value.

    Parameters:
    df (pd.DataFrame): The input dataframe (should contain a 'TIME' column).
    num (str): Numerator of the dependent variable in the regression.
    den (Union[str, bool]): Denominator of the dependent variable in the regression.
                            Set to False if not applicable.
    start (int): Initial observation date (e.g. 1972).
    end (int): Last observation date (e.g. 2017).
    sect (int, optional): Sector identifier (default is 0).
    n_min (int, optional): Minimum number of observations to include in the regression (default is 4).
    func_type (str, optional): Type of regression function ('log-log' or 'lin', default is 'lin').

    Returns:
    Union[int, float]: The optimal starting time for the regression or 1900.0 in case of an error.

    Example:
    A time series with data from 1990-2010:
    This function can reduce by 50% the number of observations (e.g., a starting date of 2000 instead of 1990), 
    only if the R-squared will (at least) double.
    """
    try:
        t = start
        r_list = []
        t_list = []
        obj_list = []
        x_col = "X"
        df[x_col] = df["X_NUM"] / df["X_DEN"]

        while t < end - n_min:
            x = df[(df.TIME >= t) & (df.TIME <= end)][x_col]
            if den is False:
                y = df[(df.TIME >= t) & (df.TIME <= end)][num]
            else:
                y = df[(df.TIME >= t) & (df.TIME <= end)][num] / df[(df.TIME >= t) & (df.TIME <= end)][den]

            n = end - t
            # `all_stats` below already considers the functional form
            # scale=1
            scale=10 if func_type == 's-curve' else 1
            all_stats=fun_regression(func_type, x, y/scale)
            r, slope =all_stats['r_squared'], all_stats['beta']
            
            # Calculate R-squared multiplied by the number of observations
            obj = r * n
            obj = r * n if obj >= 1e-70 else 0

            r_list.append(r)
            t_list.append(t)
            obj_list.append(obj)

            t += 1

        idx = [i for i, x in enumerate(obj_list) if x == max(obj_list)]
        opt_time = t_list[idx[0]] if len(idx)>0 else 0

    except Exception as e:
        print(f"Error: {e}")
        opt_time = 1900.0

    return opt_time


def step1_load_ssp_data(input_file: Path, project: str, model_patterns: List[str], ssp_model: str, ssp_scenario: str, df_iam_all: pd.DataFrame, pop: str, gdp: str) -> Tuple[pd.DataFrame, pd.DataFrame, bool, bool]:
    """
    Loads the SSP data from a CSV file, checks for missing data, appends old projections if necessary, 
    and processes the data according to the specified model and scenario.

    This function handles the loading of SSP data by checking for updated projections,
    appending older projections if needed, and verifying the availability of scenarios.
    If the specified SSP model or scenario is not available, it flags the need to use IAM
    models or scenarios instead. It also processes the data by melting it and selecting
    based on specified variables.

    Parameters:
    - input_file (Path): Path to the input file.
    - project (str): The project directory.
    - model_patterns (List[str]): List of model patterns to match.
    - ssp_model (str): The SSP model to use.
    - ssp_scenario (str): The SSP scenario to use.
    - df_iam_all (pd.DataFrame): DataFrame containing IAM results.
    - pop (str): The variable name for population.
    - gdp (str): The variable name for GDP.

    Returns:
    - pd.DataFrame: DataFrame containing the loaded SSP data.
    - pd.DataFrame: DataFrame containing the melted and processed SSP data.
    - bool: Flag indicating if the SSP model needs to be changed.
    - bool: Flag indicating if the SSP scenario needs to be changed.

    Raises:
    - ValueError: If some scenario data are missing or multiple compatible scenarios are found.
    """
    
    ssp_data_file = Path(project) / "SspDb_country_data_2013-06-12.csv"
    m = ["TIME", "ISO"]
    
    # Check if updated data is available
    if os.path.exists(input_file.parents[0] / "SSP_projections.csv"):
        read_data_from=input_file.parents[0] / "SSP_projections.csv"
        df_ssp = pd.read_csv(read_data_from)
        all_scen = set(df_ssp.SCENARIO)
        for variable in ["Population", "GDP|PPP"]:
            av_scen = set(df_ssp[df_ssp.VARIABLE == variable].SCENARIO)
            if all_scen ^ av_scen:
                raise ValueError(f"Some scenario data are missing in SSP_projections.csv for {variable}: {(all_scen ^ av_scen)}")
            if variable not in df_ssp.VARIABLE.unique():
                df_ssp = fun_index_names(df_ssp, True, int)
                append = pd.read_csv(ssp_data_file, sep=",", encoding="utf-8")
                append = fun_index_names(append, True, int).xs(variable, level="VARIABLE", drop_level=False)
                df_ssp = pd.concat([df_ssp, append], axis=0)
                df_ssp = df_ssp.reset_index()
        
        # List of models
        models = df_iam_all.MODEL.unique().tolist()
        if "Reference" in models:
            models.remove("Reference")
        models = [model for model in models if match_any_with_wildcard(model, model_patterns)]
        # Check for missing variables in SSP data
        for m in models:
            # We just check for countries that are available in `default_mapping.csv` for a given model
            # Reason: for some project (e.g. ECEMF) we only downscale a few countries (e.g. EU27 countries)- > this should not result in an error
            check_countries= list(set(fun_flatten_list(fun_regional_country_mapping_as_dict(m, input_file.parents[0]).values()))&(set(check_IEA_countries))) 
            fun_check_missing_variables(fun_xs(fun_index_names(df_ssp, True, int), {"MODEL":[m]}),["Population","GDP|PPP"], check_countries, 
                                        coerce_errors=False) # Can be optionally set to True

    else:
        read_data_from=ssp_data_file
        df_ssp = pd.read_csv(read_data_from, sep=",", encoding="utf-8")
    
    df_ssp.rename(columns={"REGION": "ISO"}, inplace=True)

    change_ssp_model = False
    if ssp_model not in df_ssp.MODEL.unique():
        change_ssp_model = True  # will use IAM model instead
    
    change_ssp_scenario = False
    if ssp_scenario not in df_ssp.SCENARIO.unique():
        if not change_ssp_model:
            available_scenarios = [x for x in df_ssp[df_ssp.MODEL == ssp_model].SCENARIO.unique() if ssp_scenario in x]
            if len(available_scenarios) != 1:
                raise ValueError(f"We found multiple SSP scenarios compatible with {ssp_scenario}: {available_scenarios}."
                                 f"We read data from: {read_data_from}."
                                 f"Because this file does not exists: {os.path.exists(input_file / '../SSP_projections.csv')}"
                                 f"These are the files available in that folder: {os.listdir(input_file.parents[0])}"
                                 )
            change_ssp_scenario = True  # will use IAM scenario instead

    # SSP selection based on variable chosen
    df_ssp = df_ssp.loc[(df_ssp.VARIABLE == pop) | (df_ssp.VARIABLE == gdp)]

    setindex(df_ssp, False)

    # Melting the SSP df
    df_ssp_melt = df_ssp.melt(id_vars=["SCENARIO", "VARIABLE", "MODEL", "UNIT", "ISO"])
    df_ssp_melt.rename(columns={"variable": "TIME", "value": "VALUE"}, inplace=True)
    df_ssp_melt["TIME"] = pd.to_numeric(df_ssp_melt["TIME"])

    # SSP data selection (MODEL AND SCENARIO)
    if os.path.exists(input_file / "../SSP_projections.csv"):
        selmodels=[ssp_model]+models
    else:
        selmodels=[ssp_model]
    df_ssp_melt = df_ssp_melt[df_ssp_melt.MODEL.isin(selmodels)]
    setindex(df_ssp_melt, ['TIME','ISO'])  # Updating variable name
    
    return df_ssp, df_ssp_melt, change_ssp_model, change_ssp_scenario


def step1_load_iea_data(
    df_iea_h: Optional[pd.DataFrame], 
    historic_data_file: Path, 
    country_mapping_file: Path,  
    iea_data_file: Path, 
    iea_fuel_file: Path,
    gdp: str
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Load / Processes IEA data by reading historical data, renaming columns, reading country mappings,
    IEA fuel dictionary, and IEA data, and computing GDP per capita.

    Parameters:
    - df_iea_h (Optional[pd.DataFrame]): DataFrame containing historical data. If None, it will be read from the specified file.
    - historic_data_file (Path): Path to the historical data file.
    - country_mapping_file (Path): Path to the country mapping file.
    - iea_data_file (Path): Path to the IEA data file.
    - iea_fuel_file (Path): Path to the IEA fuel dict file.
    - gdp (str): The GDP column name in the historical data.

    Returns:
    - Tuple[pd.DataFrame, pd.DataFrame, Dict[str, str]]: Tuple containing the historical data DataFrame, IEA data DataFrame, and country dataframe.
    """

    # Reading historical data => df_iea_h
    if df_iea_h is None:
        df_iea_h = pd.read_csv(
            historic_data_file,
            index_col=["ISO"],
            sep=",",
            encoding="utf-8",
        )

    df_iea_h.rename(columns={"COUNTRY": "COUNTRY"}, inplace=True)
    df_countries = fun_read_df_countries(country_mapping_file)
    
    # IEA fuel dict (from IEA fuel name to standardized fuel names)
    IEA_fuel_dict = fun_read_IEA_fuel_dict(iea_fuel_file)
    
    # Reading IEA data
    df_iea_all = fun_read_df_iea_all(iea_data_file)
    df_iea_all["IAM_FUEL"] = df_iea_all["PRODUCT"]
    df_iea_all = df_iea_all.replace({"IAM_FUEL": IEA_fuel_dict["FUEL"]})
    
    # Computing GDP per capita
    df_iea_h["GDPCAP"] = df_iea_h[gdp] / df_iea_h["POPULATION"]
    fun_pd_sel(df_iea_h, "", "")
    return df_iea_h, df_iea_all, df_countries


def step1_get_models(df_iam_all: pd.DataFrame, model_patterns: List[str]) -> List[str]:
    """
    Generates a sorted list of unique model names from the IAM data DataFrame,
    removing the 'Reference' model and filtering based on provided patterns.

    Parameters:
    - df_iam_all (pd.DataFrame): DataFrame containing IAM data with a 'MODEL' column.
    - model_patterns (List[str]): List of model patterns to match.

    Returns:
    - List[str]: Sorted list of filtered model names.
    """
    models = df_iam_all.MODEL.unique().tolist()
    
    if "Reference" in models:
        models.remove("Reference")
    
    models = [model for model in models if match_any_with_wildcard(model, model_patterns)]
    models.sort(reverse=True)
    
    return models

def step1_check_fuel_distribution(
    main_list: List[str], 
    iea_gas: List[str], 
    iea_oil: List[str], 
    iea_coal: List[str], 
    iea_biomass: List[str]
) -> None:
    """
    Checks if all fuels are distributed among energy carriers. Raises an error if there are differences
    between the main and selected list.

    Parameters:
    - main_list (List[str]): Main list of fuels to check against.
    - iea_gas (List[str]): List of gas fuels.
    - iea_oil (List[str]): List of oil fuels.
    - iea_coal (List[str]): List of coal fuels.
    - iea_biomass (List[str]): List of biomass fuels.

    Returns:
    - None: Raises a ValueError if fuels are not well distributed across all lists.

    Raises:
    - ValueError: If there are items in the main list that are not in the selected list,
                  or if there are differences in selected list not in main list.
    """

    # Creating the selected list by intersecting with the main list
    sel_list = (
        list(set(iea_gas).intersection(main_list)) +
        list(set(iea_oil).intersection(main_list)) +
        list(set(iea_gas).intersection(main_list)) +
        list(set(iea_coal).intersection(main_list)) +
        list(set(iea_biomass).intersection(main_list))
    )

    # Printing the differences
    res1=set(main_list) - set(sel_list)
    if len(res1)>0:
        raise ValueError(f"Differences in main list not in selected list: {res1}" )
    res2=set(sel_list) - set(main_list)
    if len(res2)>0:
        raise ValueError(f"Differences in selected list not in main list: {res2}" )
    print('all good with the fuels')

def step1_get_sectors(df_iam_all_models: pd.DataFrame,
                               split_res_and_comm: bool = False) -> List[str]:
    """
    Checks and returns availability of sectors in a DataFrame of IAM models' variables.
    Raises a ValueError if any sector from the reference sectors is not found in the DataFrame.

    Parameters:
    - df_iam_all_models (pd.DataFrame): DataFrame containing IAM models' variables.
    - split_res_and_comm (bool, optional): Flag indicating whether to split residential and commercial sectors. Defaults to False.

    Returns:
    - List[str]: List of sectors available in the IAM models' data.

    Raises:
    - ValueError: If any sector from `ref_sectors` is not available in `df_iam_all_models.VARIABLE.unique()`
                and is not the exception "Final Energy|Transportation|Solids".
    """
    ref_sectors = (
        sectors_energy_demand_split_res if split_res_and_comm else sectors_energy_demand
    )

    sectors = [x for x in ref_sectors if x in df_iam_all_models.VARIABLE.unique()]

    for x in ref_sectors:
        if x not in sectors and x != "Final Energy|Transportation|Solids":
            raise ValueError(f"Sector {x} not available from IAMs results")

    return sectors


def find_countries_declining_gdpcap(df_iea_h: pd.DataFrame,
                                    df_countries: pd.DataFrame,
                                    ) -> list:
    """
    Find list of countries with declining GDPCAP (during 2010-1980).

    Parameters:
    - df_iea_h (pd.DataFrame): DataFrame containing IEA data.
    - df_countries (pd.DataFrame): DataFrame containing country information.

    Returns:
    - List[str]: List of countries with declining GDPCAP during 2010-1980.
    """
    print("Declining GDPCAP (during 2010-1980) - list of countries:")
    res=(df_iea_h['GDPCAP'].unstack('TIME')[2010]-df_iea_h['GDPCAP'].unstack('TIME')[1980]).sort_values().dropna()
    res=list(res[res<0].index)
    print(res)
    return res

def step1_short_term_calculations(
    countrylist: List[str], 
    df_main: pd.DataFrame, 
    df_iea_h: pd.DataFrame, 
    df_iea_melt: pd.DataFrame, 
    sectors: List[str], 
    dict_y_den: Dict[str, str], 
    gdp: pd.Series, 
    pop_iea: pd.Series, 
    iam_base_year: int, 
    _print: bool, 
    func_type: str, 
    end_hist: int, 
    target: str, 
    ref_target: str, 
    convergence: int, 
    countries_incl_2100: List[str], 
    dfa_all: pd.DataFrame, 
    dfa: pd.DataFrame,
    _s: int,
    t: int = 2100
) -> pd.DataFrame:
    """
    Process data for each country in the country list, updating and calculating necessary values.

    Parameters:
    - countrylist (List[str]): List of countries to process.
    - df_main (pd.DataFrame): Main dataframe containing the data.
    - df_iea_h (pd.DataFrame): Dataframe with historical data.
    - df_iea_melt (pd.DataFrame): Melted IEA data.
    - sectors (List[str]): List of sectors.
    - dict_y_den (Dict[str, str]): Dictionary mapping sectors to their respective denominator columns.
    - gdp (pd.Series): GDP data.
    - pop_iea (pd.Series): Population data.
    - iam_base_year (int): Base year for IAM.
    - _print (bool): Flag to control printing.
    - func_type (str): Function type for the regression.
    - end_hist (int): End year for historical data.
    - target (str): Target sector or category.
    - ref_target (str): Reference target.
    - convergence (int): Maximum convergence timing.
    - countries_incl_2100 (List[str]): List of countries including year 2100 data.
    - dfa_all (pd.DataFrame): Dataframe containing all sector data.
    - dfa (pd.DataFrame): Dataframe with with specific sector data (sector = `Final Energy`).
    - _s (int): Sector index under consideration
    - t (int): time period when to include long_term regression, by default 2100

    Returns:
    - pd.DataFrame: Updated main dataframe with processed data.
    """
    for c in countrylist:
        try:
            value = fun_pd_sel(df_main, t, c)["ENLONG"].iloc[0]
        except IndexError:
            value = np.nan

        den_value = fun_pd_sel(df_iea_h, t, c)[dict_y_den[sectors[_s]]].iloc[0]

        # Assign values to `df_iea_h`
        for col in [sectors[_s], "Y_NUM", "Y_DEN"]:
            df_iea_h = fun_pd_assign(df_iea_h, t, c, col, value if col != "Y_DEN" else den_value)

        # Determine `start` of data points
        mydata = fun_pd_sel(df_iea_h, "", c)[sectors[_s]]
        mydata = mydata.loc[mydata.index.get_level_values('TIME').isin(list(range(1900,2025)))]
        start = mydata.replace(0, np.nan).dropna().index.get_level_values('TIME').min() or 1990
        setindex(df_iea_h, "ISO")
        hist_data = fun_historic_data(
            "Final Energy", c, df_iea_melt, as_percentage=True
        ).loc[1900:2010]

        hist_year_start = hist_data[hist_data == 0].index.get_level_values(0).max() if not hist_data.empty else 1900

        try:
            start = max(1900, hist_year_start, step1_reg_length_slope(
                df_iea_h.loc[c], "Y_NUM", "Y_DEN", start, end_hist, _s, n_min=15, func_type=func_type))
        except:
            start = max(1990, hist_year_start)
            if _print:
                print("Error in finding step1_reg_length_slope! Using 1990 as default.")

        end = t if c in countries_incl_2100 else end_hist

        # Perform calculations and update `df_main`
        df_main, y_hist, r_squared, beta, alpha = step1_enshort_calc_summer(
            start, end, func_type, _s, c, countrylist, sectors, dict_y_den, df_main, 
            df_iea_h, gdp, pop_iea, _base_year=iam_base_year, _print=_print
        )

        if beta >= 20:
            end = t
            df_main, y_hist, r_squared, beta, alpha = step1_enshort_calc_summer(
                start, t, func_type, _s, c, countrylist, sectors, dict_y_den, df_main, 
                df_iea_h, gdp, pop_iea, _base_year=iam_base_year, _print=_print
            )

        df_main.loc[df_main.index.get_level_values("ISO") == c, "HIST_START_YEAR"] = start

        if target == ref_target:
            y_min, y_max = 2040, convergence
            y_max = convergence
            y_min = 2040

            x_min = 0.3 * (end_hist - 1990)
            x_max = 1 * (end_hist - 1979)

            slope_tc = (y_max - y_min) / (x_max - x_min)
            intercept_tc = y_min - slope_tc * x_min

            df_main["MAX_TC"] = y_max
            df_main.loc[
                (df_main["BETA"] * df_main["BETA_ENLONG"] <= 0),
                "MAX_TC",
            ] = y_min

            df_main.loc[
                (df_main["BETA"] * df_main["BETA_ENLONG"] >= 0),
                "MAX_TC",
            ] = round(
                intercept_tc + slope_tc * df_main.loc[
                    (df_main["BETA"] * df_main["BETA_ENLONG"] >= 0),
                    "R_SQUARED",
                ] * (
                    end_hist - df_main.loc[
                        (df_main["BETA"] * df_main["BETA_ENLONG"] >= 0),
                        "HIST_START_YEAR",
                    ]
                ),
                0,
            ).clip(y_min, y_max)
        else:
            setindex(dfa_all, "ISO")
            df_main["MAX_TC"] = dfa_all.loc[
                (dfa_all.SECTOR == _s) & (dfa_all.TARGET == ref_target),
                "MAX_TC",
            ]
            setindex(dfa, m)

        df_iea_h = fun_pd_assign(df_iea_h, t, c, sectors[_s], value)
        df_main.loc[
            df_main.index.get_level_values("ISO") == c,
            "HIST_START_YEAR",
        ] = start
        df_main.loc[
            df_main.index.get_level_values("ISO") == c,
            "HIST_END_YEAR",
        ] = end


    return df_main

def get_permutations(input_list: List[str]) -> List[List[str]]:
    """
    Generate all possible permutations of a given list.

    Parameters:
    - input_list (List[str]): List of country codes.

    Returns:
    - List[List[str]]: List of all possible permutations of the input list.
    """
    return set(list(itertools.permutations(input_list)))


def split_gains_regions_to_countries(df_non_co2: pd.DataFrame, gwp:str='AR5GWP100') -> pd.DataFrame:
    """
    Splits GAINS regions into individual countries based on extrapolated historical data and appends the results to `df_non_co2`.

    Parameters:
    -----------
    df_non_co2 : pd.DataFrame
        DataFrame containing non-CO2 data from GAINS, including both individual countries and
        country aggregates (regions) such as ['UZB, TKM, TJK'] region.
    gwp: str
        Global Warming Potential (gwp) for retrieving HFCS data from PRIMAP (as they are reported in Gg CO2-equivalent /yr)
    Returns:
    --------
    pd.DataFrame
        DataFrame with GAINS regions split into individual countries and appended to `df_non_co2`.

    Calculation Steps:
    ------------------
    1. **Create a {Country:region} mapping **: 
       - Create dictionary to match individual countries to bigger GAINS region.
       - Example: 'TKM' belongs to the 'UZB, TKM, TJK' region of GAINS

    2. **Copy regional results**: 
       - Copy over regional results to individual countries (for the baseline and mitigation scenarios)
       - At this stage all countries will report the same regional results

    3. **Harmonize results**: 
       - Harmonize results so that the sum of each country (e.g. ['UZB', 'TKM', 'TJK']) will match the regional GAINS results
       - Harmonization is done based on historical PRIMAP data (shares of country within the region) in 2020
       - Append the harmonized results to the dataframe

    NOTE: The idea behind this function is to obtain as much country-level data as possible from the GAIN model,
    by downscaling larger region to individual countries. This `augmented` GAINS data will be then used to downscale
    regional IAMs scenarios (using maximum abatement potential from GAINS).

    """
    # Read PRIMAP data
    primap = fun_read_primap(CONSTANTS.INPUT_DATA_DIR)#[[2020]]

    # Check for valid gwp (only for HFCS data, as they are reported in CO2 equivalent) and create `mydict` 
    mylist=[x for x in primap.reset_index().entity.unique() if 'HFC' in x]
    allowed_gwp=[x.replace('HFCS (','').replace(')','') for x in mylist ]
    if gwp not in allowed_gwp:
        raise ValueError(f"gwp should be one of the following: {allowed_gwp}, you passed {gwp}")
    my_gwp_dict = {'HFC': f'HFCS ({gwp})'}

    # Split data: we do the calculations by gases
    for g in ["CH4", "N2O", "HFC", "SF6"]:

        # Slice for relevant data
        hist = primap.xs(['0', 'HISTCR', my_gwp_dict.get(g,g)], level=['category (IPCC2006_PRIMAP)', 'SCENARIO', 'entity']).droplevel('MODEL').droplevel(['UNIT', 'source'])

        if len(hist) > 0:
            mydict = {}
            for gains_countries in df_non_co2.reset_index().REGION.unique():
                if len(gains_countries) > 3:
                    # Step1 - Create dictionary to match individual countries to bigger GAINS region.
                    list1 = [gains_countries]  # All countries are in one list of len()==1 (This is the region name of PRIMAP)
                    list2 = fun_flatten_list([x.split(',') for x in list1])  # Separate out different countries
                    list3 = [x.replace(' ', '') for x in list2]  # Same as list2, but without blank space
                    list3 = list(set(all_countries)&set(list3)) # Keep only ISO3 names
                    if len(list3)>0:
                        res = {x.replace(' ', ''): list1 for x in list2}  # Result
                        mydict.update(res)

                        # Step2 - Copy over regional results to individual countries in `append` dataframe.
                        # At this stage all countries will report the same regional results
                        append = pd.concat([fun_xs(df_non_co2, {'REGION': v}).rename({v[0]: k}) for k, v in res.items()])
                        append = append.xs(g, level='VARIABLE', drop_level=False)
                        countries = fun_xs(pd.DataFrame(hist), {'REGION': list3})
                        countries=countries[list(range(2000,2021))]
                        countries=extrapolate_and_extend_dataframe(countries, base_year=2020, final_year=2051)

                        # Step3 - Harmonize `append` results so that the sum of each country (e.g. ['UZB', 'TKM', 'TJK']) will match the regional GAINS results
                        # Harmonization is done based on historical data (shares of country within the region)
                        shares = (countries / countries.sum())
                        append = fun_add_multiply_dfmultindex_by_dfsingleindex(append, shares, operator='*')

                        # Step4 - Append the data to `df_non_co2`
                        df_non_co2 = pd.concat([df_non_co2, append])

    return df_non_co2

def fun_extend_time_colunmns(df: pd.DataFrame, years_to_extend: range = range(2020, 2051)) -> pd.DataFrame:
    """
    Extend the DataFrame columns by adding new columns for specified years.

    Parameters:
    - df (pd.DataFrame): Input DataFrame with historical data.
    - years_to_extend (range, optional): Range of years to extend the DataFrame columns to. 
      Default is range(2020, 2051).

    Returns:
    - pd.DataFrame: The DataFrame with extended columns up to the specified years, filled with NaN values.

    Example:
    ```python
    import pandas as pd
    df = pd.DataFrame({
        'MODEL': ['Model1', 'Model1', 'Model2', 'Model2'],
        'SCENARIO': ['Scenario1', 'Scenario2', 'Scenario1', 'Scenario2'],
        'REGION': ['Region1', 'Region1', 'Region2', 'Region2'],
        '2000': [1, 2, 3, 4],
        '2005': [2, 3, 4, 5],
        '2010': [3, 4, 5, 6]
    }).set_index(['MODEL', 'SCENARIO', 'REGION'])

    df_extended = fun_extend_time_colunmns(df, years_to_extend=range(2015, 2051))
    print(df_extended)
    ```

    This will extend the DataFrame columns to include years from 2015 to 2050, filled with NaN values, 
    and reorder the columns to be in chronological order. Time values reported as strings (e.g. '2010') 
    will be converted to integers (e.g. 2010)
    """
    df=df.copy(deep=True)

    # Covert df.columns into integers
    df=fun_index_names(df, True, int)

    # Extend the DataFrame columns
    for year in years_to_extend:
        df[(year)] = np.nan

    # Reorder columns to ensure they are in chronological order
    df = df.reindex(sorted(df.columns, key=lambda x: int(x)), axis=1)
    return df

def extrapolate_and_extend_dataframe(df: pd.DataFrame, base_year: int = 2020, final_year: int = 2051) -> pd.DataFrame:
    """
    Extrapolate and extend a DataFrame using linear trends.

    This function performs the following steps:
    1. Calculates the slopes for each country based on historical data.
    2. Extends the DataFrame with missing time columns up to the final year.
    3. Extrapolates values linearly based on the calculated slopes.
    4. Adjusts the extrapolated values so they match the base year's data.
    5. Combines the historical data with the extrapolated data.

    Parameters:
    - df (pd.DataFrame): Input DataFrame with historical data. The DataFrame should have a multi-index with columns representing years.
    - base_year (int, optional): The base year from which to start the extrapolation. Default is 2020.
    - final_year (int, optional): The final year to extend the DataFrame to. Default is 2051.

    Returns:
    - pd.DataFrame: The extended DataFrame with extrapolated values.

    Example:
    ```python
    import pandas as pd
    df = pd.DataFrame({
        'MODEL': ['Model1', 'Model1', 'Model2', 'Model2'],
        'SCENARIO': ['Scenario1', 'Scenario2', 'Scenario1', 'Scenario2'],
        'REGION': ['Region1', 'Region1', 'Region2', 'Region2'],
        '2005': [1, 2, 3, 4],
        '2010': [2, 3, 4, 5],
        '2015': [3, 4, 5, 6],
        '2020': [4, 5, 6, 7]
    }).set_index(['MODEL', 'SCENARIO', 'REGION'])

    final_df = extrapolate_and_extend_dataframe(df, base_year=2020, final_year=2051)
    print(final_df)
    ```
    """
    df=df.copy(deep=True)
    df=fun_index_names(df, True, int)
    # Calculate the slopes for each country
    slopes = fun_slope_over_time_iamc_format(df)

    # Extend the DataFrame with missing time columns
    extended: pd.DataFrame = fun_extend_time_colunmns(df, years_to_extend=range(base_year, final_year))
    extended = extended[list(range(base_year, final_year))]

    # Extrapolate values based on the slopes
    if len(df.index.names)>1:
        # We found a multindex => Extrapolation will be model and scenario specific
        fun_check_iamc_index(df)
        extrapolated=pd.DataFrame()
        for model in df.reset_index().MODEL.unique():
            for scenario in df.reset_index().SCENARIO.unique():
                temp =pd.DataFrame(
                    [{k: v * t for k, v in slopes[model][scenario].items()} for t in extended.columns],
                    index=extended.columns
                ).T
                temp.index.names=['REGION']
                temp['MODEL']=model
                temp['SCENARIO']=scenario
                extrapolated=pd.concat([extrapolated, temp.reset_index().set_index(df.index.names)])
    else:
        # Single index (we do not distinguish across models and scenarios)
        extrapolated = pd.DataFrame(
            [{k: v * t for k, v in slopes.items()} for t in extended.columns],
            index=extended.columns
        ).T

    # Adjust the intercept to match the base year's data
    intercept = df[base_year] - extrapolated[base_year]
    final = pd.DataFrame({t: extrapolated[t] + intercept for t in extended.columns}).clip(0)

    # Combine the historical data with the extrapolated data
    final = pd.concat([df, final[list(range(base_year + 1, final_year))]], axis=1)
    
    return final

def fun_regression(func_type: str, x: Union[List[float], pd.Series, np.ndarray], y: Union[List[float], pd.Series, np.ndarray], predict: bool = False) -> Union[Dict[str, Union[float, object]], np.ndarray]:
    """
    Perform a regression analysis on the given data, based on a specified function type.

    Parameters:
    - func_type (str): The type of regression function to use. Must be one of 'linear', 'log-log', 's-curve', or 'logistic'. 
      NOTE:`logistic` is the same as `s-curve`.
    - x (Union[List[float], pd.Series, np.ndarray]): The independent variable data.
    - y (Union[List[float], pd.Series, np.ndarray]): The dependent variable data.
    - predict (bool, optional): Whether to return the predicted values based on the regression model. Default is False.

    Returns:
    - Union[Dict[str, Union[float, object]], np.ndarray]: If `predict` is True, returns the predicted values as a numpy array.
      If `predict` is False, returns a dictionary containing the fitted regression function (`fit_func`), the R-squared value (`r_squared`),
      the beta coefficient (`beta`), and the alpha coefficient (`alpha` if applicable).

    Example:
    ```python
    from fit_funcs import func_dict  # Assuming func_dict is defined in fit_funcs.py

    x = [1, 2, 3, 4, 5]
    y = [2, 4, 6, 8, 10]
    
    regression_result = fun_regression("linear", x, y)
    print(regression_result)
    ```

    Note: The `func_dict` should contain the regression function classes. Each regression function class should have `fit`, `r_squared`, `beta`,
    `alpha` attributes or methods, and a `predict_y` method.
    """
    allowed_func_type=["linear","log-log", 's-curve', 's_curve',  "logistic", "lin"] 
    if func_type not in allowed_func_type:
        txt_note='NOTE: `s-curve` and `logistic` refer to the same function '
        raise ValueError(f"func_type should be one of the following: {allowed_func_type}, you passed {func_type}. {txt_note}")
    # Alias name: 
    alias_dict = {'s-curve': 'logistic','s_curve':'logistic', 'lin':'linear'}
    func_type=alias_dict.get(func_type, func_type)
    info_dict = {"fit_func": func_dict[func_type]()}
    info_dict["fit_func"].fit(pd.Series(x), pd.Series(y))
    info_dict["r_squared"] = info_dict["fit_func"].r_squared
    info_dict["beta"] = info_dict["fit_func"].beta
    if func_type in ["linear", 'log-log']:
        info_dict['alpha'] = info_dict["fit_func"].alpha
    if predict:
        return func_dict[func_type].predict_y(info_dict["fit_func"], np.array(x))
    return info_dict

def fun_projections(func_type:str, info_dict:dict, x:Union[List[float], pd.Series, np.ndarray]) -> np.ndarray:
    """
    Generate projections based on the specified function type and provided fit information.

    Parameters:
    - func_type (str): The type of regression function to use for projections. Should be one of 'linear', 'log-log', or 's-curve'.
      NOTE: 's-curve' is treated as 'logistic'.
    - info_dict (Dict[str, Union[float, object]]): Dictionary containing information about the fit function, including the fitted function object.
    - x (Union[List[float], pd.Series, np.ndarray]): The independent variable data for which projections are to be made.

    Returns:
    - np.ndarray: The projected values based on the regression model.

    Example:
    ```python
    from fit_funcs import func_dict  # Assuming func_dict is defined in fit_funcs.py

    # Example usage
    x = [1, 2, 3, 4, 5]
    y = [2, 4, 6, 8, 10]
    info_dict = fun_regression("linear", x, y)
    projections = fun_projections("linear", info_dict, np.array(x))
    NOTE: this should coincide with `fun_regression("linear", x, y, predict=True)` (unless you change the alpha to match the base year data)
    ```

    Note: The `func_dict` should contain the regression function classes. Each regression function class should have a `predict_y` method.
    """
    if func_type=='s-curve':
        func_type='logistic'
    elif func_type=='lin':
        func_type='linear'
    return func_dict[func_type].predict_y(info_dict["fit_func"], x)

def fun_harmonize_alpha(t: Optional[int], 
                        _func: str, 
                        _x: pd.Series, 
                        _y: pd.Series, 
                        info_dict: Dict[str, Union[float, object]], 
                        func_dict: Dict[str, object],
                        base_year:int =2010
                        ) -> Dict[str, Union[float, object]]:
    """
    Harmonize the intercept (alpha) parameter in the `info_dict` based on the specified time `t` and update the fit function.

    Parameters:
    - t (Optional[int]): The year to use for harmonization. If None, it will default to the maximum between 2010 and the minimum available year in `_y`.
    - _func (str): The type of function being used. Should be one of 'log-log', 'lin', or 's-curve'.
    - _x (pd.Series): The independent variable data.
    - _y (pd.Series): The dependent variable data.
    - info_dict (Dict[str, Union[float, object]]): Dictionary containing information about the fit function, including 'alpha' and 'beta'.
    - func_dict (Dict[str, object]): Dictionary containing the regression function classes.
    - base_year: The base year in your data (default is 2010)

    Returns:
    - Dict[str, Union[float, object]]: The updated info_dict with harmonized alpha and updated fit function.
    """
    info_dict=info_dict.copy()
    if t is None:
        # This means that we get the base_year (e.g. 2010) data if data is available.
        # If data at the base year is `np.nan` or equal to zero, we look for the first available data point (e.g. 2015 or 2020)
        t = max(base_year, _y.replace(0, np.nan).dropna().index.min())

    alias_dict= {'s-curve': 'logistic', 'lin':'linear'}
    if t in list(_y.index):
        if _func == 'log-log':
            info_dict['alpha'] += np.log(_y.loc[t]) - (info_dict['alpha'] + np.log(_x.loc[t]) * info_dict['beta'])
            info_dict['fit_func'] = func_dict['log-log'](alpha=info_dict['alpha'], beta=info_dict['beta'])
        elif _func == 'linear':
            info_dict['alpha'] += _y.loc[t] - (info_dict['alpha'] + _x.loc[t] * info_dict['beta'])
            info_dict['fit_func'] = func_dict['linear'](alpha=info_dict['alpha'], beta=info_dict['beta'])
        elif _func == 'logistic':
            info_dict['alpha_harm'] = _y.loc[t] - fun_projections('logistic', info_dict,_x).loc[t]
            info_dict['fit_func'] = func_dict['logistic'](gamma=info_dict['fit_func'].A+info_dict['alpha_harm'], alpha=info_dict['fit_func'].K, beta=info_dict['fit_func'].beta, x_0=info_dict['fit_func'].x_0, )#fun_projections('logistic', info_dict,_x)
            # info_dict['fit_func'] = func_dict['logistic'](gamma=info_dict['fit_func'].A, alpha=info_dict['fit_func'].K, beta=info_dict['fit_func'].beta, x_0=info_dict['fit_func'].x_0,alpha_harm=info_dict['alpha_harm'] )#fun_projections('logistic', info_dict,_x)
    else:
        print(f"Unable to harmonize alhoa: {t} is not available in _y.index: {list(_y.index)}")
    return info_dict

def step1_select_sectors(n_sectors: Union[None, int, float, str, List[str]], sectors: List[str]) -> Tuple[List[str], int]:
    """
    Determine and return the relevant sectors based on the input criteria.

    Parameters:
    - n_sectors (Union[None, int, float, str, List[str]]): The criteria for selecting sectors. This can be:
        - None: Selects all available sectors.
        - int or float: Selects the first x number of sectors from the list.
        - str: If a string is provided, treats it as a single sector name to be included.
        - List[str]: A list of sector names to be included, potentially with recursive dependencies.
    - sectors (List[str]): The list of available sectors.

    Returns:
    - Tuple[List[str], int]: A tuple containing:
        - A list of selected sectors based on the provided criteria.
        - An integer representing the count of selected sectors.

    Notes:
    - If `n_sectors` is a list, the function will recursively add dependent sectors to the list,
      ensuring that all necessary hierarchical sectors are included.
    - The function uses a maximum of three recursion steps to resolve sector dependencies.

    Example:
    ```python
    sectors = ["Final Energy|Residential", "Final Energy|Commercial", "Final Energy|Gases"]
    selected_sectors, count = step1_get_sectors(["Final Energy|Gases"], sectors)
    print(selected_sectors, count)
    ```
    """
    if n_sectors is None or isinstance(n_sectors, int) or isinstance(n_sectors, float) :
        last_s = len(sectors) if n_sectors is None else n_sectors
    elif isinstance(n_sectors, str):
        sectors=[sectors]
    if isinstance(n_sectors, list):
        mycount=0
        while mycount<3:
            # Recursevely add sectors to the list (e.g. 'Final Energy|Residential and Commercial|Gases' requires 'Final Energy|Gases', and 'Final Energy')
            n_sectors=[v for k,v in dict_y_den.items() if k in n_sectors]+n_sectors
            mycount+=1
        # Below we keep the order of the sectors
        sectors=[x for x in sectors if x in n_sectors]
        last_s = len(sectors)
    return sectors,last_s


def fun_get_scenarios_enhanced(project: str, missing: bool = False, by_model: bool = True) -> list:
    """
    Retrieves a list of scenarios for a given `project`. The scenarios can be filtered by models (`by_model=True`), 
    and optionally return a list of missing scenarios for each model (`missing=True`).	

    Parameters
    ----------
    project : str
        The name of the project for which to retrieve the scenario data.
    missing : bool, optional
        If True, returns the list of scenarios missing for each model (default is False).
        When True, the function returns the scenarios that are missing for each model 
        compared to the full set of scenarios across all models.
    by_model : bool, optional
        If True, returns the scenarios organized by model (default is True). 
        If False and `missing=False`, returns a flat list of all unique scenarios across all models.

    Returns
    -------
    list or dict
        If `missing=True`, returns a dictionary where the keys are models and the values are lists of scenarios that are missing for each model.
        If `missing=False` and `by_model=True`, returns a dictionary where keys are models and values are lists of scenarios available for each model.
        If `missing=False` and `by_model=False`, returns a flat list of all unique scenarios across all models.

    Notes
    -----
    - When `missing=True`, the option `by_model=False` is not available because 
      missing scenarios are calculated per model.
    - The list of all scenarios is based on the `df_iam` dataframe, which is derived 
      from the `fun_read_df_iams` function.
    """
    # Read `df_iam` snapshot for all models
    df_iam = fun_read_df_iams(project, None)
    
    # List of models
    models = df_iam.reset_index().MODEL.unique()
    
    # List of all scenarios available across all models
    scenarios = df_iam.reset_index().SCENARIO.unique()

    if missing:
        if not by_model:
            print('For `missing=True`, the option `by_model=False` is not available.'
                 'Reason: we assume you want to know which scenarios are missing for each model ' 
                 '(Otherwise your request does not make any sense, considering that the list of all scenarios is taken from the `df_iam`)')
        # Return the list of missing scenarios for each model
        return {m: list(set(fun_xs(df_iam, {'MODEL': m}).reset_index().SCENARIO.unique()) ^ set(scenarios)) for m in models}
        
    else:
        # Return the list of scenarios present for each model
        scenarios = {m: list(set(fun_xs(df_iam, {'MODEL': m}).reset_index().SCENARIO.unique()) & set(scenarios)) for m in models}
        
        if by_model:
            return scenarios
        else:
            # Return a flat list of all unique scenarios across models
            return list(set(fun_flatten_list(scenarios.values())))
        

def fun_compare_NGFS_rounds(fname1: str, fname2: str, drop: dict = None, keep: dict = None):
    """
    Compares two datasets (csv files) from different NGFS rounds by reading and processing the files, 
    applying optional filtering, and identifying major percentage differences between the datasets.
    It also plots the key discrepancies found between the two datasets.

    Parameters
    ----------
    fname1 : str
        Path to the first CSV file (NGFS round 1 dataset) to be compared.
    fname2 : str
        Path to the second CSV file (NGFS round 2 dataset) to be compared.
    drop : dict, optional
        Dictionary specifying elements to drop from both datasets. The keys in the dictionary 
        represent index levels, and values represent items to exclude from those levels. Default is None.
    keep : dict, optional
        Dictionary specifying elements to keep in both datasets. The keys in the dictionary 
        represent index levels, and values represent items to include in those levels. Default is None.

    Raises
    ------
    ValueError
        If the filtering results in no data remaining in either of the datasets after applying 
        the drop/keep filters.

    Returns
    -------
    None
        The function does not return anything, but generates comparison plots that highlight 
        the maximum and minimum percentage differences between the datasets across specific variables, regions, and scenarios.

    Notes
    -----
    - The function reads two datasets and drops the `MODEL` index level to allow for cross-model comparisons.
    - The optional `drop` argument is used to exclude specific data, while `keep` is used to retain only certain data.
    - It checks for percentage deviations between the datasets, drops historical data (2010-2025), 
      and identifies the variables, regions, and scenarios with the maximum and minimum deviations.
    - The resulting discrepancies are plotted with customized legends that remove common parts of the file names.
    """
    
    # Read fname1
    df1 = fun_read_csv({'aa': fname1}, True, int)['aa']
    df1 = fun_rename_index_name(df1, {'ISO': 'REGION'}).droplevel('MODEL')  # This allows comparison across models

    # Read fname2
    df2 = fun_read_csv({'aa': fname2}, True, int)['aa']
    df2 = fun_rename_index_name(df2, {'ISO': 'REGION'}).droplevel('MODEL')  # This allows comparison across models

    # Drop elements in the dataframe
    if drop is not None:
        df1 = fun_xs(df1, drop, exclude_vars=True)
        df2 = fun_xs(df2, drop, exclude_vars=True)

    # Keep specific elements in the dataframe
    if keep is not None:
        df1 = fun_xs(df1, keep, exclude_vars=False)
        df2 = fun_xs(df2, keep, exclude_vars=False)

    # Raise an error if no data is found after filtering
    if len(df1) == 0:
        raise ValueError(f'No data found in {fname1} after applying the drop/keep filters')
    if len(df2) == 0:
        raise ValueError(f'No data found in {fname2} after applying the drop/keep filters')
        
    # Check major % differences across the two datasets (min and max percentage deviations)
    check = (df1 / df2).dropna(how='all').replace({np.inf: np.nan, -np.inf: np.nan}).dropna()
    check = check.drop(list(range(2010, 2025, 5)), axis=1)  # Drop historical data to avoid multiple variables
    myindex_max = fun_xs_fuzzy(check, [check.max().max()]).index
    myindex_min = fun_xs_fuzzy(check, [check.min().min()]).index

    # Plot major differences found
    # Custom legend: find the common part of the file names and remove it for brevity in the legend
    for _ in range(2):
        common = find_longest_common_substring(str(fname1), str(fname2))
        fname1 = str(fname1).replace(common, '')
        fname2 = str(fname2).replace(common, '')
    
    mylegend = [fname1, fname2]

    # Plot maximum and minimum discrepancies
    for x in [myindex_max, myindex_min]:
        mydrop = ['VARIABLE', 'UNIT', 'SCENARIO']  # Drop these index levels for plotting
        
        ax = pd.concat([df1.loc[x].droplevel(mydrop), df2.loc[x].droplevel(mydrop)], axis=0).T.plot(label=mylegend, alpha=0.75)
        ax.legend(mylegend)
        
        plt.title(f"{x.get_level_values('VARIABLE')[0]} - {x.get_level_values('REGION')[0]} - {x.get_level_values('SCENARIO')[0]}")

def fun_clip_geo_hydro_step2b(df_desired: pd.DataFrame) -> pd.DataFrame:
    """
    Clips the values of geothermal and hydro energy sources to their 2010 values, ensuring that 
    these energy sources do not go below their 2010 levels. The clipping only applies in cases 
    where regional data are not declining. This function is specifically designed to handle 
    data standardization for geothermal and hydro energy.

    Parameters
    ----------
    df_desired : pd.DataFrame
        A dataframe containing energy data, where 'TIME' and 'ISO' have been set as indexes. 
        The dataframe must include the energy types 'HYDRO' and 'GEO', along with their values 
        for each time period and region.

    Returns
    -------
    pd.DataFrame
        The dataframe with the 'HYDRO' and 'GEO' values clipped to their 2010 levels, 
        ensuring they do not fall below the 2010 values unless the regional data itself 
        shows a decline.

    Notes
    -----
    - The function assumes that time and iso (region) have already been set as indexes 
      in the input dataframe `df_desired`.
    - Clipping is done for the energy types 'HYDRO' and 'GEO' using 2010 values as a lower bound.
    - After clipping, the 'TIME' index is re-set to its original position.
    - This is a step in a larger process for standardizing energy data within regions, ensuring that 
      geothermal and hydro energy data are not unrealistically lower than historical levels.
    """
    # Ensure time and iso (region) are set as indexes
    fun_pd_sel(df_desired, "", "")  # Sets time and iso as indexes

    # List of fuels that cannot go below their 2010 values
    clipped_fuels_2010 = ["HYDRO", "GEO"]

    # Clip HYDRO and GEO to 2010 values (do not allow values to drop below 2010 levels)
    df_desired[clipped_fuels_2010] = df_desired[clipped_fuels_2010].clip(
        df_desired.loc[2010, clipped_fuels_2010], np.inf
    )

    # Re-set the 'TIME' index
    setindex(df_desired, "TIME")  # Puts back the 'TIME' index

    return df_desired


def fun_dynamic_downs(
    myiam: pd.DataFrame,
    mydf: pd.DataFrame,
    mycriteria: pd.DataFrame,
    base_year: int,
    fuel_list: List[str],
    to: int = 2105,
    dt: int = 5,
) -> pd.DataFrame:
    """Dynamic calculations based on the difference of IAMs results 
    (in time t vs the base year). This difference is allocated to the country 
    level using `mycriteria`, and added to the base year data at the country level. 

    Please note that these calculations (difference compared to base year data) 
    are equivalent to recursive calculations year by year only if criteria do not change over time.

    Args:
        myiam (pd.DataFrame): Dataframe with IAM results, expected to contain columns for the fuels.
        mydf (pd.DataFrame): Dataframe with initial data (up to the last available historical year).
        mycriteria (pd.DataFrame): Criteria to allocate IAM results to the country level. 
                                    The sum of country level results should equal 1.
        base_year (int): The base year data (reference time period) to calculate the difference for each time t.
        fuel_list (List[str]): List of fuels (column names) to be processed.
        to (int, optional): Last year from IAMs results plus dt. Defaults to 2105.
        dt (int, optional): Time period difference in IAM results. Defaults to 5.

    Returns:
        pd.DataFrame: Updated downscaled results, with new values computed for each year in the range.
    """
    # Filter dataframes to keep only relevant fuel columns
    myiam = myiam[fuel_list]
    mydf = mydf[fuel_list]
    mycriteria = mycriteria[fuel_list]

    # DIFFERENCE COMPARED TO BASE YEAR
    res = ((myiam - myiam.loc[base_year]) * mycriteria).fillna(0) + mydf.loc[base_year]
    res = res.clip(1e-9, np.inf)  # Clipping values to avoid negative results

    # Standardize the results based on the defined function
    new = fun_standardise_df_desired_within_region(fuel_list, res, myiam)

    # Update the original dataframe with new values for specified years
    mydf.loc[range(base_year, to, dt), fuel_list] = new
    
    return mydf


def fun_dynamic_downs_recursive(
    myiam: pd.DataFrame,
    mydf: pd.DataFrame,
    mycriteria: pd.DataFrame,
    base_year: int,
    fuel_list: list,
) -> pd.DataFrame:
    """
    Perform dynamic downscaling of IAM (Integrated Assessment Model) results to a country level by recursively allocating
    differences between IAM projections and base year data across countries using predefined criteria.

    The difference between the IAM results for a given time period `t` and the base year is allocated to individual
    countries using the criteria provided in `mycriteria`. The difference is added to the base year data for each country. 
    These calculations mimic recursive calculations (year-by-year) only if the allocation criteria remain constant over time.

    Parameters
    ----------
    myiam : pd.DataFrame
        Dataframe containing IAM results over time for various fuels.
    mydf : pd.DataFrame
        Dataframe containing initial country-level data (e.g., historical data) up to the base year.
    mycriteria : pd.DataFrame
        Dataframe with allocation criteria that sum to 1 across countries. Used to distribute IAM changes to the country level.
        The shape and indices should match those of `myiam`.
    base_year : int
        The base year for reference (starting point of comparison).
    fuel_list : list
        List of fuels for which the downscaling process is performed. These fuels must exist as columns in `myiam`, `mydf`, and `mycriteria`.

    Returns
    -------
    pd.DataFrame
        Updated dataframe with downscaled country-level data, harmonized to match IAM results for the given fuels.

    Notes
    -----
    - The calculations involve finding the difference between IAM results at each time `t` and the base year, and distributing 
      these differences to individual countries based on `mycriteria`.
    - The process assumes that `mycriteria` remains constant over time; otherwise, the recursive calculations may not be accurate.
    - The results are further harmonized to ensure that the aggregated country-level data aligns with IAM results at each time step.
    """
    
    # Limit IAM, dataframe, and criteria to the fuel list provided
    myiam = myiam[fuel_list]
    mydf = mydf[fuel_list]
    mycriteria = mycriteria[fuel_list]

    # Copy the original dataframe for recursive updates
    res = mydf.copy(deep=True)

    # Iterate over each time step in IAM data (for years beyond the base year)
    for t in myiam.index.unique():
        if t > base_year:
            # Calculate the difference between the IAM results and the previous time period,
            # and allocate this difference to the country level using the criteria
            res.loc[t, :] = (
                (myiam.diff().loc[t] * mycriteria.loc[t]).fillna(0)
                + res.groupby(["ISO"])  # Group by country (ISO)
                .shift(1)               # Get the value from the previous time period
                .xs(t, drop_level=False)  # Maintain time index for consistency
            ).clip(1e-9, np.inf)  # Clip to avoid extremely small or negative values

    # Harmonize the downscaled results to ensure they match the IAM results at the aggregated level
    ratio = myiam / res.groupby("TIME").sum()
    
    return res * ratio  # Return the harmonized downscaled results


def fun_cost_curve_downs_without_reg_mc(
    countrylist: List[str], 
    df_iam_sel: pd.DataFrame
) -> pd.DataFrame:
    """
    Calculates the annual production cost curves for different energy sources 
    based on IAM selected data.

    This function reads cost and production data for various energy sources, 
    merges them, and computes the annual production by country and time.

    Args:
        countrylist (List[str]): A list of country ISO codes to be processed.
        df_iam_sel (pd.DataFrame): Dataframe containing selected IAM data.

    Returns:
        pd.DataFrame: A dataframe with calculated annual production for different 
                       fuel types, indexed by TIME and ISO code.
    """
    cost_curve_dict = {
        "SOL": "_PV",
        "BIO": "_1stGen_Bio",
        "WIND": "_onshore_Wind",
        "HYDRO": "HYD_lpjml_gfdl-esm2m_ewembi_rcp26_rcp26soc_co2_qtot_global_daily_2031_2070_merge_monmean_Hydro",
    }
    
    res_dict: Dict[str, Dict[str, float]] = {}
    
    for i, j in cost_curve_dict.items():
        # Read cost and production data files
        df_cost = fun_read_dat_file(f"{CONSTANTS.INPUT_DATA_DIR}/CostCurve{j}.dat")
        df_res = fun_read_dat_file(f"{CONSTANTS.INPUT_DATA_DIR}/MaxProd{j}.dat")
        
        # Filter country columns from production data
        mycols = [c for c in countrylist if c in df_res.columns]
        
        # Merge cost and production data
        df_cost_merged = fun_merge_cost_and_res(i, df_cost, df_res, mycols)

        # Calculate annual production by year
        res_dict = fun_calculate_annual_prod(
            countrylist, df_iam_sel, res_dict, i, mycols, df_cost_merged
        )

    df = pd.DataFrame(res_dict).fillna(0)
    
    # Ensure all fuel types are represented in the dataframe
    flist = ["COAL", "NUC", "GAS", "BIO", "SOL", "WIND", "HYDRO", "OIL", "GEO"]
    for f in flist:
        if f not in df.columns:
            df[f] = np.nan
    
    df.index.names = ["TIME", "ISO"]
    
    return df[flist]


def fun_calculate_annual_prod(
    countrylist: List[str],
    df_iam_sel: pd.DataFrame,
    res_dict: Dict[str, pd.Series],
    i: str,
    mycols: List[str],
    df_cost_merged: pd.DataFrame
) -> Dict[str, pd.Series]:
    """
    Calculates annual production for each country based on IAM selected data 
    and cost-merged data for a specific energy source.

    This function iterates over the indices of the IAM selected dataframe, 
    computes country-level production using the given cost data, and then 
    harmonizes the production results to ensure they are non-declining over time.

    Args:
        countrylist (List[str]): A list of country ISO codes for which 
                                  production is calculated.
        df_iam_sel (pd.DataFrame): Dataframe containing selected IAM data 
                                    indexed by time.
        res_dict (Dict[str, pd.Series]): A dictionary to store 
                                          calculated production results.
        i (str): The key representing the specific energy source in 
                  the production calculation.
        mycols (List[str]): A list of country columns to be used in the 
                            production results.
        df_cost_merged (pd.DataFrame): A dataframe that contains merged cost 
                                        and production data.

    Returns:
        Dict[str, pd.Series]: Updated results dictionary containing 
                               calculated production for the specified energy 
                               source, organized as a stacked Series.
    """
    res_list = []
    
    # Print the merged cost dataframe for debugging purposes
    print(df_cost_merged[df_cost_merged.index < 2].reset_index().to_dict())
    
    for t in df_iam_sel.index:
        # Get production at the country level based on regionally aggregated production
        country_prod = fun_get_country_prod_from_reg_prod(
            df_iam_sel.loc[t, i], df_cost_merged, countrylist
        )
        res_list.append(country_prod)

    df_prod = pd.DataFrame(res_list, index=df_iam_sel.index)

    # Harmonize to match IAM results, ensuring that production data does not decline
    df_prod = fun_harmonize_while_trying_avoiding_decline_prod(
        df_iam_sel, i, mycols, df_prod
    )
    
    # Adjust the production DataFrame to match country columns
    df_prod_adj = df_prod.copy()
    df_prod_adj.columns = mycols
    
    # Stack the adjusted production data and store it in the results dictionary
    res_dict[i] = df_prod_adj.stack()

    return res_dict


def fun_harmonize_while_trying_avoiding_decline_prod(
    df_iam_sel: pd.DataFrame,
    i: str,
    mycols: List[str],
    df_prod: pd.DataFrame,
    max_iter: Union[range, List[int]] = range(1, 5),
    dt: int = 5
) -> pd.DataFrame:
    """
    Harmonizes production data to ensure it does not decline over time and 
    matches IAM results.

    This function updates the production values so that renewable energy 
    generation will never decrease over time. It then scales the production 
    values to align with the total IAM results for the specified energy source.

    Args:
        df_iam_sel (pd.DataFrame): Dataframe containing selected IAM data 
                                    indexed by time.
        i (str): The key representing the specific energy source in 
                  the harmonization process.
        mycols (List[str]): A list of country columns used in the 
                            production results.
        df_prod (pd.DataFrame): Dataframe containing production data to 
                                be harmonized.
        max_iter (Union[range, List[int]], optional): The maximum number of 
                                                       iterations for 
                                                       harmonization. 
                                                       Defaults to range(1, 5).
        dt (int, optional): The time difference between production entries. 
                            Defaults to 5.

    Returns:
        pd.DataFrame: Harmonized production data that does not decline over 
                       time and matches IAM results.
    """
    for _ in max_iter:
        # Update results to ensure renewables generation never declines
        for t in df_prod.index:
            if t > df_prod.index[0]:
                df_prod.loc[t] = np.maximum(df_prod.loc[t], df_prod.loc[t - dt])

        # Harmonize results to match IAM results
        ratio = df_iam_sel[i] / df_prod.sum(axis=1)
        df_prod = pd.concat([df_prod[c] * ratio for c in mycols], axis=1)
        df_prod.columns = mycols
        
    return df_prod


def fun_merge_cost_and_res(
    i: str,
    df_cost: pd.DataFrame,
    df_res: pd.DataFrame,
    mycols: List[str]
) -> pd.DataFrame:
    """
    Merges cost and resource data into a single DataFrame at the country level.

    This function calculates the production by share based on the given 
    cost and resource data, converts production units, and sorts the 
    resulting DataFrame by cost.

    Args:
        i (str): The key representing the specific energy source for which 
                  the cost and resource data are being merged.
        df_cost (pd.DataFrame): Dataframe containing cost data indexed by 
                                different energy sources.
        df_res (pd.DataFrame): Dataframe containing resource data indexed by 
                               different energy sources.
        mycols (List[str]): A list of country columns used for merging.

    Returns:
        pd.DataFrame: A merged DataFrame containing cost and annual production 
                       data, indexed by regional production.
    """
    # Country level resources by share
    df_res_by_share = pd.concat([df_res[mycols] * x for x in df_cost.index])
    df_res_by_share.index = df_cost.index

    df_res_by_share = (
        df_res_by_share[mycols]
        .melt()
        .rename({"value": "prod", "variable": "ISO"}, axis=1)
    )
    df_cost = (
        df_cost[mycols].melt().rename({"value": "cost", "variable": "ISO"}, axis=1)
    )

    # Merged Dataframe with cost and annual production
    df_cost_merged = pd.concat([df_res_by_share, df_cost["cost"]], axis=1)

    # Convert Production to EJ/yr
    df_cost_merged.loc[:, "prod"] = df_cost_merged.loc[:, "prod"] * max_prod_res_unit[i]

    # Sort values by cost
    df_cost_merged = df_cost_merged.sort_values(["cost", "prod"])

    # Add regional production
    df_cost_merged["reg_prod"] = df_cost_merged["prod"].cumsum()
    df_cost_merged = df_cost_merged.set_index("reg_prod")
    
    return df_cost_merged


def fun_all_criteria(
    ssp: Any,
    m: Any,
    cost_curve_list: List[str],
    df_gov_all: pd.DataFrame,
    countrylist: List[str],
    df_iam_sel: pd.DataFrame,
    df_demand: pd.DataFrame,
    df_iam: pd.DataFrame,
    df_desired: pd.DataFrame,
    df_base_year_share: pd.DataFrame,
    df_last_year_share: pd.DataFrame,
    fuel_list: List[str],
    file_name_dict: Dict[str, str],
    lifetime_dict: Dict[str, int],
) -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    Combines various criteria to produce a dataframe of desired values for energy resources.

    This function processes different criteria such as cost curves, stranded assets, 
    base year fuel shares, and governance indicators. The results are combined 
    into a dataframe for further analysis.

    Args:
        ssp (Any): Shared socioeconomic pathways data or object (exact type depends on the implementation).
        m (Any): Model parameter or object (exact type depends on the implementation).
        cost_curve_list (List[str]): List of cost curve identifiers.
        df_gov_all (pd.DataFrame): Dataframe containing governance data for all countries.
        countrylist (List[str]): List of country ISO codes.
        df_iam_sel (pd.DataFrame): Dataframe containing IAM selected data.
        df_demand (pd.DataFrame): Dataframe containing demand data.
        df_iam (pd.DataFrame): Dataframe containing IAM data.
        df_desired (pd.DataFrame): Dataframe to hold the desired results.
        df_base_year_share (pd.DataFrame): Dataframe with base year fuel shares.
        df_last_year_share (pd.DataFrame): Dataframe with last year fuel shares.
        fuel_list (List[str]): List of fuels (column names) to be processed.
        file_name_dict (Dict[str, str]): Dictionary mapping fuel names to file names.
        lifetime_dict (Dict[str, int]): Dictionary mapping asset types to their lifetimes.

    Returns:
        Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:
            - pd.DataFrame: Dataframe containing governance indicators.
            - Dict[str, pd.DataFrame]: Dictionary of dataframes containing weighted criteria.
    """
    ## Criteria 1: Cost curves
    df_cost_criteria = fun_cost_curve_downs_without_reg_mc(countrylist, df_iam_sel)

    ## Criteria 2: Stranded assets
    df_gw_all_fuels = fun_stranded_assets(
        ["GEO", "OIL", "COAL", "GAS"], ["OPR", "PLN", "CON"], countrylist, lifetime_dict
    )

    ## Criteria 3: Base year fuel share (within region)
    df_base_year_share, df_last_year_share = fun_baseyear_criteria(
        countrylist, df_base_year_share, df_last_year_share
    )

    # Criteria 4: Governance
    df_gov = fun_governance_criteria(ssp, m, df_gov_all, countrylist, fuel_list)

    print("====================================================================")
    print('Combining weighted criteria into dataframe "df_desired"')
    print("====================================================================")

    df_cost_criteria = df_cost_criteria.groupby(["TIME", "ISO"]).sum()

    # Normalize the criteria
    df_criteria_not_weighted = {
        "df_cost_criteria": df_cost_criteria.fillna(1e-9).clip(1e-9)
        / df_cost_criteria.fillna(1e-9).clip(1e-9).groupby("TIME").sum(),
        "df_gw_all_fuels": df_gw_all_fuels.fillna(1e-9).clip(1e-9)
        / df_gw_all_fuels.fillna(1e-9).clip(1e-9).groupby("TIME").sum(),
        "df_base_year_share": df_base_year_share,  # Unit: % within region
        "df_gov": df_gov.fillna(1e-9).clip(1e-9)
        / df_gov.fillna(1e-9).clip(1e-9).groupby("TIME").sum(),  # Governance indicators
    }

    return df_gov, df_criteria_not_weighted



def fun_make_step2b_input_data(
    var: str,
    file: str,
    region: str,
    model: str,
    target: str,
    file_suffix: str,
    df_iam_all_models: pd.DataFrame,
    df_weights: pd.DataFrame,
    pyam_mapping_file: str,
    PREV_STEP_RES_DIR: str,
    file_name_dict: dict,
    sectors: list,
    fuel_list: list,
    IAM_fuel_dict: dict,
    df_iea_all: pd.DataFrame,
    df_platts: pd.DataFrame,
    base_year: int,
    _func: callable,
):
    """
    This function prepares input data for the second step (Step 2B) of the analysis process. 
    It reads historical data, International Energy Agency (IEA) data, and Integrated Assessment Model (IAM) results.
    It also manages the selection of sectors, regions, and fuels required for downstream processing.

    Parameters
    ----------
    var : str
        The variable name, typically representing the type of energy data (e.g., 'Final Energy|Electricity').
    file : str
        Filename or identifier of the input data file.
    region : str
        Region of interest (e.g., 'Europe', 'Asia', etc.).
    model : str
        IAM model being used (e.g., 'IMAGE', 'REMIND').
    target : str
        The target metric or scenario for processing (e.g., '2C scenario').
    file_suffix : str
        Suffix for file names to specify scenario variations.
    df_iam_all_models : pd.DataFrame
        DataFrame containing IAM results across all models.
    df_weights : pd.DataFrame
        DataFrame containing weights used for allocating IAM results to country-level data.
    pyam_mapping_file : str
        Path to the mapping file used for linking model regions to country groups.
    PREV_STEP_RES_DIR : str
        Directory containing results from the previous steps of the analysis.
    file_name_dict : dict
        Dictionary containing filenames or paths to various input files.
    sectors : list
        List of sectors being analyzed (e.g., 'Electricity', 'Industry').
    fuel_list : list
        List of fuel types to include in the analysis.
    IAM_fuel_dict : dict
        Dictionary mapping IAM fuels to their respective categories.
    df_iea_all : pd.DataFrame
        DataFrame containing historical data from IEA.
    df_platts : pd.DataFrame
        DataFrame containing energy data from Platts (e.g., electricity generation).
    base_year : int
        Base year for the analysis (used as a reference point for comparing changes).
    _func : callable
        A function passed as an argument to be used for specific custom calculations in the workflow.

    Returns
    -------
    tuple
        Returns a tuple of several dataframes and lists required for the next steps in the analysis:
        (df_iea_all, df_platts, m, cost_curve_list, df_gov_all, countrylist, df_iam_sel, df_demand, df_iea_melt, df_iam, t_graph_list).

    Notes
    -----
    - The function reads and processes IEA, IAM, and regional data, adjusting the model mapping, sectors, and fuels to match the 
      region of interest.
    - The `fun_step2b_read_csv_data` function is called to handle IEA data and country lists.
    - Additional filtering, renaming, and index setting operations are performed on the IAM and historical data.
    """
    
    # Defining the index list for the dataframes
    m = ["TIME", "ISO"]
    print(model, region, target, var, file)

    # Read and process necessary input data from various sources (e.g., IEA, Platts, etc.)
    (
        df_iea_all,
        df_countries,
        fuel_dict,
        cost_curve_list,
        df_gov_all,
    ) = fun_step2b_read_csv_data(df_iam_all_models, df_iea_all, file_name_dict)

    # Check validity of the weights dataframe
    fun_check_df_weights(df_weights)
    setindex(df_weights, "IAM_FUEL")

    # The base year share is calculated for countries within a region for each fuel (e.g., UK consumes 10% of regional coal consumption)
    region_name = fun_region_name(model, region)
    sel_sectors = fun_sel_sectors_elc("Electricity", sectors)
    
    # Load model mapping between regions and countries based on the specified model
    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    # Create a deep copy of the IAM results dataframe
    df_iam_all = df_iam_all_models.copy(deep=True)
    setindex(df_iam_all, False)

    # Reload the sector selection for electricity
    sel_sectors = fun_sel_sectors_elc("Electricity", sectors)
    df_countries, regions = load_model_mapping(model, df_countries, pyam_mapping_file)

    print("====================================================================")
    print("Reading historical data and IAM results:")
    print("====================================================================")
    
    # Call the function to select the necessary variable and energy types
    (
        countrylist,
        df_iam_sel,
        max_year,
        df_demand,
        df_iea_melt,
        df_platts,
        _fuel_list,
    ) = fun_selected_var_secondary_energy(
        fuel_list,
        var,
        df_iam_all,
        region_name,
        target,
        IAM_fuel_dict,
        df_countries,
        sel_sectors,
        file_suffix,
        PREV_STEP_RES_DIR,
        df_iea_all,
        df_platts,
        model,
        fuel_dict,
        _func=_func
    )

    # Rename columns for consistency
    df_demand.rename(columns={"Final Energy|Electricity": var}, inplace=True)
    df_iam = df_iam_sel
    setindex(df_gov_all, "TIME")

    # Adding 2100 data to df_gov by renaming 2099 to 2100
    df_gov_all.rename(index={2099: 2100}, inplace=True)

    # Define a list of years from IAM models, starting from the base year
    t_graph_list = list(
        df_iam_all_models[df_iam_all_models.TIME >= base_year].dropna().TIME.unique()
    ).copy()

    return (
        df_iea_all,
        df_platts,
        m,
        cost_curve_list,
        df_gov_all,
        countrylist,
        df_iam_sel,
        df_demand,
        df_iea_melt,
        df_iam,
        t_graph_list,
    )


def fun_static_downscaling(
    _df_all_criteria: pd.DataFrame,
    base_year: int,
    countrylist: list,
    df_iam_sel: pd.DataFrame,
    df_iam: pd.DataFrame,
    time_periods: list,
    df_desired: pd.DataFrame,
    df_base_year_share: pd.DataFrame,
    df_last_year_share: pd.DataFrame,
    fuel_list: list,
) -> pd.DataFrame:
    """
    This function performs static downscaling of electricity generation based on predefined criteria weights. 
    Unlike dynamic approaches, no time-period differences (`df_iam_sel.diff()`) are considered, making the allocation 
    of electricity generation to countries proportional to the predefined criteria.

    The result is a downscaled electricity generation DataFrame (`df_desired`) for each country in `countrylist` that 
    sums up to match the regional Integrated Assessment Model (IAM) results.

    Args:
        _df_all_criteria (pd.DataFrame): DataFrame containing the initial criteria for downscaling at the country level.
        base_year (int): The reference year for the downscaling process (e.g., 2010).
        countrylist (list): List of countries for which the electricity generation needs to be downscaled.
        df_iam_sel (pd.DataFrame): IAM results at the regional level by fuel (electricity generation).
        df_iam (pd.DataFrame): Full IAM results including time periods and regions.
        time_periods (list): List of time periods considered for the downscaling process.
        df_desired (pd.DataFrame): DataFrame for storing the desired downscaled electricity generation (initially copied from `_df_all_criteria`).
        df_base_year_share (pd.DataFrame): Electricity generation share of each country within the region for the base year (e.g., 2010).
        df_last_year_share (pd.DataFrame): Electricity generation share of each country within the region for the last historical year (e.g., 2016).
        fuel_list (list): List of fuel types (e.g., ['COAL', 'GAS', 'OIL', 'RENEWABLE']) for which the downscaling is performed.

    Returns:
        pd.DataFrame: The downscaled electricity generation DataFrame (`df_desired`) at the country level, 
                      standardized to match the IAM results for each region. This DataFrame adds up to the regional 
                      IAM results for each fuel and time period.

    Notes:
        - The downscaled results ensure that the sum of electricity generation for all countries in a region matches 
          the IAM regional results.
        - The function `fun_standardise_df_desired_within_region` ensures that the downscaled data matches the IAM data 
          for each region by normalizing the country-level data.
        - The function `fun_check_reg_results` is used to verify that the downscaled results match the regional IAM results.
    """

    # Create a deep copy of _df_all_criteria to avoid modifying the original dataframe
    df_desired = _df_all_criteria.copy(deep=True)
    
    # Check if the criteria (df_desired) adds up to one (i.e., sum of country shares should be 1 for each fuel)
    fun_check_adding_up_to_one(df_desired[fuel_list])

    # Standardize df_desired to ensure it matches the IAM regional results for each fuel and region
    df_desired = fun_standardise_df_desired_within_region(fuel_list, df_desired, df_iam)

    # Add missing historical data for countries without any available data in the base year and last year
    df_base_year_share = fun_add_countries_wo_hist_data(countrylist, df_base_year_share, fuel_list)
    df_last_year_share = fun_add_countries_wo_hist_data(countrylist, df_last_year_share, fuel_list)

    # Perform the head operation on df_desired based on base year, last year share, and IAM selected data
    df_desired = fun_head(
        df_desired,
        df_base_year_share,
        df_last_year_share,
        df_iam_sel,
        fuel_list,
        [2020, 2015, 2010],
    )

    # Re-standardize df_desired to match the IAM results for the selected region
    df_desired = fun_standardise_df_desired_within_region(fuel_list, df_desired, df_iam)

    # Check whether df_desired matches the regional IAM results for consistency
    fun_check_reg_results(df_iam_sel, df_desired, fuel_list)
    
    return df_desired

def fun_step2b_read_csv_data(
    df_iam_all_models: pd.DataFrame, 
    df_iea_all: pd.DataFrame, 
    file_name_dict: dict
) -> Union[pd.DataFrame, list, dict]:
    """
    Reads and processes various CSV files required for step 2b in the energy modeling pipeline.
    The function reads country mappings, fuel dictionaries, cost curve data, and governance data.
    It also processes IAM and IEA data for further use.

    Args:
        df_iam_all_models (pd.DataFrame): DataFrame containing IAM model results for all models.
        df_iea_all (pd.DataFrame): DataFrame containing IEA data.
        file_name_dict (dict): Dictionary containing file paths for required CSV files.

    Returns:
        Tuple:
        - pd.DataFrame: Processed IEA data with standard fuel names added.
        - pd.DataFrame: DataFrame containing country mappings.
        - dict: Dictionary mapping Platts fuel names to standard fuel names.
        - list: List of cost curve fuel types (e.g., ['SOL', 'WIND', 'HYDRO', 'BIO']).
        - pd.DataFrame: Governance data required for the modeling.
    """
    # Reading country-region mapping data
    df_countries = fun_read_df_countries(
        CONSTANTS.INPUT_DATA_DIR / "MESSAGE_CEDS_region_mapping_2020_02_04.csv"
    )  # Regional Mapping Data

    # Reading IEA fuel dictionary to map IEA fuel names to standard fuel names
    IEA_fuel_dict = fun_read_IEA_fuel_dict(
        CONSTANTS.INPUT_DATA_DIR / "IEA_Fuel_dict.csv"
    )  # IEA Fuel Dictionary

    # Reading Platts fuel dictionary for standard fuel name mapping
    df_fuel_dict = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "Fuel_dict.csv", sep=","
    )  # Platts Fuel Dictionary
    setindex(df_fuel_dict, "PLATTS")
    fuel_dict = df_fuel_dict.to_dict()

    # List of fuel types for cost curves (e.g., solar, wind, hydro, biomass)
    cost_curve_list = ["SOL", "WIND", "HYDRO", "BIO"]

    # Create CSV files from .dat files if they are missing
    fun_create_csv_files_from_dat_if_missing(cost_curve_list, file_name_dict)

    # ===================================================
    # Processing IAM and IEA data
    # ===================================================
    
    # Copy IAM data from the models for further processing
    df_iam_all = df_iam_all_models.copy(deep=True)
    
    # Reading governance data
    df_gov_all = fun_read_df_gov()

    # Add IAM_FUEL column with standard fuel names to IEA data
    df_iea_all["IAM_FUEL"] = df_iea_all["PRODUCT"]
    
    # Replacing IEA fuel names with standard IAM fuel names using the fuel dictionary
    df_iea_all = df_iea_all.replace({"IAM_FUEL": IEA_fuel_dict["FUEL"]})

    return df_iea_all, df_countries, fuel_dict, cost_curve_list, df_gov_all

def fun_check_df_weights(df_weights: pd.DataFrame):
    """
    Checks if the sum of criteria weights in `df_weights` adds up to 1 for each row.
    This ensures that the weights for downscaling are valid and properly normalized.

    Args:
        df_weights (pd.DataFrame): DataFrame containing the weight criteria for downscaling,
                                   where each row represents a different set of weights.

    Raises:
        ValueError: If the sum of the weights in any row does not add up to 1, 
                    this exception is raised with information about the discrepancy.

    Returns:
        None: Prints "Weighted Criteria map is OK" if all weights are valid.
    """
    max_sum = round(df_weights.sum(axis=1).max(), 3)
    min_sum = round(df_weights.sum(axis=1).min(), 3)

    # If max and min are equal, the weights are normalized correctly
    if max_sum == min_sum:
        print("Weighted Criteria map is OK")
    else:
        # Raise an error if the weights do not add up to 1
        raise ValueError(
            "Please check your criteria weights: they do not add up to 1"
            f"\n Max: {max_sum}, Min: {min_sum}"
        )


def fun_combine_weighted_criteria(
    df_weights: pd.DataFrame, 
    weighted_criteria: pd.DataFrame
) -> pd.DataFrame:
    """
    Combines weighted criteria based on given weights for each criterion.

    For each criterion, it drops duplicates, multiplies the resulting DataFrame by the corresponding
    weights from `df_weights`, and appends the result to a cumulative DataFrame. Finally, it groups 
    the data by 'TIME' and 'ISO', summing the values.

    Args:
        df_weights (pd.DataFrame): A DataFrame containing weights for each criterion.
        weighted_criteria (pd.DataFrame): A DataFrame containing weighted criteria values.

    Returns:
        pd.DataFrame: A DataFrame containing the combined and summed criteria values, grouped by 'TIME' and 'ISO'.
    """
    df_all = pd.DataFrame()
    cols = list(df_weights.columns)
    for col in cols:
        df_append = fun_drop_duplicates(weighted_criteria[col]) * df_weights[col]
        df_all = df_all.append(df_append)
    return df_all.fillna(0).groupby(["TIME", "ISO"]).sum()


def fun_add_total_demand_other(
    var: str, 
    df_demand: pd.DataFrame, 
    df_all_criteria: pd.DataFrame, 
    fuel_list: List[str]
) -> pd.DataFrame:
    """
    Calculates the total demand and other demand not covered by specific fuel types.

    This function sums the criteria values for specified fuels to calculate total energy production.
    It then calculates the 'OTHER' category, which represents demand not met by the listed fuels.

    Args:
        var (str): The variable name in `df_demand` to extract total demand.
        df_demand (pd.DataFrame): A DataFrame containing demand data.
        df_all_criteria (pd.DataFrame): A DataFrame containing criteria values for various fuels.
        fuel_list (List[str]): A list of fuel types to include in the total demand calculation.

    Returns:
        pd.DataFrame: Updated `df_all_criteria` DataFrame with additional columns for total demand and other demand.
    """
    df_all_criteria["TOTAL"] = df_all_criteria[fuel_list].sum(axis=1)
    df_all_criteria["DEMAND"] = fun_pd_sel(df_demand, "", "")[var]
    df_all_criteria["OTHER"] = df_all_criteria["DEMAND"] - df_all_criteria["TOTAL"]
    df_all_criteria["OTHER"] = df_all_criteria["OTHER"].clip(0, np.inf)
    return df_all_criteria


def fun_governance_criteria(
    ssp: str, 
    m: list, 
    df_gov_all: pd.DataFrame, 
    countrylist: List[str], 
    fuel_list: List[str]
) -> pd.DataFrame:
    """
    Retrieves and prepares governance criteria data for specified countries.

    This function selects governance data for given countries and regions based on the provided 
    scenario (ssp). It adds median values for any missing countries by looking up the existing data
    for the relevant region.

    Args:
        ssp (str): The shared socioeconomic pathway scenario identifier.
        m (list): A list of metrics or measures used for processing.
        df_gov_all (pd.DataFrame): A DataFrame containing governance data for all countries.
        countrylist (List[str]): A list of country identifiers for which governance criteria are requested.
        fuel_list (List[str]): A list of fuel types related to the governance criteria.

    Returns:
        pd.DataFrame: A DataFrame containing the governance criteria for the specified countries.
    """
    # NOTE: Cache strategy here for each region (this function does not seem to be target dependent)
    df_gov = fun_gov_sel(countrylist, ssp, df_gov_all, fuel_list)

    #### Block added: 2021_03_25 Adding median values (in that region) for missing countries
    # a) We search for missing countries in this region
    countries_df_gov_all = df_gov_all[df_gov_all.ISO.isin(countrylist)].ISO.unique()
    missing_countries = list(set(countrylist) - set(countries_df_gov_all))

    # b) Adding regional governance data for missing countries (using median values):
    for i in missing_countries:
        df_temp = df_gov.groupby("TIME").median().copy(deep=True)
        df_temp["ISO"] = i
        setindex(df_temp, m)
        # This will add the data (turn this on when implementing!!!)
        df_gov = df_gov.append(df_temp)
    return df_gov


def fun_sel_sectors_elc(string: str, sectors: list) -> list:
    """
    This function identifies sectors containing a specified substring (`string`) from a given list of sectors (`sectors`).
    It distinguishes between the 'main' sector and its 'sub-sectors' based on the number of delimiters ('|').

    The 'main' sector is defined as the sector with the fewest delimiters, while 'sub-sectors' are the next level sectors 
    that contain one more delimiter than the main sector, to avoid double-counting.

    Args:
        string (str): The substring to search for in the sector names (e.g., "Electricity").
        sectors (list): A list of sector strings, where sectors are delimited by '|'.

    Returns:
        list: A list of two lists:
            - The first list contains the 'main' sector. If no main sector is found, it returns [False].
            - The second list contains the 'sub-sectors' that fall under the main sector.
    """
    
    # Initialize lists to store delimiter counts and matching sectors
    mylist_val = []  # To store the number of delimiters in each sector
    mylist_pos = []  # To store the sectors containing the search string

    # Iterate through the sectors and search for the string
    for s in sectors:
        if string in s:
            # Count the number of delimiters in the sector string
            counter = s.count("|")
            # Add the count and sector to their respective lists
            mylist_val.append(counter)
            mylist_pos.append(s)

    # =========================
    # FINDING THE MAIN SECTOR
    # =========================
    if mylist_val:
        min_val = min(mylist_val)  # Get the minimum number of delimiters

        # Find the position of the main sector (sector with the fewest delimiters)
        res = [i for i, j in enumerate(mylist_val) if j == min_val]

        if len(res) == 1:  # Ensure there's only one 'main' sector
            main_sector = [mylist_pos[res[0]]]  # Select the main sector
            print("MAIN sector:", main_sector)

            # Remove the main sector and its delimiter count from the lists
            mylist_pos.pop(res[0])
            mylist_val.pop(res[0])

            # Find the sub-sectors with exactly one more delimiter than the main sector
            sub_sect_val = [i for i, j in enumerate(mylist_val) if j == min_val + 1]
            sub_sectors = [mylist_pos[i] for i in sub_sect_val]
        else:
            # Multiple potential main sectors found, treat as no main sector
            main_sector = [False]
            sub_sectors = mylist_pos
            print("No MAIN sector found")
    else:
        # No sectors matched the search string
        main_sector = [False]
        sub_sectors = []
        print("No sectors found")

    print("SUB-sectors:", sub_sectors)

    # Return both the main sector and sub-sectors as a list of lists
    sel_sectors = [main_sector] + [sub_sectors]
    return sel_sectors

def fun_selected_var_secondary_energy(
    _fuel_list: list,
    var: str,
    df_iam_all: pd.DataFrame,
    region_name: str,
    target: str,
    IAM_fuel_dict: dict,
    df_countries: pd.DataFrame,
    sel_sectors: list,
    file_suffix: str,
    PREV_STEP_RES_DIR: str,
    df_iea_all: pd.DataFrame,
    df_platts: pd.DataFrame,
    model: str,
    fuel_dict: dict,
    _func: str
) -> tuple:
    """
    This function processes and filters data based on fuel type and region for secondary energy targets.
    It updates the list of fuels by including 'OTHER' and returns relevant dataframes for further analysis.

    Args:
        _fuel_list (list): List of fuel types to be used.
        var (str): Variable representing the energy type (e.g., "Electricity").
        df_iam_all (pd.DataFrame): IAM data containing energy results for various regions and sectors.
        region_name (str): Name of the region to filter data by.
        target (str): Energy target to filter the data by.
        IAM_fuel_dict (dict): Dictionary mapping IAM fuel names to standardized fuel names.
        df_countries (pd.DataFrame): DataFrame containing country and region mapping.
        sel_sectors (list): List of main and sub-sectors for sectoral data filtering.
        file_suffix (str): Suffix used to retrieve previously processed files.
        PREV_STEP_RES_DIR (str): Directory path for previous results.
        df_iea_all (pd.DataFrame): IEA data containing historical energy data.
        df_platts (pd.DataFrame): Platts data containing information about fuels and power plants.
        model (str): Model name used to filter data.
        fuel_dict (dict): Dictionary mapping fuel names from various datasets to standardized names.
        _func (str): Function to apply in certain data operations.

    Returns:
        tuple: 
        - countrylist (list): List of countries in the specified region.
        - df_iam_sel (pd.DataFrame): Filtered IAM data for the specific region and energy target.
        - max_year (int): The last available year in the IEA data.
        - df_demand (pd.DataFrame): DataFrame with energy demand information.
        - df_iea_melt (pd.DataFrame): Melted IEA data for easier processing.
        - df_platts (pd.DataFrame): Platts data with mapped fuel types.
        - _fuel_list (list): Updated list of fuels including 'OTHER'.
    """

    # Extract region name from the full model string if needed
    region = region_name.replace(f"{model}|", "")

    # ====================================================
    # IAM Data Filtering (df_iam_sel)
    # ====================================================
    setindex(df_iam_all, False)  # Reset index for the IAM dataframe
    try:
        # Try to filter based on full region name (model + region)
        df_iam_sel = fun_df_iam_sel(df_iam_all, region_name, target, 2010)
    except:
        # If filtering fails, fall back to region only
        df_iam_sel = fun_df_iam_sel(df_iam_all, region, target, 2010)

    # Filter IAM data to focus on the energy mix for electricity and the specified fuels
    df_iam_sel = fun_fuel_mix(df_iam_sel, "Secondary Energy|Electricity", _fuel_list, IAM_fuel_dict)

    # ====================================================
    # Country List for the Given Region
    # ====================================================
    # Create a list of countries belonging to the specified region
    countrylist = df_countries[df_countries.REGION == region].ISO.unique()

    # ====================================================
    # Reading Energy Demand (df_demand)
    # ====================================================
    # Load previously processed data from step 1, with appropriate modifications
    df_sect = fun_load_data_from_step1_and_modify_column_names(
        CONSTANTS.CURR_RES_DIR('step1'), region, model, file_suffix, target, True, ['ENSHORT_REF'], _func
    )
    # Replace column names related to ENSHORT_REF
    df_sect.columns = [x.replace('ENSHORT_REF', '') for x in df_sect.columns]
    df_sect = df_sect.loc[:, df_sect.columns != False]  # Remove columns with False as their name

    # Extract demand data (for 'wo_smooth_enlong' method) and rename columns
    df_demand = df_sect[df_sect.METHOD == 'wo_smooth_enlong'][["Final Energy|Electricity"]]
    df_demand.loc[:, "SECTOR"] = df_demand.columns[0]
    df_demand.rename(columns={df_demand.columns[0]: var})

    # ====================================================
    # Melting IEA Data (df_iea_melt)
    # ====================================================
    # Find the maximum available year from the IEA data
    max_year = int(pd.to_numeric(df_iea_all.columns, errors="coerce").max())
    range_list = range(1960, max_year + 1)
    range_list = [str(i).zfill(2) for i in range_list]

    # Define the required sectors for filtering IEA data
    sectors_required = [
        "   Total final consumption",
        "      Transport",
        "      Industry",
        "      Residential",
        "      Commercial and public services",
    ]

    # Filter IEA data by flow type and relevant sectors, and include only countries in the selected region
    df_iea_sel = df_iea_all[
        (
            (df_iea_all.FLOW == "Electricity output (GWh)") |
            (df_iea_all.FLOW.isin(sectors_required)) |
            (df_iea_all.FLOW == "Imports") |
            (df_iea_all.FLOW == "Production") |
            (df_iea_all.FLOW == "Exports") |
            (df_iea_all.FLOW == "   Total final consumption")
        ) & df_iea_all.ISO.isin(countrylist)
    ]

    # Melt the selected IEA data to convert it into a long format (for time series analysis)
    df_iea_melt = df_iea_sel.melt(
        id_vars=["COUNTRY", "FLOW", "PRODUCT", "IAM_FUEL", "ISO"],
        value_vars=range_list,
    )
    df_iea_melt.rename(columns={"variable": "TIME", "value": "VALUE"}, inplace=True)
    df_iea_melt["VALUE"] = pd.to_numeric(df_iea_melt["VALUE"], errors="coerce")

    # ====================================================
    # Processing PLATTS Data (df_platts)
    # ====================================================
    # Map fuel names from the PLATTS data to the IAM fuel names using the provided dictionary
    df_platts["FUEL_IAM"] = df_platts["FUEL"].map(fuel_dict["FUEL"])

    # ====================================================
    # Updating Fuel List
    # ====================================================
    # Add 'OTHER' to the list of fuels for further analysis
    _fuel_list = _fuel_list + ["OTHER"]

    # Return all the processed dataframes and the updated fuel list
    return (
        countrylist,
        df_iam_sel,
        max_year,
        df_demand,
        df_iea_melt,
        df_platts,
        _fuel_list,
    )

def fun_check_adding_up_to_one(df):
    """
    This function checks if the values in a given DataFrame, grouped by the "TIME" column, sum up to 1 for each group. 
    It raises a ValueError if the sum of any group (for any time period) does not equal 1, ensuring the data integrity 
    (e.g., shares or proportions that are supposed to sum to 1).

    Args:
        df (pd.DataFrame): A pandas DataFrame where the rows are grouped by the "TIME" column, and the values are expected 
                           to sum up to 1 for each group.

    Raises:
        ValueError: If the sum of values within any group (for each time period) does not add up to 1, the function 
                    raises a ValueError with a message indicating the problem.
    """

    if (
        np.round(df.groupby("TIME").sum().max().max(), 6) != 1  # Max sum for any group of "TIME" must be 1
        and np.round(df.groupby("TIME").sum().min().min(), 6) != 1  # Min sum for any group of "TIME" must also be 1
    ):
        raise ValueError(
            "Your dataframe `df_desired` does not add up to 1 (when summing up all countries). Please check your data"
        )


def unmelt_variable(
    _df: pd.DataFrame,
    countrylist: List[str],
    _index: str = "TIME",
    _columns: str = "IAM_FUEL",
    _values: str = "VALUE",
) -> pd.DataFrame:
    """
    Unmelts a DataFrame by pivoting it based on specified index, columns, and values.

    This function takes a DataFrame (_df) and transforms it by pivoting the data, 
    such that the specified column becomes the new index, another column becomes the 
    columns in the resulting DataFrame, and a third column provides the values.

    Args:
        _df (pd.DataFrame): The input DataFrame to be unmelted.
        countrylist (List[str]): A list of country identifiers to filter the DataFrame.
        _index (str): The name of the index to set in the resulting DataFrame.
                       Defaults to "TIME".
        _columns (str): The name of the column to be used as the columns in the resulting DataFrame.
                        Defaults to "IAM_FUEL".
        _values (str): The name of the column to be used for the values in the resulting DataFrame.
                       Defaults to "VALUE".

    Returns:
        pd.DataFrame: The resulting DataFrame after unmelting, with the specified index,
                      columns, and values.
    """
    setindex(_df, False)
    final = pd.DataFrame()  # This will be your future DataFrame
    a = pd.DataFrame()
    m = ["TIME", "ISO"]

    for c in countrylist:
        a = _df[_df.ISO == c][[_values, _index, _columns, "ISO"]].pivot(
            index=_index, columns=_columns, values=_values
        )
        setindex(a, False)
        a["ISO"] = c
        final = final.append(a, ignore_index=True)

    setindex(final, m)
    return final



def fun_gov_sel(
    countrylist: List[str], 
    _ssp: str, 
    df_gov_all: pd.DataFrame, 
    fuel_list: List[str]
) -> pd.DataFrame:
    """
    Selects and prepares governance data for a specified list of countries and SSP.

    This function filters the governance DataFrame to include only the selected countries and 
    relevant scenarios. It retrieves observed governance data for the year 2010 and scenario 
    data for the specified SSP from 2015 onwards. Additionally, it duplicates the governance 
    values for the specified fuel types.

    Args:
        countrylist (List[str]): A list of country identifiers for which governance data is requested.
        _ssp (str): The shared socioeconomic pathway scenario identifier.
        df_gov_all (pd.DataFrame): A DataFrame containing governance data for all countries and scenarios.
        fuel_list (List[str]): A list of fuel types for which governance values need to be retrieved.

    Returns:
        pd.DataFrame: A DataFrame containing governance data for the selected countries, SSP, and fuel types.
    """
    ## SSP SELECTION
    setindex(df_gov_all, "TIME")
    
    df_gov = df_gov_all[
        (
            ((df_gov_all.scenario == "Observed") & (df_gov_all.index == 2010))
            | ((df_gov_all.scenario == _ssp) & (df_gov_all.index >= 2015))
        )
    ].loc[:, ["governance", "ISO"]]  # Select governance and ISO columns

    fun_pd_sel(df_gov, "", "")
    
    for f in fuel_list:
        df_gov[f] = fun_pd_sel(df_gov, "", "")[["governance"]]

    df_gov = fun_pd_sel(df_gov, "", countrylist).dropna()
    
    return df_gov



def fun_standardise_df_desired_within_region(
    fuel_list: list, 
    df_desired: pd.DataFrame, 
    df_iam: pd.DataFrame
) -> pd.DataFrame:
    """
    This function standardizes the `df_desired` DataFrame by adjusting the fuel shares within each region 
    to match the regional IAM (Integrated Assessment Models) results. It ensures that the sum of fuel 
    shares across countries aligns with the total regional IAM results for the respective fuels.

    The function computes the share of fuel use within a region (`df_share`) based on the `df_desired` values, 
    and then scales the `df_desired` values by multiplying the calculated shares with the corresponding regional 
    IAM data (`df_iam`).

    Args:
        fuel_list (list): A list of fuel types (e.g., "Coal", "Gas", etc.) for which the shares and standardization 
                          are performed.
        df_desired (pd.DataFrame): A DataFrame containing the desired downscaled results of fuel use at the country 
                                   level. This DataFrame needs to be standardized so that the sum of the fuel shares 
                                   matches the IAM regional results.
        df_iam (pd.DataFrame): A DataFrame containing the IAM results at the regional level, against which the `df_desired` 
                               data will be scaled.

    Returns:
        pd.DataFrame: A standardized DataFrame where fuel shares in `df_desired` have been adjusted to match the 
                      regional IAM results across countries.
    """

    # Calculate the share of fuel within the region based on the desired fuel data
    df_share = make_share_region(df_desired, fuel_list)  # CALCULATING SHARE WITHIN REGION
    
    # Set the index for the calculated share and the desired DataFrame
    setindex(df_share, m)  # Assuming 'm' is predefined
    setindex(df_desired, m)

    # Multiply the share by IAM regional values to adjust df_desired
    df_desired[fuel_list] = df_share[fuel_list] * df_iam[fuel_list]
    
    return df_desired


def fun_add_countries_wo_hist_data(
    countrylist: list, 
    df: pd.DataFrame, 
    fuel_list: list
) -> pd.DataFrame:
    """
    This function adds countries that are missing historical data to the given DataFrame `df`. 
    For each country missing data, the function appends rows with zeros for all time periods and fuel types, 
    ensuring that all countries in the `countrylist` are represented in the DataFrame.

    Args:
        countrylist (list): A list of country codes (ISO format) representing all countries to be included.
        df (pd.DataFrame): A DataFrame containing historical data for some countries. The DataFrame should 
                           contain the columns 'TIME', 'IAM_FUEL', 'ISO', and 'VALUE'.
        fuel_list (list): A list of fuel types (e.g., "Coal", "Gas", etc.) for which data should be added 
                          for countries missing historical data.

    Returns:
        pd.DataFrame: An updated DataFrame with missing countries added. For each missing country, rows are 
                      appended with zero values for all time periods and fuel types.
    """
    
    # List of unique time periods from the DataFrame
    my_range = list(df.TIME.unique())
    
    # List of countries that are in countrylist but missing from the DataFrame
    countries_w_missing_hist_data = [x for x in countrylist if x not in df.ISO.unique()]

    # Loop through each country missing historical data and append zero-value rows
    for c in countries_w_missing_hist_data:
        # Create a new DataFrame for the missing country with zero values for all fuels and time periods
        df_append = pd.DataFrame(
            {
                "TIME": [t for x in fuel_list for t in my_range],  # Time periods repeated for each fuel
                "IAM_FUEL": [f for f in fuel_list for _ in my_range],  # Each fuel repeated for all time periods
                "ISO": [c for _ in fuel_list for _ in my_range],  # Country ISO code repeated for all fuels and time periods
                "VALUE": [0 for _ in fuel_list for _ in my_range],  # Zero values for the missing data
            }
        )

        # Append the new rows to the existing DataFrame and reset the index
        df = df.append(df_append).reset_index(drop=True)
    
    return df


def fun_head(
    df_desired: pd.DataFrame,
    df_base_year_share: pd.DataFrame,
    df_last_year_share: pd.DataFrame,
    df_iam_sel: pd.DataFrame,
    fuels: list,
    time_periods: list = [2020, 2015, 2010],
) -> Tuple[pd.DataFrame, list]:
    """
    This function updates the head of the `df_desired` DataFrame by:
    1) Ensuring that the base year coincides with the `df_base_year_share` DataFrame.
    2) Ensuring that the last historical year coincides with the `df_last_year_share` DataFrame.

    The function modifies `df_desired` by applying the base year and last year values, adjusts the fuel data accordingly,
    and returns the updated DataFrame. It also returns the updated list of time periods used in the process.

    Args:
        df_desired (pd.DataFrame): The DataFrame containing the desired historical and future data for a region.
        df_base_year_share (pd.DataFrame): The DataFrame containing base year share data to be applied to `df_desired`.
        df_last_year_share (pd.DataFrame): The DataFrame containing the last year's share data to be applied to `df_desired`.
        df_iam_sel (pd.DataFrame): The IAM results DataFrame to be multiplied by the shares to update `df_desired`.
        fuels (list): List of fuel types to be considered during the update process.
        time_periods (list, optional): List of time periods to consider for the `fun_fill_missing_hist_data_with_future_data` function. 
                                       Defaults to [2020, 2015, 2010].

    Returns:
        Tuple[pd.DataFrame, list]: Returns a tuple containing the updated `df_desired` DataFrame and the list of time periods.
    """
    
    df_head = pd.DataFrame(
        (df_base_year_share.append(df_last_year_share))
        .groupby(["TIME", "ISO", "IAM_FUEL"])
        .sum()
    )["VALUE"].unstack(["IAM_FUEL"])
    df_desired.loc[df_head.index, df_head.columns] = df_head * df_iam_sel

    # possible solution for nuclear in the Rest centrally planned asia region
    df_desired = fun_fill_missing_hist_data_with_future_data(
        df_desired, fuels, time_periods=time_periods, dt=5
    )
    return df_desired


def fun_check_reg_results(
    df_iam_sel: pd.DataFrame,
    df_desired: pd.DataFrame,
    fuel_list: list,
    decimals: int = 4
) -> None:
    """
    This function checks whether the `df_desired` DataFrame matches the regional IAM results in `df_iam_sel` for the specified fuels.
    It performs the check by comparing the rounded sums of `df_desired` grouped by "TIME" with the corresponding data in `df_iam_sel`.
    If the data does not match, an error is raised.

    Args:
        df_iam_sel (pd.DataFrame): The DataFrame containing the Integrated Assessment Model (IAM) regional results for comparison.
        df_desired (pd.DataFrame): The DataFrame containing the disaggregated or downscaled country-level data to check.
        fuel_list (list): List of fuel types (columns) to be compared between `df_iam_sel` and `df_desired`.
        decimals (int, optional): Number of decimal places to use when rounding for comparison. Defaults to 5.

    Raises:
        ValueError: If `df_desired` is not a DataFrame, or if the rounded values in `df_desired` do not match those in `df_iam_sel`.
    """
    from pandas.testing import assert_frame_equal

    # Check if the provided df_desired is a DataFrame
    if not isinstance(df_desired, pd.DataFrame):
        raise ValueError(
            f"`df_desired` needs to be a DataFrame. You provided a {type(df_desired)}"
        )

    try:
        # Compare the IAM regional results (df_iam_sel) to the sum of df_desired grouped by "TIME"
        assert_frame_equal(
            np.round(df_iam_sel[fuel_list], decimals),
            np.round(df_desired[fuel_list].groupby("TIME").sum(axis=0), decimals),
            check_index_type=False,  # Ignore index type differences
            check_dtype=False,       # Ignore data type differences
        )
    except AssertionError:
        # Raise a ValueError if the DataFrames do not match
        raise ValueError("`df_desired` does not match IAM results. Please check your data.")


def fun_create_csv_files_from_dat_if_missing(
    cost_curve_list: list, file_name_dict: dict
) -> None:
    """
    This function checks if CSV files with cost curve data for each fuel exist. If the CSV file doesn't exist,
    it creates a new CSV file by converting from the corresponding `.dat` file.

    Args:
        cost_curve_list (list): List of fuel types for which we have cost curve data.
        file_name_dict (dict): Dictionary mapping fuel types to their corresponding file name (without extension).
    
    Raises:
        FileNotFoundError: If the specified .dat file is not found.
    """

    for fuel in cost_curve_list:
        # Check if the CSV for the cost curve already exists
        csv_path = CONSTANTS.INPUT_DATA_DIR / (file_name_dict[fuel] + ".csv")
        if not os.path.isfile(csv_path):
            print(f"Creating CSV with cost curve data from {file_name_dict[fuel]}.dat")
            # Convert .dat file to .csv if .csv doesn't exist
            dat_path = CONSTANTS.INPUT_DATA_DIR / (file_name_dict[fuel] + ".dat")
            fun_convert_dat_to_csv_file(dat_path)

        # Check if the CSV for the MaxProd data already exists
        maxprod_csv_path = CONSTANTS.INPUT_DATA_DIR / (file_name_dict[fuel].replace("CostCurve", "MaxProd") + ".csv")
        if not os.path.isfile(maxprod_csv_path):
            print(f"Creating CSV with MaxProd data from {file_name_dict[fuel].replace('CostCurve', 'MaxProd')}.dat")
            # Convert .dat file to .csv for MaxProd if .csv doesn't exist
            maxprod_dat_path = CONSTANTS.INPUT_DATA_DIR / (file_name_dict[fuel].replace("CostCurve", "MaxProd") + ".dat")
            fun_convert_dat_to_csv_file(maxprod_dat_path)


def fun_read_df_gov() -> pd.DataFrame:
    """
    Reads governance data from Andrijevic et al. (2019) and returns it as a pandas DataFrame.

    The data includes governance indicators such as government effectiveness and corruption control.
    
    Source:
        - Andrijevic et al. (2019) "Governance in socioeconomic pathways and its role for future adaptive capacity". 
          Nature Sustainability. DOI: https://doi.org/10.1038/s41893-019-0405-0
        - GitHub Repository: https://github.com/marina-andrijevic/governance2019
    
    File Path:
        The function reads from a CSV file located in `CONSTANTS.INPUT_DATA_DIR / "master_proj_obs.csv"`.
    
    Returns:
        pd.DataFrame: DataFrame containing governance data with renamed columns:
            - 'eff' for government effectiveness (gov.eff)
            - 'corr' for corruption control (corr.cont)
            - 'ISO' for country code (countrycode)
            - 'TIME' for year (year)
    
    Example:
        df_gov = fun_read_df_gov()
    
    Notes:
        - CSV file is assumed to be encoded in 'latin-1'.
    """
    
    _df_gov = pd.read_csv(
        CONSTANTS.INPUT_DATA_DIR / "master_proj_obs.csv",
        sep=",",
        encoding="latin-1",
    )
    # Renaming columns for consistency and clarity
    _df_gov.rename(
        columns={
            "gov.eff": "eff",        # Government effectiveness
            "corr.cont": "corr",      # Corruption control
            "countrycode": "ISO",     # ISO country code
            "year": "TIME",           # Year
        },
        inplace=True,
    )
    
    return _df_gov


def fun_df_iam_sel(
    _df: pd.DataFrame, _region: str, _target: str, _time: int
) -> pd.DataFrame:
    """
    Selects and returns a subset of an IAM DataFrame based on the specified region, scenario target, and time period.
    
    Args:
        _df (pd.DataFrame): The full IAM dataframe.
        _region (str): The region to filter the dataframe.
        _target (str): The target scenario for the filter.
        _time (int): The starting year for filtering (inclusive).

    Returns:
        pd.DataFrame: A filtered IAM DataFrame containing the selected region, scenario target, and time greater than or equal to `_time`.

    Example:
        df_iam_sel = fun_df_iam_sel(df, 'World', 'Scenario_X', 2010)
    """
    _df_iam_sel = _df[
        (_df.SCENARIO == _target) & (_df.REGION == _region) & (_df.TIME >= _time)
    ]
    _df_iam_sel["ISO"] = _df_iam_sel["REGION"]  # Adding ISO as a copy of REGION
    return _df_iam_sel


def fun_inv_map(_my_map: Dict[str, str]) -> Dict[str, str]:
    """
    Inverts a dictionary by swapping its keys and values.
    
    Args:
        _my_map (Dict[str, str]): A dictionary to invert.
        
    Returns:
        Dict[str, str]: The inverted dictionary where keys become values and vice versa.
    
    Example:
        inverted_dict = fun_inv_map({'a': '1', 'b': '2'})  # returns {'1': 'a', '2': 'b'}
    
    Notes:
        - The input dictionary should have unique values to avoid loss of data.
    """
    return {v: k for k, v in _my_map.items()}


def fun_convert_dat_to_csv_file(_file: str) -> None:
    """
    Converts a .dat file into a .csv file by reading the .dat file and saving it as a .csv file.
    
    Args:
        _file (str): The path to the .dat file that needs to be converted.
        
    Returns:
        None: The function does not return a value but writes the converted content to a new .csv file.
    
    Example:
        fun_convert_dat_to_csv_file("path/to/file.dat")
    
    Notes:
        - The function assumes that the .dat file content is space-separated.
        - The new .csv file is saved in the same directory as the .dat file with the same name.
    """
    # Read the .dat file content into a list of lists (assuming space-separated values)
    _datContent = [_i.strip().split() for _i in open(_file).readlines()]

    # Write the list to a new CSV file (same name as .dat but with .csv extension)
    with open(str(_file).replace(".dat", ".csv"), "w", newline='') as _f:
        _writer = csv.writer(_f)
        _writer.writerows(_datContent)


def fun_fuel_mix(
    _df_iam_sel: pd.DataFrame, 
    _var: str, 
    _fuel_list: List[str], 
    IAM_fuel_dict: Dict[str, str]
) -> pd.DataFrame:
    """
    This function creates a dataset representing the fuel mix for a given variable (e.g., 'Secondary Energy|Electricity')
    and a list of fuels (e.g., ['COAL', 'NUC']). It returns a DataFrame with the corresponding fuel mix.

    Args:
        _df_iam_sel (pd.DataFrame): The IAM dataframe filtered by region, scenario, and time period.
        _var (str): The variable representing the energy type, such as 'Secondary Energy|Electricity'.
        _fuel_list (List[str]): List of fuels (e.g., ['COAL', 'NUC']).
        IAM_fuel_dict (Dict[str, str]): Dictionary that maps fuel names to IAM variables.

    Returns:
        pd.DataFrame: A DataFrame representing the fuel mix over time, with columns for each fuel, 'OTHER', and 'TOTAL'.
        
    Example:
        df_fuel_mix = fun_fuel_mix(df_iam_sel, 'Secondary Energy|Electricity', ['COAL', 'NUC'], IAM_fuel_dict)
    
    Dependencies:
        1) fun_inv_map: Function to invert the fuel mapping dictionary.
        2) IAM_fuel_dict: Dictionary containing fuel mapping.

    Notes:
        - The function assumes that `_df_iam_sel` contains 'VARIABLE' and 'VALUE' columns.
        - The 'OTHER' column represents the remaining energy to match the total.
        - The `TOTAL` column represents the total energy for the given variable.
    """
    
    # Set the index of _df_iam_sel by 'TIME'
    _df_iam_sel = _df_iam_sel.set_index("TIME")

    # Initialize a dataframe to store the fuel mix
    _df_iam_bis = pd.DataFrame()
    
    # Invert the fuel dictionary to map variable strings to their corresponding fuel names
    inv_fuel_dict = fun_inv_map(IAM_fuel_dict)
    
    # ========
    # Creating the TOTAL column based on the given _var
    # ========
    total_var_def = str(_var) + str(inv_fuel_dict.get("TOTAL", ""))
    _df_iam_bis["TOTAL"] = _df_iam_sel[_df_iam_sel.VARIABLE == total_var_def].VALUE

    # ========
    # Create individual fuel columns based on _fuel_list
    # ========
    try:
        _fuel_list.remove("OTHER")  # Remove 'OTHER' from the fuel list if present
    except ValueError:
        pass

    for _fuel in _fuel_list:
        _var_name = _fuel + "_IAM"
        _var_def = str(_var) + str(inv_fuel_dict.get(_fuel, ""))
        
        print(f"{_var_name} = {_var_def}")  # e.g., NUC_IAM = 'Secondary Energy|Electricity|Nuclear'
        
        # Add the fuel-specific data to the DataFrame
        _df_iam_bis[_fuel] = _df_iam_sel[_df_iam_sel.VARIABLE == _var_def].VALUE

    # ========
    # Calculate the 'OTHER' column
    # ========
    _df_iam_bis["OTHER"] = _df_iam_bis["TOTAL"] - _df_iam_bis[_fuel_list].sum(axis=1)

    # Add the 'ISO' column as a constant (assuming 'region' is predefined or passed in globally)
    _df_iam_bis["ISO"] = _df_iam_sel["REGION"].iloc[0]  # Assuming 'REGION' is in _df_iam_sel

    # Reset the index to use 'TIME' as a column
    _df_iam_bis = _df_iam_bis.reset_index()

    return _df_iam_bis


def make_share_region(_df: pd.DataFrame, _fuel_list: List[str]) -> pd.DataFrame:
    """
    This function calculates and returns the share of each country's fuel data as a percentage of the overall region 
    for each fuel in the provided fuel list.

    Args:
        _df (pd.DataFrame): DataFrame containing country data with a 'TIME' column and fuel columns for each country.
        _fuel_list (List[str]): List of fuel column names (e.g., ['COAL', 'NUC', 'GAS']) that should be used for share calculation.

    Returns:
        pd.DataFrame: A DataFrame with the fuel data expressed as a percentage of the regional total for each fuel 
                      at each point in time (i.e., each 'TIME').

    Example:
        df_share = make_share_region(df, ['COAL', 'NUC', 'GAS'])

    Notes:
        - The function normalizes each fuel by dividing each country's value by the total value for that fuel across 
          all countries at a given time period.
        - To avoid division by zero, a small constant (1e-9) is added when summing the fuel values.
    """

    setindex(_df, "ISO")
    _df_share = _df.copy(deep=True)

    for _t in _df.TIME.unique():
        _df_share.loc[_df_share.TIME == _t, _fuel_list] = _df_share[
            _df_share.TIME == _t
        ][_fuel_list] / (_df_share[_df_share.TIME == _t][_fuel_list].sum(axis=0) + 1e-9)

    return _df_share


def fun_fill_missing_hist_data_with_future_data(
    df_desired: pd.DataFrame,
    fuels: List[str],
    time_periods: List[int] = [2010, 2015],
    dt: int = 5
) -> pd.DataFrame:
    """
    Fills missing historical data in the desired DataFrame by using future data.
    If the sum of the specified fuel's values for a given time period is zero,
    it attempts to fill this value with the corresponding value from a future period.

    Args:
        df_desired (pd.DataFrame): DataFrame containing the desired data, indexed by time,
                                    with columns for different fuels.
        fuels (List[str]): List of fuel column names to be processed.
        time_periods (List[int], optional): List of time periods to check for missing data.
                                             Default is [2010, 2015]. The list will be sorted 
                                             in descending order.
        dt (int, optional): Time step (in years) to look ahead for filling missing data. 
                            Default is 5 years.

    Returns:
        pd.DataFrame: The updated DataFrame with missing historical data filled from future data.

    Example:
        df_filled = fun_fill_missing_hist_data_with_future_data(df_desired, ['COAL', 'NUC'])

    Notes:
        - If the future value is also missing (sum == 0), it sets the current value to a small constant (1e-5).
        - The function only processes columns that correspond to the specified fuels.
    """
    
    # Sort time periods in descending order to fill data from future to past
    time_periods.sort(reverse=True)
    
    for t in time_periods:
        for f in fuels:
            if df_desired.loc[t][f].sum() == 0:  # Check if the sum for the current period is zero
                if df_desired.loc[t + dt][f].fillna(0).sum() == 0:  # Check the future period
                    df_desired.loc[t][f] = 1e-5  # Set a small constant if the future value is also zero
                else:
                    df_desired.loc[t][f] = df_desired.loc[t + dt][f]  # Fill with future value

    return df_desired


def fun_df_desired(
    base_year: int,
    _last_year: int,
    df_demand: pd.DataFrame,
    df_gov_all: pd.DataFrame,
    countrylist: List[str],
    ssp: str,
    fuel_list: List[str],
    df_iam: pd.DataFrame,
    df_iea_melt: pd.DataFrame,
    time_periods: List[int],
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Create a DataFrame called 'df_desired', containing a 'desired' electricity mix for each country,
    based on governance criteria. It establishes LOWER and UPPER values for electricity generation 
    based on future (previously downscaled) electricity demand and the maximum historical trade 
    reached.

    Parameters:
    - base_year (int): The base year for analysis.
    - _last_year (int): The last year of data availability.
    - df_demand (pd.DataFrame): DataFrame containing electricity demand data.
    - df_gov_all (pd.DataFrame): DataFrame containing governance criteria data.
    - countrylist (List[str]): List of country ISO codes to consider.
    - ssp (str): Scenario string for filtering the governance data.
    - fuel_list (List[str]): List of fuel types for which to calculate the desired electricity mix.
    - df_iam (pd.DataFrame): DataFrame with IAM data for fuel mix calculation.
    - df_iea_melt (pd.DataFrame): Melted DataFrame for electricity output.
    - time_periods (List[int]): List of time periods to filter in the final output.

    Returns:
    - df_desired (pd.DataFrame): DataFrame with the desired electricity mix based on governance data.
    - df_base_year_share (pd.DataFrame): DataFrame with base year criteria (share of country within a region, for each fuel).
    - df_last_year_share (pd.DataFrame): DataFrame with last year criteria (share of country within a region, for each fuel).

    Note:
    The df_desired DataFrame should be adjusted later to include additional criteria based on a weighted criteria matrix.
    """

    ## Creating df_desired. WE START BY SETTING ALL FUELS USING GOVERNANCE CRITERIA (THEY WILL BE OVERWRITTEN LATER)
    setindex(df_demand, False)
    df_demand["TIME"] = pd.to_numeric(df_demand["TIME"])

    df_desired = df_gov_all[
        (df_gov_all.ISO.isin(countrylist))
        & (df_gov_all.index >= base_year)
        & (((df_gov_all.scenario == "Observed") | (df_gov_all.scenario == ssp)))
    ].loc[:, ["governance", "ISO"]]
    df_desired.rename(columns={"governance": "GOV"}, inplace=True)
    
    for f in fuel_list:
        df_desired[f] = df_desired["GOV"]  ## SAME CRITERIA FOR ALL FUELS

    # Reading ENSHORT from df_demand, to be stored as 'UPPER' and 'LOWER'
    setindex(df_desired, ["ISO", "TIME"])
    setindex(df_demand, ["ISO", "TIME"])

    m = ["TIME", "ISO"]
    df_share = pd.DataFrame()
    setindex(df_desired, m)

    ## STANDARDISING df_desired using shares
    df_share = make_share_region(df_desired, fuel_list)  ## CALCULATING SHARE WITHIN REGION
    setindex(df_share, m)
    setindex(df_desired, m)
    df_desired[fuel_list] = df_share[fuel_list] *df_iam[fuel_list] ## MULTIPLY SHARE BY DF_IAM

    df_desired_orig = df_desired.copy(deep=True)  ## Creating a copy of the DF

    # ======================
    ## Base year % criteria (share of country within a region, for each fuel)
    # ======================
    y = base_year
    setindex(df_iea_melt, "ISO")
    flow = "Electricity output (GWh)"
    tot = (
        df_iea_melt[(df_iea_melt.TIME == str(y)) & (df_iea_melt.FLOW == flow)]
        .groupby("IAM_FUEL")["VALUE"]
        .sum()
    )  ## TOTAL BY FUEL IN THE REGION
    df_base_year_share = pd.DataFrame(
        (
            df_iea_melt[(df_iea_melt.TIME == str(y)) & (df_iea_melt.FLOW == flow)]
            .groupby(["IAM_FUEL", "ISO"])["VALUE"]
            .sum()
            / tot
        )
    )
    setindex(df_base_year_share, False)

    df_base_year_criteria = df_base_year_share.pivot(
        index="ISO", columns="IAM_FUEL", values="VALUE"
    )  ## Unmelt from long to wide format

    # ======================
    ## Last available year % criteria
    # ======================
    y = _last_year
    setindex(df_iea_melt, "ISO")
    flow = "Electricity output (GWh)"
    tot = (
        df_iea_melt[(df_iea_melt.TIME == str(y)) & (df_iea_melt.FLOW == flow)]
        .groupby("IAM_FUEL")["VALUE"]
        .sum()
    )  ## TOTAL BY FUEL IN THE REGION
    df_last_year_share = pd.DataFrame(
        (
            df_iea_melt[(df_iea_melt.TIME == str(y)) & (df_iea_melt.FLOW == flow)]
            .groupby(["IAM_FUEL", "ISO"])["VALUE"]
            .sum()
            / tot
        )
    )
    setindex(df_last_year_share, False)

    df_last_year_criteria = df_last_year_share.pivot(
        index="ISO", columns="IAM_FUEL", values="VALUE"
    )  ## Unmelt from long to wide format

    return df_desired.loc[time_periods], df_base_year_share, df_last_year_share

## DO NOT DELETE!
# ===========================================================
# ## CAPITAL VINTAGING PROJECTIONS WRITING FOR ALL COUNTRIES
# ## PROJECTIONS HERE ARE ONLY BASED ON PLATTS DB. IMPORTANT - DO NOT DELETE!
# ===========================================================

# df_gw_all_fuels=pd.DataFrame()
# ## THE BLOCK BELOW COULD BE DONE ONCE FOR ALL COUNTRIES AND THEN IMPORTING CSV FILE (THIS WOULD SAVE TIME)
# for f in ['GEO','OIL','COAL','GAS']: ## fuel list
#     status=['OPR','PLN','CON'] ## selected status
#     string='df_gw_all_'+f

#     ## WRITING CSV FOR ALL COUNTRIES (countrylist=False)
#     print("working on",f)
#     exec("%s = %s" % (string, "fun_capital_projections(RESULTS_DATA_DIR, df_platts, df_iam_sel, df_countries, False, f,"+ str(status)+
#                       ",lifetime_dict[f], _show_plot=False, _save_csv=True)"))

# ### Capital Vintaging

# DO NOT DELETE!
# ===========================================================
# ## CAPITAL VINTAGING PROJECTIONS WRITING FOR ALL COUNTRIES
# ## PROJECTIONS HERE ARE ONLY BASED ON PLATTS DB. IMPORTANT - DO NOT DELETE!
# ===========================================================

# df_gw_all_fuels=pd.DataFrame()
# ## THE BLOCK BELOW COULD BE DONE ONCE FOR ALL COUNTRIES AND THEN IMPORTING CSV FILE (THIS WOULD SAVE TIME)
# for f in ['GEO','OIL','COAL','GAS']: ## fuel list
#     status=['OPR','PLN','CON'] ## selected status
#     string='df_gw_all_'+f

#     ## WRITING CSV FOR ALL COUNTRIES (countrylist=False)
#     print("working on",f)
#     exec("%s = %s" % (string, "fun_capital_projections(RESULTS_DATA_DIR, df_platts, df_iam_sel, df_countries, False, f,"+ str(status)+
#                       ",lifetime_dict[f], _show_plot=False, _save_csv=True)"))

# NOTE this function does not seem to be affected by TARGET => caching strategy (for df_gw_all_fuels)?

def fun_stranded_assets(
    _f_list: List[str], 
    _status: List[str], 
    countrylist: List[str], 
    lifetime_dict: Dict[str, int], 
    add_uncertainty_average: bool = True
) -> pd.DataFrame:
    """
    Returns a DataFrame with downscaled data for stranded assets 
    for each fuel in the provided fuel list.

    Parameters:
    - _f_list (List[str]): List of fuels to analyze (e.g., ['GEO', 'OIL', 'COAL', 'GAS']).
    - _status (List[str]): Status of power plants from the Platts database (e.g., ['OPR', 'PLN']).
    - countrylist (List[str]): List of countries to include in the analysis.
    - lifetime_dict (Dict[str, int]): Dictionary containing the lifetime of each fuel type.
    - add_uncertainty_average (bool): Flag indicating whether to include uncertainty in the results (default is True).

    Returns:
    - pd.DataFrame: A DataFrame with downscaled stranded asset data for each fuel, possibly including uncertainty.
    """

    # ===========================================================
    ## READING CSV WITH ALL COUNTRIES, SELECTING COLUMNS IN COUNTRYLIST AND COMBINE ALL data IN ONE DF: df_gw_all_fuels
    # ===========================================================
    df_gw_all_fuels = pd.DataFrame()
    
    for f in _f_list:  # ['GEO','OIL','COAL','GAS']: ## fuel list
        status = _status  # ['OPR','PLN','CON'] ## selected status
        string = "df_gw_all_" + f

        string_read = (
            "'"
            + str(
                CONSTANTS.INPUT_DATA_DIR
                / (
                    "Projected GW _ "
                    + f
                    + " _ Status="
                    + str(status).replace("'", "")
                    + " _"
                    + str(lifetime_dict[f])
                    + " years_all_countries.csv"
                )
            )
            + "'"
        )  # index_col=['ISO'] )
        
        exec(
            "%s = %s"
            % (
                string,
                "pd.read_csv(r"  ## r added 2021_07_21
                + str(string_read + ",sep=',',  encoding='latin-1')"),
            )
        )

        eval(string)["TIME"] = eval(string)["Unnamed: 0"]  # eval(string).index
        setindex(eval(string), "TIME")

        ## Selecting only columns (countries) belonging to the countrylist:
        exec(
            "%s = %s"
            % (
                string,
                string
                + "["
                + string
                + ".columns["
                + string
                + ".columns.isin(countrylist)]]",
            )
        )

        # Melting the df below:
        setindex(eval(string), False)
        exec(
            "%s = %s"
            % (
                string + "_melt",
                "eval(string).melt(id_vars ='TIME', value_vars ="
                + string
                + ".columns.drop('TIME')"
                + ")",
            )
        )
        
        eval(string + "_melt").rename(columns={"variable": "ISO"}, inplace=True)
        setindex(eval(string + "_melt"), ["TIME", "ISO"])

        df_gw_all_fuels[f] = eval(string + "_melt")["value"]  ## Storing data into a single DataFrame for all fuels

    # Result based on technical lifetime
    res = df_gw_all_fuels.replace(0, np.nan).groupby(["ISO"]).ffill().fillna(0)

    if not add_uncertainty_average:
        return res
    else:
        # We add a bit of delay/uncertainty (phase out could happen later/earlier)
        periods = [1, 2]
        res_with_uncertainty = {
            p: res.shift(p).bfill() * 0.5 + res.shift(-p).ffill() * 0.5 for p in periods
        }
        # We return an average value
        return sum(res_with_uncertainty.values()) / len(res_with_uncertainty)


def fun_df_cost_criteria(
    df_desired: pd.DataFrame,
    cost_curve_list: List[str],
    fuel_list: List[str],
    df_iam: pd.DataFrame,
    countrylist: List[str],
    file_name_dict: Dict[str, str],
    df_iam_sel: pd.DataFrame,
) -> pd.DataFrame:
    """
    This function wraps up all the downscaled values for all fuels 
    (based on the supply cost curves criteria) in one dataset called 
    "df_cost_criteria" for all fuels.

    Parameters:
    - df_desired (pd.DataFrame): Desired DataFrame to shape the output.
    - cost_curve_list (List[str]): List of cost curves to process.
    - fuel_list (List[str]): List of fuels for which to calculate costs.
    - df_iam (pd.DataFrame): IAM DataFrame containing relevant information.
    - countrylist (List[str]): List of countries to include in the analysis.
    - file_name_dict (Dict[str, str]): Dictionary mapping fuel names to file names.
    - df_iam_sel (pd.DataFrame): Selected IAM DataFrame.

    Returns:
    - pd.DataFrame: A DataFrame containing cost criteria for all fuels.
    """

    # Cost curve blueprint (store result in a dictionary)
    res_dict = {}
    res_dict_melt = {}
    
    for f in cost_curve_list:
        res_dict[f"df_{f}_melt"] = pd.DataFrame(fun_read_cost(f, file_name_dict))
        res_dict[f"df_{f}_melt_sel0"] = pd.DataFrame(
            fun_sort_cost(res_dict[f"df_{f}_melt"], countrylist)
        )
        res_dict_melt[f] = pd.DataFrame(
            fun_downs_cost(f, res_dict[f"df_{f}_melt_sel0"], df_iam_sel)
        )

    # ============================================================
    # Wrapping all info in one dataset called "df_cost_criteria"
    # ============================================================
    setindex(df_desired, m)
    df_cost_criteria = pd.DataFrame(
        columns=fuel_list, index=df_desired.index
    )  # Workaround

    for fuel, j in res_dict_melt.items():
        for t in df_iam.index:
            for c in countrylist:
                sel = (df_cost_criteria.index.get_level_values(0) == t) & (
                    df_cost_criteria.index.get_level_values(1) == c
                )
                try:
                    df_cost_criteria.loc[sel, fuel] = float(
                        j[(j.ISO == c) & (j.TIME == t)]["DOWNS_VALUE"]
                    )
                except Exception:
                    df_cost_criteria.loc[sel, fuel] = 0

    return df_cost_criteria.dropna(how="all")


def fun_capital_projections(
    RESULTS_DATA_DIR: str,
    df_platts: pd.DataFrame,
    df_iam_sel: pd.DataFrame,
    df_countries: pd.DataFrame,
    _countrylist: Union[List[str], bool],
    _f: str,
    _s: List[str],
    _l: int,
    _show_plot: bool = True,
    _save_csv: bool = False,
) -> pd.DataFrame:
    """
    This function projects installed capacity over time based on 
    remaining lifetime assumptions (_l) using PLATTS data for fuel _f.
    It returns a DataFrame with projected installed capacity over time. 
    It projects data considering the time length of the df_iam_sel.

    Parameters:
    - RESULTS_DATA_DIR (str): Directory to save results.
    - df_platts (pd.DataFrame): DataFrame containing PLATTS data.
    - df_iam_sel (pd.DataFrame): Selected IAM DataFrame.
    - df_countries (pd.DataFrame): DataFrame containing country information.
    - _countrylist (Union[List[str], bool]): List of countries or False for all.
    - _f (str): Fuel type (e.g. 'coal').
    - _s (List[str]): Status of power plants (e.g. ['OPR']).
    - _l (int): Assumption on technical lifetime.
    - _show_plot (bool): Flag to indicate if plot should be displayed (default is True).
    - _save_csv (bool): Flag to indicate if results should be saved as CSV (default is False).

    Returns:
    - pd.DataFrame: DataFrame containing projected installed capacity over time.
    """

    """This function projects installed capacity over time based on remanining lifetime assumptions (_l) using PLATTS data for fuel _f.
    It returns a dataframe with projected installed capacity over time. It projects data considering the time lenght of the df_iam_sel
    It takes as inputs:
    _countrylist: list of countries
    _f: fuel (e.g. coal based power plants)
    _s: status (e.g. 'OPR' => operational)
    _y: assumption on technical lifetime
    NOTE: time steps of IAMs results (from df_iam_sel), should be constant over time (e.g. 10 years time steps: 2010, 2020 etc.)
    It requires matplotlib.pyplot and pandas.
    """

    _df_gw_all = pd.DataFrame()
    _df_platts_sel = pd.DataFrame()
    _df_platts_sel = df_platts[
        df_platts.STATUS.isin(_s)
    ]  ## selecting power plants status (e.g. s=['OPR'])
    _h = df_platts[
        df_platts.STATUS.isin(_s)
    ].YEAR.max()  ## last historical data available (we apply remaining lifetime calculations only for t>h). e.g.  currently installed power plants might have already exceed technical lifetime (we should not exclude those power plants from calculations)

    if _countrylist == False:
        _countrylist = df_platts.ISO.unique()
        _no_region = 1
    else:
        _no_region = 0

    for _c in _countrylist:
        _y = df_iam_sel.index[0]  # base year of IAMs results
        # l=40 # technical lifetime
        _dt = (
            df_iam_sel.index[2] - df_iam_sel.index[1]
        )  # 1 # time steps (assuming this is constant over time)

        _df_ret_schedule = pd.DataFrame()
        _timehorizon = range(
            df_iam_sel.index[-1], _y - _dt, -_dt
        )  # (e.g. range(2100, 2000, -10)) ## WE do a loop for each time period in the time horizon. We start the loop FROM 2100, going BACKWARD TO 2010 (OTHERWISE it overwrites the calculations previously done: e.g. 2100 overwrites the data for 2099 and so on.)

        for _t in _timehorizon:
            ## We first define a True/false series to be used to filter out the data (True means a power plants is in operation: year <= end of lifetime date) => we call this series 'ret'
            _ret = (
                _df_platts_sel[
                    (_df_platts_sel.ISO == _c) & (_df_platts_sel.FUEL_IAM == _f)
                ].YEAR
                + _l
                >= _t
            )  ## For each t(in time horizon 2010-2100) we check if t is higher than reteriment data (based on remaninig technical lifetime)

            ## Currently Operational power plants might already exceed technical lifetime assumptions.
            ## In this case we assume power plants will operate for additional 5 years.
            ## Remaning lifetime calculations only apply for Future values (not for historical data).
            #  h= year of last historical data
            _ret2 = min(_t, _h + 5) >= _t
            _ret = (
                _ret + _ret2
            )  # we overwrite (ret) in case of historical data (in this case all operating power plants are in operation, ret=True )

            ## We filter out the data based on 'ret'
            _df_ret_schedule.loc[_df_platts_sel["YEAR"] <= _t, _t] = (
                _df_platts_sel[
                    (_df_platts_sel.ISO == _c)
                    & (_df_platts_sel.FUEL_IAM == _f)
                    & (_ret)
                ].MW
                / 1e3
            )
            _df_ret_schedule.loc[_df_platts_sel["YEAR"] <= _t, "YEAR"] = _df_platts_sel[
                (_df_platts_sel.ISO == _c) & (_df_platts_sel.FUEL_IAM == _f)
            ].YEAR

        _df_ret_schedule["ISO"] = _c
        _df_ret_graph = _df_ret_schedule
        _df_ret_graph = _df_ret_graph.drop("ISO", axis=1)
        _df_ret_graph = _df_ret_graph.drop("YEAR", axis=1)

        # if _countrylist!=False:

        if _show_plot == True:
            plt.plot(_timehorizon, _df_ret_graph.sum())
            plt.title("Installed GW | " + _c)
            plt.show()

        ## STORING DATA IN A DATAFRAME, creating a df with all country-level data (GW installed projected over time)
        _df_gw_all[_c] = _df_ret_graph.sum()
        _df_gw_all.sort_index()

    _df_gw_all = _df_gw_all.sort_index()  ## Sort TIME

    if _save_csv == True:
        if _no_region == 0:
            _region = df_countries[df_countries.ISO == _c].REGION[0]  ## region
            _df_gw_all.to_csv(
                RESULTS_DATA_DIR / "Projected GW _ "
                + _f
                + " _ Status="
                + str(_s).replace("'", "")
                + " _"
                + str(_l)
                + " years_"
                + str(_region)
                + ".csv"
            )  ## Save to CSV
        else:
            _df_gw_all.to_csv(
                RESULTS_DATA_DIR / "Projected GW _ "
                + _f
                + " _ Status="
                + str(_s).replace("'", "")
                + " _"
                + str(_l)
                + " years_"
                + "all_countries"
                + ".csv"
            )  ## Save to CSV

    print("Capital has not been harmonised to match IEA data")
    return _df_gw_all

## NOTE: function below is actually used in the exec script
def fun_downs_cost(_f: str, df_melt_sel: pd.DataFrame, df_iam_sel: pd.DataFrame) -> pd.DataFrame:
    """
    Downscales fuel production based on supply cost curves.

    Args:
        _f (str): The fuel type to downscale (e.g., "SOL", "BIO").
        df_melt_sel (pd.DataFrame): A melted dataframe containing supply cost curves for different countries.
        df_iam_sel (pd.DataFrame): A dataframe containing IAM data, including regional production for the fuel.

    Returns:
        pd.DataFrame: A modified dataframe containing downscaled values in the 'DOWNS_VALUE' column for each 'TIME'.
    """
    _string = "df_melt_sel"
    setindex(eval(_string), False)
    eval(_string)["TIME"] = 0
    eval(_string)["DOWNS_VALUE"] = 0
    setindex(df_iam_sel, "TIME")
    
    for _t in df_iam_sel.index:
        _reg_prod = df_iam_sel.loc[_t][_f]  # The regional production
        _countries_to_be_allo = eval(_string)[eval(_string).REG_CUM <= _reg_prod].ISO.unique()  # Countries eligible for allocation
        for _c in _countries_to_be_allo:
            _idx = eval(_string)[(eval(_string).REG_CUM <= _reg_prod) & (eval(_string).ISO == _c)].index.max()
            if _idx < 0:
                _idx = eval(_string)[(eval(_string).ISO == _c)].index.min()

            eval(_string).loc[_idx, "TIME"] = _t
            eval(_string).loc[_idx, "DOWNS_VALUE"] = (eval(_string).loc[_idx]["PERC"] + 0.01) * eval(_string).loc[_idx]["RES"]  # production in country c

        _sum_all = eval(_string)[eval(_string).TIME == _t]["DOWNS_VALUE"].sum()

        # Computing relative share compared to regional production
        for _c in _countries_to_be_allo:
            _idx = eval(_string)[(eval(_string).REG_CUM <= _reg_prod) & (eval(_string).ISO == _c)].index.max()
            eval(_string).loc[_idx, "TIME"] = _t
            eval(_string).loc[_idx, "DOWNS_VALUE"] = (eval(_string).loc[_idx, "DOWNS_VALUE"] / _sum_all * _reg_prod)

        # Appending data for each year
        if _t != df_iam_sel.index[-1]:  # Skip appending for the last year
            exec(f"{_string} = {_string}.append({_string}[_string.TIME==_t], ignore_index=True)")

    return eval(_string)


## NOTE: function below is actually used in the exec script
import pandas as pd
from typing import List

def fun_sort_cost(_dataframe: pd.DataFrame, _countrylist: List[str]) -> pd.DataFrame:
    """
    Orders the cost-curve dataframe based on cost and filters it for specified countries.

    This function filters the given dataframe by the list of country ISO codes, 
    sorts it by 'value' and 'PERC', and computes a cumulative regional value.

    Args:
        _dataframe (pd.DataFrame): A dataframe containing cost curve data with at least 
                                    'ISO', 'value', and 'PERC' columns.
        _countrylist (List[str]): A list of country ISO codes to filter the dataframe.

    Returns:
        pd.DataFrame: A sorted and filtered dataframe based on the specified country list 
                       and ordered by 'value' and 'PERC'. The cumulative regional value 
                       is stored in the 'REG_CUM' column.
    """
    _string = "_dataframe"
    
    # Set the index of the dataframe
    setindex(eval(_string), False)
    
    # Filter and sort the dataframe
    exec(
        f"{_string}_sel = {_string}[_dataframe.ISO.isin(_countrylist)].sort_values(by=['value', 'PERC'])"
    )
    exec(f"{_string}_sel = {_string}_sel[_dataframe.value != 0] ")

    _prev = 0
    for _i in eval(_string + "_sel").index:
        if _i == 0:
            eval(_string + "_sel").loc[_i, "REG_CUM"] = 0
        else:
            eval(_string + "_sel").loc[_i, "REG_CUM"] = (
                0.01 * eval(_string + "_sel").loc[_i, "RES"] + _prev
            )  # We order the data at the regional level
            _prev = eval(_string + "_sel").loc[_i, "REG_CUM"]

    print("Returned dataframe:", eval(_string + "_sel"))
    return eval(_string + "_sel")


## NOTE: function below is actually used in the exec script
def fun_read_cost(_f: str, file_name_dict: Dict[str, str]) -> pd.DataFrame:
    """
    Reads cost curve data for a given fuel.

    Args:
        _f (str): The fuel type (e.g., "SOL", "BIO").
        file_name_dict (Dict[str, str]): A dictionary mapping fuel types to their respective file names.

    Returns:
        pd.DataFrame: A melted dataframe containing cost and resource data for the specified fuel.
    """
    _string = "_df_" + _f
    _string2 = f"pd.read_csv(CONSTANTS.INPUT_DATA_DIR / (file_name_dict[_f]+'.csv'), sep=';', encoding='latin-1')"
    _string3 = f"pd.read_csv(CONSTANTS.INPUT_DATA_DIR / (file_name_dict[_f].replace('CostCurve','MaxProd')+'.csv'), sep=';', encoding='latin-1')"
    
    exec(f"{_string} = {str(_string2)}")  # creating cost df
    exec(f"{_string + '_res'} = {str(_string3)}")  # creating resources df

    # Renaming columns in cost df
    for _c in eval(_string):
        eval(_string).rename(columns={_c: _c[-3:]}, inplace=True)
    eval(_string).rename(columns={eval(_string).columns[0]: "PERC"}, inplace=True)

    # Renaming columns in resources df
    for _c in eval(_string + "_res"):
        eval(_string + "_res").rename(columns={_c: _c[-3:]}, inplace=True)
        eval(_string + "_res").rename(columns={eval(_string + "_res").columns[0]: "RES"}, inplace=True)

    exec(f"{_string + '_res'} = { _string + '_res' } / 1e9 * 3.6 / 1000")  # Converting data from KWH/YR to EJ/YR

    # Melting the cost dataframe
    setindex(eval(_string), False)
    exec(f"{_string + '_melt'} = { _string }.melt(id_vars=['PERC'])")
    eval(_string + "_melt").rename(columns={"variable": "ISO"}, inplace=True)
    setindex(eval(_string + "_melt"), "PERC")

    # Melting the resource dataframe
    setindex(eval(_string + "_res"), False)
    exec(f"{_string + '_res_melt'} = { _string + '_res' }.melt(id_vars=['RES'])")
    eval(_string + "_res_melt").rename(columns={"variable": "ISO"}, inplace=True)
    setindex(eval(_string + "_res_melt"), "RES")

    # Adding Resources DATA to df_pv_melt
    setindex(eval(_string + "_melt"), "ISO")
    setindex(eval(_string + "_res_melt"), "ISO")
    exec(f"{_string + '_melt'}['RES'] = { _string + '_res_melt'}['value']")

    print("Dataframe returned:", _string + "_melt", "| RES data are in EJ, cost values are in $/kwh")
    return eval(_string + "_melt")


def fun_baseyear_criteria(
    countrylist: List[str],
    df_base_year_share: pd.DataFrame,
    df_last_year_share: pd.DataFrame
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Extends the base year and last year share dataframes to cover a specified range of years,
    and reformats them to have a consistent structure.

    This function first extends the `df_base_year_share` and `df_last_year_share` DataFrames
    to include years from 2010 to 2105 in 5-year increments. It then reshapes the DataFrames
    to have 'TIME' as the index and 'IAM_FUEL' as columns, with values filled accordingly.

    Args:
        countrylist (list[str]): A list of country identifiers for which the shares are calculated.
        df_base_year_share (pd.DataFrame): A DataFrame containing base year shares for the countries.
        df_last_year_share (pd.DataFrame): A DataFrame containing last year shares for the countries.

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:
            - The extended and reshaped `df_base_year_share`.
            - The extended and reshaped `df_last_year_share`.
    """
    # NOTE: Cache strategy here for each region (this function does not seem to be target dependent)
    df_base_year_share["TIME"] = 2010
    df_base_year_share = fun_pd_extend_time(
        df_base_year_share, list(range(2010, 2105, 5))
    )

    df_base_year_share = unmelt_variable(
        df_base_year_share,
        countrylist,
        _index="TIME",
        _columns="IAM_FUEL",
        _values="VALUE",
    )

    ## Same for last_year_share (added 2020_12_15)
    df_last_year_share["TIME"] = 2015
    df_last_year_share = fun_pd_extend_time(
        df_last_year_share, list(range(2010, 2105, 5))
    )
    df_last_year_share = unmelt_variable(
        df_last_year_share,
        countrylist,
        _index="TIME",
        _columns="IAM_FUEL",
        _values="VALUE",
    )

    return df_base_year_share, df_last_year_share


def fun_allowed_nuclear(
    df_platts: pd.DataFrame, 
    countrylist: List[str], 
    df_gov: pd.DataFrame, 
    _df_all_criteria: pd.DataFrame
) -> pd.DataFrame:
    """Adjusts the nuclear energy criteria for countries based on governance and 
    the presence of nuclear energy resources within a region.

    This function creates a list of countries that are allowed to have nuclear energy
    based on the input DataFrames and adjusts the values in the provided criteria 
    DataFrame accordingly. Countries without nuclear capabilities are assigned a 
    very low governance-adjusted value.

    Parameters
    ----------
    df_platts : pd.DataFrame
        A DataFrame containing fuel data including the fuel type and country ISO codes.
        
    countrylist : list[str]
        A list of countries in the region under consideration.
        
    df_gov : pd.DataFrame
        A DataFrame containing governance data with country ISO codes and governance 
        values for the countries.
        
    _df_all_criteria : pd.DataFrame
        A DataFrame containing all criteria metrics including a column for nuclear energy.

    Returns
    -------
    pd.DataFrame
        The modified criteria DataFrame with adjusted nuclear energy values for the 
        countries based on governance and the allowed nuclear status.
    """
    # Creating List of ALL nuclear countries based on PLATTS
    zero_val = 0
    nuclear_countries = df_platts[df_platts.FUEL == "UR"].ISO.unique()
    nuclear_countries_within_region = list(
        set(nuclear_countries).intersection(countrylist)
    )  # Creating list of nuclear countries within that region
    no_nuclear_within_region = list(
        set(countrylist) - set(nuclear_countries_within_region)
    )  # Creating list of countries NOT allowed to have nuclear (within that region)

    if len(nuclear_countries) == 0:
        # Very low (not zero) value if there is no nuclear today in a given region 
        # (because nuclear could show up later in the region)
        zero_val = 1e-9

    # Setting ZERO NUCLEAR (very low value instead of zero: governance/1e9) IN SELECTED COUNTRIES:
    for c in no_nuclear_within_region:  # List of countries with no nuclear
        _df_all_criteria.loc[
            (_df_all_criteria.index.get_level_values(1).isin([c])), "NUC"
        ] = (
            df_gov[(df_gov.index.get_level_values(1).isin([c]))]["governance"]
            * zero_val
        )

    _df_all_criteria.loc[:, "NUC"] = (
        _df_all_criteria["NUC"] / _df_all_criteria["NUC"].groupby("TIME").sum()
    )
    
    return _df_all_criteria

def fun_select_step2_variables(
    random_electricity_weights: bool,
    model: str,
    target: str,
    region: str,
    df_all: pd.DataFrame,
    ra: int,
    file_name_iamc: str,
    list_of_strings:list=['BLEND']
) -> Union[pd.DataFrame, str]:
    """This function selects variables, and add an unit columns to the dataframe.
       It returns the updated dataframe with the selected variables and the the csv file name (which depends based on criteria weights)

    Args:
        random_electricity_weights (bool): False if we use default electricity criteria weights, otherwise True
        model (str): model 
        target (str): scenario
        region (str): region
        df_all (pd.DataFrame): pd.DataFrame
        ra (int): random criteria weight seed
        file_name_iamc (str): Name of csv file

    Returns:
        Union[pd.DataFrame, str]: Dataframe with selected downscaled results and csv file name
    """

    df_long=df_all.copy(deep=True)
    # Selecting variables to be included
    for mystring in list_of_strings:
        df_long = df_long.iloc[:, df_long.columns.str.contains(mystring)]
    df_long = fun_pd_long_format(df_long)
    # Csv file name
    if random_electricity_weights:
        file_name_iamc = f"{file_name_iamc.replace('.csv','')}_{ra}"
    # if ".csv" not in file_name_iamc:
    file_name_iamc =file_name_iamc.replace('.csv','') + ".csv"

    df_all = fun_pd_wide_format(df_long, region, model, target, "EJ/yr")
    df_all["UNIT"] = "EJ/yr"
    iamc_index = ["MODEL", "SCENARIO", "ISO", "VARIABLE", "UNIT"]
    setindex(df_all, iamc_index)
    return df_all, file_name_iamc

def step1_get_detailed_method_info(df: pd.DataFrame) -> pd.DataFrame:
    """
    Enhance the input DataFrame with detailed method information.

    This function processes the 'METHOD' column of the input DataFrame,
    replacing method codes with more descriptive labels and extracting
    additional information into new columns.

    Parameters:
    -----------
    df : pd.DataFrame
        Input DataFrame containing a 'METHOD' column with method codes.

    Returns:
    --------
    pd.DataFrame
        The input DataFrame with the following modifications:
        - 'METHOD' column values replaced with more descriptive labels.
        - New 'Until' column with the target year (if applicable).
        - New 'Over' column with the variable being used for progression (if applicable).
        - New 'Scale' column with the scale type (linear or log-scale, if applicable).

    Notes:
    ------
    - The function uses a predefined dictionary to map method codes to descriptive labels.
    - It assumes the existence of helper functions:
      - fun_check_if_all_characters_are_numbers()
      - extract_text_in_parentheses()
    """
    # Dictionary mapping method codes to descriptive labels
    mydict = {
        'wo_smooth_enlong_ENLONG_RATIO': 'Default method',
        'TIME_True_ENSHORT_REF_to_ENLONG_RATIO_2100_thenENLONG_RATIO': '2100, over time (linear)',
        'TIME_True_ENSHORT_REF_to_ENLONG_RATIO_2050_thenENLONG_RATIO': '2050, over time (linear)',
        'TIME_False_ENSHORT_REF_to_ENLONG_RATIO_2100_thenENLONG_RATIO': '2100, over time (log-scale)',
        'TIME_False_ENSHORT_REF_to_ENLONG_RATIO_2050_thenENLONG_RATIO': '2050, over time (log-scale)',
        'GDPCAP_True_ENSHORT_REF_to_ENLONG_RATIO_2100_thenENLONG_RATIO': '2100, over GDP per capita (log-scale)',
        'GDPCAP_True_ENSHORT_REF_to_ENLONG_RATIO_2050_thenENLONG_RATIO': '2050, over GDP per capita (log-scale)',
        'GDPCAP_False_ENSHORT_REF_to_ENLONG_RATIO_2100_thenENLONG_RATIO': '2100, over GDP per capita (linear)',
        'GDPCAP_False_ENSHORT_REF_to_ENLONG_RATIO_2050_thenENLONG_RATIO': '2050, over GDP per capita (linear)',
    }
    
    # Replace method codes with descriptive labels
    df = df.replace(mydict)
    
    # Extract 'Until' year information
    d1 = {x: int(x[:4]) for x in df.METHOD.unique() if fun_check_if_all_characters_are_numbers(x[:4])}
    df['Until'] = [d1.get(x, 'default') for x in df.METHOD]
    
    # Extract 'Over' information (time or GDP per capita)
    d2 = {x: x[11:15] for x in df.METHOD.unique() if fun_check_if_all_characters_are_numbers(x[:4])}
    df['Over'] = [d2.get(x, 'default') for x in df.METHOD]
    
    # Extract 'Scale' information (linear or log-scale)
    d3 = {x: extract_text_in_parentheses(x) for x in df.METHOD.unique() if fun_check_if_all_characters_are_numbers(x[:4])}
    df['Scale'] = [d3.get(x, 'default') for x in df.METHOD]
    
    return df
